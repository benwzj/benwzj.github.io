<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://benwzj.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://benwzj.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-09T13:35:10+00:00</updated><id>https://benwzj.github.io/feed.xml</id><title type="html">BEN WEN</title><subtitle>A website to show the world of Ben Wen </subtitle><entry><title type="html">Important Concepts in ML</title><link href="https://benwzj.github.io/blog/2025/ml-concepts/" rel="alternate" type="text/html" title="Important Concepts in ML"/><published>2025-06-09T00:00:00+00:00</published><updated>2025-06-09T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/ml-concepts</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/ml-concepts/"><![CDATA[<h2 id="standard-deviation">Standard Deviation</h2> <p>Standard deviation measures how spread out the values in a dataset are from the mean (average).</p> <h3 id="example">Example:</h3> <p>Let‚Äôs say you have test scores from two classes:</p> <p>Class A: <code class="language-plaintext highlighter-rouge">[78, 80, 82, 81, 79] ‚Üí Avg = 80</code> Standard deviation is low: everyone scored close to the mean.</p> <p>Class B: <code class="language-plaintext highlighter-rouge">[50, 60, 80, 95, 100] ‚Üí Avg = 77</code> Standard deviation is high: big spread from the mean.</p> <h3 id="formula-simplified">Formula (simplified):</h3> <p>For a dataset with values x1, x2,‚Ä¶,xn:</p> \[\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2}\] <h2 id="data-distribution">Data Distribution</h2> <p>Understanding data distribution is key in statistics, data analysis, and machine learning. Data distribution describes how values in a dataset are spread out or arranged. It tells you:</p> <ul> <li>Which values occur most often</li> <li>How the values are grouped</li> <li>Whether the data is symmetrical, skewed, or has outliers</li> </ul> <h3 id="example-1">Example:</h3> <p>Imagine test scores from a class:</p> <p><code class="language-plaintext highlighter-rouge">[70, 72, 75, 75, 76, 78, 80, 85, 90, 100]</code></p> <p>You can see most scores are around 75‚Äì85, with one outlier (100). That pattern of how often each score occurs is its distribution.</p> <h3 id="random-module-example">Random module Example:</h3> <p>Use Random to analog Data Distribution, it is a list of all possible values, which shows how often each value occurs.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">numpy</span> <span class="kn">import</span> <span class="n">random</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">p</code> is the Data Distribution. The sum of all probability numbers should be 1. Here the <code class="language-plaintext highlighter-rouge">9</code> never show up because of probability is 0.</p> <p>Data Distribution Concepts:</p> <ul> <li><strong>Frequency</strong>: How often each value appears</li> <li><strong>Mean (average)</strong>: Where the center of the data is</li> <li><strong>Spread</strong>: How far the values are from each other</li> <li><strong>Shape</strong>: Overall pattern (normal, skewed, etc.)</li> </ul> <p>Data Distribution helps answer: üìå What‚Äôs common? üìå What‚Äôs rare? üìå How is it spread out?</p> <p>Such lists are important when working with statistics and data science. The random module offer methods that returns randomly generated data distributions.</p> <h3 id="numpy-offers-many-distribution-functions">NumPy Offers Many Distribution Functions</h3> <p>Why we need different distribution? Different types of randomness behave differently in nature. Each distribution models a specific type of uncertainty, For example:</p> <table> <thead> <tr> <th>Distribution</th> <th>Common Use Case</th> </tr> </thead> <tbody> <tr> <td><strong>Uniform</strong></td> <td>Equal chance outcomes (e.g., rolling a die)</td> </tr> <tr> <td><strong>Normal (Gaussian)</strong></td> <td>Natural measurements (e.g., height, test scores)</td> </tr> <tr> <td><strong>Binomial</strong></td> <td>Repeated yes/no trials (e.g., coin flips)</td> </tr> <tr> <td><strong>Poisson</strong></td> <td>Counting rare events (e.g., calls per hour)</td> </tr> <tr> <td><strong>Exponential</strong></td> <td>Time between events (e.g., waiting time)</td> </tr> <tr> <td><strong>Beta / Gamma / Chi-squared</strong></td> <td>Advanced statistical modeling</td> </tr> </tbody> </table> <h4 id="example-using-normal-distribution---human-heights">Example: Using Normal Distribution - Human Heights</h4> <p>Because Human traits like height follow a bell-shaped (normal) distribution. So use normal distribution:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Simulate 1000 people's heights (mean=170cm, std=10cm)
</span><span class="n">heights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">170</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">skyblue</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Simulated Human Heights (Normal Distribution)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Height (cm)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Frequency</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div> <h4 id="example-binomial-distribution--coin-tosses">Example: Binomial Distribution ‚Äì Coin Tosses</h4> <p>Models ‚Äúyes/no‚Äù outcomes like coin tosses, quiz answers, etc.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simulate flipping a fair coin 10 times, repeat 1000 experiments
</span><span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">orange</span><span class="sh">'</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="sh">'</span><span class="s">left</span><span class="sh">'</span><span class="p">,</span> <span class="n">rwidth</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">10 Coin Flips per Trial (Binomial Distribution)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Number of Heads</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Frequency</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h4 id="exponential-distribution--wait-times">Exponential Distribution ‚Äì Wait Times</h4> <p>Time between events like arrivals, failures, or clicks.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simulate wait times between buses (mean wait = 10 minutes)
</span><span class="n">wait_times</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">exponential</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">wait_times</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">purple</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Wait Time Between Buses (Exponential Distribution)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Minutes</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Frequency</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h4 id="poisson-distribution--call-center-events">Poisson Distribution ‚Äì Call Center Events</h4> <p>Models how often rare events happen in a fixed time (calls, emails, etc.)</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simulate number of calls per minute (avg = 3 calls/min)
</span><span class="n">calls</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">poisson</span><span class="p">(</span><span class="n">lam</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">calls</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="sh">'</span><span class="s">left</span><span class="sh">'</span><span class="p">,</span> <span class="n">rwidth</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Number of Calls Per Minute (Poisson Distribution)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Calls</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Frequency</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h2 id="normalize-data">Normalize data</h2> <p>When creating a model with multiple features, the values of each feature should span roughly the same range. If one feature‚Äôs values range from 500 to 100,000 and another feature‚Äôs values range from 2 to 12, the model will need to have weights of extremely low or extremely high values to be able to combine these features effectively. This could result in a low quality model. To avoid this, normalize features in a multi-feature model.</p> <p>This can be done by converting each raw value to its <strong>Z-score</strong>.</p> <blockquote> <p>The Z-score for a given value is how many standard deviations away from the mean the value is.</p> </blockquote> <p>Consider a feature with a <strong>mean</strong> of 60 and a <strong>standard deviation</strong> of 10. The raw value 75 would have a Z-score of +1.5: <code class="language-plaintext highlighter-rouge">Z-score = (75 - 60) / 10 = +1.5</code> The raw value 38 would have a Z-score of -2.2: <code class="language-plaintext highlighter-rouge">Z-score = (38 - 60) / 10 = -2.2</code></p> <h2 id="label-leakage">Label Leakage</h2> <p>It‚Äôs important to prevent the model from getting the label as input during training, which is called label leakage.</p> <h3 id="why">Why?</h3> <p>If you include the label in your input features, the model <strong>cheats</strong> ‚Äî it learns the answer directly rather than learning to predict it from the actual data.</p> <h3 id="example-2">Example</h3> <p>Imagine you‚Äôre training a model to predict house prices, and you accidentally include the actual sale price (price) as a feature:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">location</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">size</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">price</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># üö´ WRONG
</span><span class="n">target</span> <span class="o">=</span> <span class="sh">'</span><span class="s">price</span><span class="sh">'</span>
</code></pre></div></div> <p>The model would just learn to copy the price column instead of learning how location and size affect price.</p> <h3 id="training-a-model-to-predict-dog-or-cat">training a model to predict dog or cat</h3> <p>When training a model to predict something (like ‚Äúdog or cat‚Äù), you must provide both:</p> <ul> <li>Inputs: the images themselves (the pixel data ‚Äî e.g. arrays, tensors, etc.)</li> <li>Labels: the correct answer (e.g. ‚Äúdog‚Äù or ‚Äúcat‚Äù) ‚Äî this is what the model tries to learn to predict</li> </ul> <p>In this case, You do provide labels during training, but You do not include the label as part of the <strong>input</strong> features(like the image containing ‚Äòdog‚Äô letters).</p> <p>Let‚Äôs say you have:</p> <ul> <li>Image 1: üê∂ (dog) ‚Üí label: 0</li> <li>Image 2: üê± (cat) ‚Üí label: 1</li> </ul> <p>Training Code (simplified):</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">image_1</span><span class="p">,</span> <span class="n">image_2</span><span class="p">,</span> <span class="p">...]</span>   <span class="c1"># Input: pixel data only
</span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">...]</span>               <span class="c1"># Target: labels (dog = 0, cat = 1)
</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Python"/><category term="ML"/><category term="Data-Distribution"/><summary type="html"><![CDATA[Standard Deviation]]></summary></entry><entry><title type="html">What is Keras</title><link href="https://benwzj.github.io/blog/2025/Keras/" rel="alternate" type="text/html" title="What is Keras"/><published>2025-06-07T00:00:00+00:00</published><updated>2025-06-07T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/Keras</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/Keras/"><![CDATA[<h2 id="what-is-keras">What is Keras</h2> <p>Keras is an open-source, <strong>high-level</strong> deep learning API written in Python. It is designed to simplify the process of building and training neural networks. Keras acts as a user-friendly interface for complex machine learning libraries like TensorFlow, Theano, or CNTK, though today it is fully integrated into TensorFlow 2.x.</p> <p>Keras allows developers and researchers to build deep learning models with just a few lines of code. It supports common neural network layers (dense, convolutional, recurrent, etc.) and techniques like dropout, batch normalization, and activation functions. Its modular design makes it easy to build sequential models (a stack of layers) or more complex models using its Functional API.</p> <p>Keras has Built-in tools for model training, evaluation, and prediction.</p> <h2 id="example">Example</h2> <h3 id="creates-a-simple-feedforward-neural-network">Creates a simple feedforward neural network</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">from</span> <span class="n">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="n">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,)),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div> <p>This example creates a simple feedforward neural network.</p> <h3 id="set_random_seed-function">set_random_seed Function</h3> <p><code class="language-plaintext highlighter-rouge">keras.utils.set_random_seed(42)</code> is used to make your machine learning experiment <strong>reproducible</strong> by setting a random seed.</p> <p>üîç What It Does: It sets the random seed for:</p> <ul> <li>NumPy</li> <li>Python‚Äôs built-in random module</li> <li>TensorFlow</li> </ul> <p>This ensures that all random operations (like weight initialization, data shuffling, dropout, etc.) behave the same way every time you run the code.</p> <p>üîí Why Use It? Machine learning involves randomness ‚Äî for example:</p> <ul> <li>Initializing model weights</li> <li>Shuffling data during training</li> <li>Splitting datasets randomly</li> </ul> <p>Without a fixed seed, you‚Äôll get slightly different results each time. Setting the seed makes results reproducible, which is critical for:</p> <ul> <li>Debugging</li> <li>Experiment comparison</li> <li>Scientific reliability</li> </ul> <p>üß† What is 42? The number 42 is just a commonly used seed value (a nod to The Hitchhiker‚Äôs Guide to the Galaxy). You can use any integer.</p> <p>‚úÖ Summary: Ensures consistent behavior across runs by fixing the random seed used by Keras/TensorFlow and related libraries.</p>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[What is Keras]]></summary></entry><entry><title type="html">Random in Coding</title><link href="https://benwzj.github.io/blog/2025/py-random/" rel="alternate" type="text/html" title="Random in Coding"/><published>2025-06-07T00:00:00+00:00</published><updated>2025-06-07T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/py-random</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/py-random/"><![CDATA[<h2 id="about-random-number">About Random Number</h2> <p>Random number does NOT mean a different number every time. Random means something that can not be predicted logically.</p> <h3 id="pseudo-random-and-true-random">Pseudo Random and True Random.</h3> <p>Computers work on programs, and programs are definitive set of instructions. So it means there must be some algorithm to generate a random number as well.</p> <p>Random numbers generated through a generation algorithm are called <strong>pseudo random</strong>. It can be predicted.</p> <p>In order to generate a truly random number on our computers we need to get the random data from some outside source. This outside source is generally our keystrokes, mouse movements, data on network etc.</p> <p>We do not need truly random numbers, unless it is related to security (e.g. encryption keys) or the basis of application is the randomness (e.g. Digital roulette wheels).</p> <h2 id="how-languages-implement-randomness">How languages implement randomness</h2> <p>Most programming languages (like Python, Java, C++) implement pseudo-random number generators (PRNGs).</p> <ul> <li>PRNGs are algorithms that produce sequences of numbers that appear random, but are completely determined by an initial value called the <strong>seed</strong>.</li> <li>Once you set a seed, the sequence of random numbers is repeatable.</li> </ul> <h3 id="algorithm-behind-prngs">Algorithm Behind PRNGs</h3> <table> <thead> <tr> <th>Algorithm</th> <th>Used In</th> </tr> </thead> <tbody> <tr> <td>Linear Congruential Generator (LCG)</td> <td>C, early Java, Python (older versions)</td> </tr> <tr> <td>Mersenne Twister</td> <td>Python <code class="language-plaintext highlighter-rouge">random</code>, NumPy (default before v1.17)</td> </tr> <tr> <td>PCG (Permuted Congruential Generator)</td> <td><code class="language-plaintext highlighter-rouge">numpy.random.default_rng()</code></td> </tr> <tr> <td>Xoroshiro/Xoshiro</td> <td>Modern C/C++ libraries and game engines</td> </tr> </tbody> </table> <h3 id="seed">Seed</h3> <p>Seed: Controlling the Sequence Setting a seed ensures repeatability (useful for debugging, testing, reproducibility). Without a seed, most languages auto-generate one using something like the current time or system entropy.</p> <h3 id="state">State</h3> <p>A pseudo-random number generator (PRNG) doesn‚Äôt truly generate random numbers, it produces a long, predictable sequence of numbers based on:</p> <ul> <li>A starting point (seed)</li> <li>An internal state that evolves as numbers are generated</li> </ul> <p>So, the state is like a snapshot of the PRNG‚Äôs memory at a moment in time. If you save it, you can pause and resume the sequence exactly where you left off.</p> <h3 id="true-randomness-trngs">True Randomness (TRNGs)</h3> <p>Most of time we only need PRNG, but some applications (e.g. cryptography) need true randomness: Collected from physical sources like:</p> <ul> <li>Mouse movement</li> <li>Atmospheric noise</li> <li>Hardware random number generators</li> </ul> <h2 id="python-random-module">Python Random module</h2> <p>Python Random module implements pseudo-random number generators for various distributions. (Other Languea, Library will do smiliar things for Random)</p> <ul> <li>For <strong>integers</strong>, there is uniform selection from a range.</li> <li>For <strong>sequences</strong>, there is uniform selection of a random element, a function to generate a random permutation of a list in-place, and a function for random sampling without replacement.</li> <li>Almost all module functions depend on the basic function <code class="language-plaintext highlighter-rouge">random()</code>, which generates a random float uniformly in the half-open range 0.0 &lt;= X &lt; 1.0.</li> <li>Python uses the <strong>Mersenne Twister</strong> as the core generator.</li> <li>It produces 53-bit precision floats and has a period of 2**19937-1.</li> <li>It is completely unsuitable for cryptographic purposes.</li> <li>For security or cryptographic uses, see the secrets module.</li> <li>The functions supplied by this module are actually bound methods of a hidden instance of the <code class="language-plaintext highlighter-rouge">random.Random</code> class. You can instantiate your own instances of Random to get generators that don‚Äôt share state.</li> </ul> <h3 id="bookkeeping">Bookkeeping</h3> <h4 id="randomseedanone-version2"><code class="language-plaintext highlighter-rouge">random.seed(a=None, version=2)</code></h4> <p>Initialize the random number generator.</p> <ul> <li>If <code class="language-plaintext highlighter-rouge">a</code> is omitted or None, the current system time is used.</li> <li>If <code class="language-plaintext highlighter-rouge">a</code> is an int, it is used directly.</li> </ul> <h4 id="randomgetstate"><code class="language-plaintext highlighter-rouge">random.getstate()</code></h4> <p>Return an object capturing the current internal state of the generator. This object can be passed to <code class="language-plaintext highlighter-rouge">setstate()</code> to restore the state.</p> <h4 id="randomsetstatestate"><code class="language-plaintext highlighter-rouge">random.setstate(state)</code></h4> <p>It can restore the state for the random generator.</p> <h3 id="for-sequences-random-selection">For Sequences random selection</h3> <h4 id="randomchoiceseq">random.choice(seq)</h4> <p>Return a random element from the non-empty sequence <code class="language-plaintext highlighter-rouge">seq</code>.</p> <h4 id="randomshufflex">random.shuffle(x)</h4> <p>Shuffle the sequence <code class="language-plaintext highlighter-rouge">x</code> in place. (please note that, it will mutate the sequence). To shuffle an immutable sequence and return a new shuffled list, use <code class="language-plaintext highlighter-rouge">sample(x, k=len(x))</code> instead.</p> <h4 id="randomsamplepopulation-k--countsnone">random.sample(population, k, *, counts=None)</h4> <p>Return a <code class="language-plaintext highlighter-rouge">k</code> length, new list of unique elements chosen from the <code class="language-plaintext highlighter-rouge">population</code> sequence or set. Used for random sampling without replacement.</p> <h2 id="numpy-random-module">NumPy Random module</h2> <p>Python built-in random module vs. numpy.random</p> <table> <thead> <tr> <th>Feature</th> <th><code class="language-plaintext highlighter-rouge">random</code> (Python built-in)</th> <th><code class="language-plaintext highlighter-rouge">numpy.random</code> (NumPy)</th> </tr> </thead> <tbody> <tr> <td><strong>Library</strong></td> <td>Built-in (<code class="language-plaintext highlighter-rouge">import random</code>)</td> <td>Requires NumPy (<code class="language-plaintext highlighter-rouge">import numpy as np</code>)</td> </tr> <tr> <td><strong>Speed</strong></td> <td>Slower</td> <td>Much faster (optimized with C under the hood)</td> </tr> <tr> <td><strong>Data Types</strong></td> <td>Works with basic types (int, float)</td> <td>Works with NumPy arrays</td> </tr> <tr> <td><strong>Best For</strong></td> <td>Simple randomness (games, small apps)</td> <td>Scientific computing, machine learning</td> </tr> <tr> <td><strong>Random Arrays</strong></td> <td>Not supported</td> <td>Easily generates arrays of random numbers</td> </tr> <tr> <td><strong>Distributions</strong></td> <td>Basic (uniform, normal)</td> <td>Extensive (normal, binomial, Poisson, etc.)</td> </tr> <tr> <td><strong>Seeding</strong></td> <td><code class="language-plaintext highlighter-rouge">random.seed()</code></td> <td><code class="language-plaintext highlighter-rouge">np.random.seed()</code> or <code class="language-plaintext highlighter-rouge">np.random.default_rng()</code></td> </tr> <tr> <td><strong>Modern Interface</strong></td> <td>Basic only</td> <td><code class="language-plaintext highlighter-rouge">np.random.default_rng()</code> (recommended in NumPy ‚â• 1.17)</td> </tr> </tbody> </table> <p>Python random:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">random</span>
<span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>  <span class="c1"># Random int between 1 and 10
</span></code></pre></div></div> <p>NumPy random:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">rng</span><span class="p">.</span><span class="nf">integers</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>    <span class="c1"># Same, but faster and better control
</span></code></pre></div></div>]]></content><author><name></name></author><category term="Python"/><category term="Python"/><category term="ML"/><category term="Data-Distribution"/><summary type="html"><![CDATA[About Random Number]]></summary></entry><entry><title type="html">Logistic Regression</title><link href="https://benwzj.github.io/blog/2025/logistic-regresion/" rel="alternate" type="text/html" title="Logistic Regression"/><published>2025-05-22T00:00:00+00:00</published><updated>2025-05-22T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/logistic-regresion</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/logistic-regresion/"><![CDATA[<h2 id="what-is-logistic-regression">What is Logistic Regression</h2> <p>Logistic regression is designed to predict the probability of a given outcome.</p> <p>Classic example is spam-prediction model.</p> <p>Simply saying, Logistic regression = Logistic function + linear regression.</p> <h2 id="sigmoid-function">Sigmoid function</h2> <p>There‚Äôs a family of functions called logistic functions whose output represents a probability, always outputting a value between 0 and 1. The standard logistic function, also known as the sigmoid function (sigmoid means ‚Äús-shaped‚Äù), has the formula:</p> \[f(x) = \frac{1}{1 + e^{-x}}\] <ul> <li>The ‚Äòe‚Äô is Euler‚Äôs number, a fundamental mathematical constant. <code class="language-plaintext highlighter-rouge">e‚âà2.71828...</code></li> <li>Logistic functions is one of The most important exponential function.</li> </ul> <p>Here are the classic corresponding graph of the sigmoid function:</p> <figure> <picture> <img src="/assets/img/sigmoid_function_with_axes.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>As the input, x, increases, the output of the sigmoid function approaches but never reaches 1.</li> <li>Similarly, as the input decreases, the sigmoid function‚Äôs output approaches but never reaches 0.</li> <li>The sigmoid function will bend the linear equation straight line into an s-shape.</li> </ul> <h3 id="what-is-exponent">What is Exponent</h3> <table> <thead> <tr> <th>Expression</th> <th>Meaning</th> <th>Result</th> <th>Explanation</th> </tr> </thead> <tbody> <tr> <td>\(2^3\)</td> <td>\(2 \times 2 \times 2\)</td> <td>8</td> <td>Multiply 2 three times</td> </tr> <tr> <td>\(5^2\)</td> <td>\(5 \times 5\)</td> <td>25</td> <td>Square of 5</td> </tr> <tr> <td>\(10^0\)</td> <td>‚Äî</td> <td>1</td> <td>Any non-zero number to the 0 power is 1</td> </tr> <tr> <td>\(2^{-3}\)</td> <td>\(\frac{1}{2^3} = \frac{1}{8}\)</td> <td>0.125</td> <td>Negative = reciprocal</td> </tr> <tr> <td>\(4^{-1}\)</td> <td>\(\frac{1}{4}\)</td> <td>0.25</td> <td>Negative exponent = 1 over base</td> </tr> <tr> <td>\(9^{1/2}\)</td> <td>\(\sqrt{9}\)</td> <td>3</td> <td>Fractional = root</td> </tr> <tr> <td>\(27^{1/3}\)</td> <td>\(\sqrt[3]{27}\)</td> <td>3</td> <td>Cube root</td> </tr> <tr> <td>\(16^{3/4}\)</td> <td>\(\left(\sqrt[4]{16}\right)^3\)</td> <td>8</td> <td>Root first, then power</td> </tr> </tbody> </table> <h3 id="transforming-linear-output-using-the-sigmoid-function">Transforming linear output using the sigmoid function</h3> <p>Left: graph of the linear function z = 2x + 5, with three points highlighted. Right: Sigmoid curve with the same three points highlighted after being transformed by the sigmoid function:</p> <figure> <picture> <img src="/assets/img/linear_to_logistic.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="logistic-regression-trainning">Logistic regression trainning</h2> <p>Logistic regression models are trained using the same process as linear regression models, with two key distinctions:</p> <ul> <li>Logistic regression models use Log Loss as the loss function instead of squared loss.</li> <li>Applying regularization is critical to prevent overfitting.</li> </ul> <h3 id="log-loss">Log Loss</h3> <p>In the Linear regression module, you used squared loss (also called L2 loss) as the loss function. loss function for logistic regression is Log Loss.</p> <figure> <picture> <img src="/assets/img/logloss-func.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="regularization">Regularization</h3> <p>Regularization, a mechanism for penalizing model complexity during training.</p> <p>Regularization is extremely important in logistic regression modeling. Without regularization, the asymptotic nature of logistic regression would keep driving loss towards 0 in cases where the model has a large number of features.</p> <p>Consequently, most logistic regression models use one of the following two strategies to decrease model complexity:</p> <ul> <li>L2 regularization</li> <li>Early stopping: Limiting the number of training steps to halt training while loss is still decreasing.</li> </ul>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[What is Logistic Regression]]></summary></entry><entry><title type="html">Classification</title><link href="https://benwzj.github.io/blog/2025/classification/" rel="alternate" type="text/html" title="Classification"/><published>2025-05-22T00:00:00+00:00</published><updated>2025-05-22T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/classification</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/classification/"><![CDATA[<p>Classification is the task of predicting which of a set of classes (categories) an example belongs to. Classification is converting a <strong>logistic regression model</strong> that predicts a probability into a binary classification model that predicts one of two classes.</p> <p>There are some terms you need to be understood and clear:</p> <ul> <li>TP, FP, TN, FN</li> <li>Threshold</li> <li>Accuracy</li> <li>Recall, or true positive rate</li> <li>False positive rate</li> <li>Precision</li> </ul> <h2 id="threshold">Threshold</h2> <p>Choosing a threshold is very important for Classification. You can use tool like Confusion matrix to understand more about threshold in your model.</p> <h3 id="confusion-matrix">Confusion matrix</h3> <table> <thead> <tr> <th>¬†</th> <th>Actual positive</th> <th>Actual negative</th> </tr> </thead> <tbody> <tr> <td>Predicted positive</td> <td>True positive (TP): A spam email correctly classified as a spam email. These are the spam messages automatically sent to the spam folder.</td> <td>False positive (FP): A not-spam email misclassified as spam. These are the legitimate emails that wind up in the spam folder.</td> </tr> <tr> <td>Predicted negative</td> <td>False negative (FN): A spam email misclassified as not-spam. These are spam emails that aren‚Äôt caught by the spam filter and make their way into the inbox.</td> <td>True negative (TN): A not-spam email correctly classified as not-spam. These are the legitimate emails that are sent directly to the inbox.</td> </tr> </tbody> </table> <p>For example it can look like this:</p> <figure> <picture> <img src="/assets/img/confusion-matrix.png" class="img-fluid rounded z-depth-1" width="50%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>TP = it is Spam and labled as Spam FP = ir is not spam but labled as spam TN = it is not spam and labled as not spam FN = it is spam but labled as not spam</p> <p>When the classification threshold increases, both true and false positives decrease. Becuase the model will likely predict fewer positives overall.</p> <h2 id="measure-model-metrics">Measure Model Metrics</h2> <p>They are Accuracy, TPR, FPR, Precision.</p> <p>TPR and FPR will be used more in Measurement.</p> <h3 id="accuracy">Accuracy</h3> <p>Accuracy is the proportion of all classifications that were correct, whether positive or negative. It is mathematically defined as:</p> \[\text{Accuracy} = \frac{\text{correct classifications}}{\text{total classifications}} = \frac{TP + TN}{TP + TN + FP + FN}\] <h3 id="recall-or-true-positive-rate">Recall, or true positive rate</h3> <p>The true positive rate (TPR), or the proportion of all actual positives that were classified correctly as positives, is also known as recall.</p> <p>Recall is mathematically defined as:</p> \[\text{Recall (or TPR)} = \frac{\text{correctly classified actual positives}}{\text{all actual positives}} = \frac{TP}{TP + FN}\] <p>A hypothetical perfect model would have zero false negatives and therefore a recall (TPR) of 1.0, which is to say, a 100% detection rate.</p> <h3 id="false-positive-rate">False positive rate</h3> <p>The false positive rate (FPR) is the proportion of all actual negatives that were classified incorrectly as positives, also known as the probability of false alarm. It is mathematically defined as:</p> \[FPR = \frac{\text{incorrectly classified actual negatives}}{\text{all actual negatives}} = \frac{FP}{FP + TN}\] <p>A perfect model would have zero false positives and therefore a FPR of 0.0, which is to say, a 0% false alarm rate.</p> <h3 id="precision">Precision</h3> <p>Precision is the proportion of all the model‚Äôs positive classifications that are actually positive. It is mathematically defined as:</p> \[\text{Precision} = \frac{\text{correctly classified actual positives}}{\text{everything classified as positive}} = \frac{TP}{TP + FP}\] <p>Precision improves as false positives decrease, while recall improves when false negatives decrease.</p> <h3 id="choice-of-metric-and-tradeoffs">Choice of metric and tradeoffs</h3> <table> <thead> <tr> <th>Metric</th> <th>Guidance</th> </tr> </thead> <tbody> <tr> <td>Accuracy</td> <td>Use as a rough indicator of model training progress/convergence for balanced datasets. <br/> For model performance, use only in combination with other metrics. <br/> Avoid for imbalanced datasets. Consider using another metric.</td> </tr> <tr> <td>Recall (True positive rate)</td> <td>Use when false negatives are more expensive than false positives.</td> </tr> <tr> <td>False positive rate</td> <td>Use when false positives are more expensive than false negatives.</td> </tr> <tr> <td>Precision</td> <td>Use when it‚Äôs very important for positive predictions to be accurate.</td> </tr> </tbody> </table> <p>Accoring to my understanding, Spam email model should use FPR matric, because I can‚Äôt accept labling legitimate email as spam.</p> <h2 id="roc-and-auc">ROC and AUC</h2> <h3 id="roc">ROC</h3> <p>ROC curve, short for Receiver-operating characteristic curve. The long version of the name, receiver operating characteristic, is a holdover from WWII radar detection.</p> <p>The ROC curve is a visual representation of model performance across all thresholds.</p> <p>The ROC curve is drawn by calculating the true positive rate (TPR) and false positive rate (FPR) at every possible threshold (in practice, at selected intervals), then graphing TPR over FPR.</p> <h3 id="auc">AUC</h3> <p>AUC, short for Area under the curve.</p> <p>The area under the ROC curve (AUC) represents the probability that the model, if given a randomly chosen positive and negative example, will rank the positive higher than the negative.</p> <p>AUC is a useful measure for comparing the performance of two different models, as long as the dataset is roughly balanced. The model with greater area under the curve is generally the better one.</p> <h4 id="understand-auc">Understand AUC</h4> <p>For a binary classifier, a model that does exactly as well as random guesses or coin flips has a ROC that is a diagonal line from (0,0) to (1,1). The AUC is 0.5, representing a 50% probability of correctly ranking a random positive and negative example.</p> <h3 id="auc-and-roc-for-choosing-model-and-threshold">AUC and ROC for choosing model and threshold</h3> <p>AUC is a useful measure for comparing the performance of two different models, as long as the dataset is roughly balanced. The model with greater area under the curve is generally the better one.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/auc_0-65.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/auc_0-93.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The points on a ROC curve closest to (0,1) represent a range of the best-performing thresholds for the given model.</p> <figure> <picture> <img src="/assets/img/auc_abc.png" class="img-fluid rounded z-depth-1" width="50%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>If false positives (false alarms) are highly costly, it may make sense to choose a threshold that gives a lower FPR, like the one at point A, even if TPR is reduced. Conversely, if false positives are cheap and false negatives (missed true positives) highly costly, the threshold for point C, which maximizes TPR, may be preferable. If the costs are roughly equivalent, point B may offer the best balance between TPR and FPR.</p> <p>Understand AUC and ROC:</p> <figure> <picture> <img src="/assets/img/AUC.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="prediction-bias">Prediction bias</h2> <p>Prediction bias is the difference between the mean of a model‚Äôs predictions and the mean of ground-truth labels in the data. A model trained on a dataset where 5% of the emails are spam should predict, on average, that 5% of the emails it classifies are spam. In other words, the mean of the labels in the ground-truth dataset is 0.05, and the mean of the model‚Äôs predictions should also be 0.05. If this is the case, the model has zero prediction bias. Of course, the model might still have other problems.</p> <h2 id="multi-class-classification">Multi-class classification</h2> <p>Multi-class classification can be treated as an extension of binary classification to more than two classes.</p> <p>For example, in a three-class multi-class classification problem, where you‚Äôre classifying examples with the labels A, B, and C, you could turn the problem into two separate binary classification problems. First, you might create a binary classifier that categorizes examples using the label A+B and the label C. Then, you could create a second binary classifier that reclassifies the examples that are labeled A+B using the label A and the label B.</p> <h2 id="references">References</h2> <ul> <li><a href="https://developers.google.com/machine-learning/crash-course/classification">Google crash course</a></li> </ul>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[Classification is the task of predicting which of a set of classes (categories) an example belongs to. Classification is converting a logistic regression model that predicts a probability into a binary classification model that predicts one of two classes.]]></summary></entry><entry><title type="html">Linear Regression</title><link href="https://benwzj.github.io/blog/2025/linear-regression/" rel="alternate" type="text/html" title="Linear Regression"/><published>2025-05-21T00:00:00+00:00</published><updated>2025-05-21T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/linear-regression</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/linear-regression/"><![CDATA[<h2 id="what-is-linear-regression">What is Linear Regression</h2> <p>Training data to form a model, simply say, it is to find the bias and weights among the data. linear regression is one of the <strong>methods</strong> that find the relationship between features and a label to get the bias and weights.</p> <figure> <picture> <img src="/assets/img/car-data-points-with-model.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>During training, the model calculates the <strong>weight</strong> and <strong>bias</strong> that produce the best model.</p> <figure> <picture> <img src="/assets/img/linear-regression-equation.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="loss">Loss</h2> <p>Loss is a numerical metric that describes how wrong a model‚Äôs predictions are. Loss measures the distance between the model‚Äôs predictions and the actual labels. The goal of training a model is to minimize the loss, reducing it to its lowest possible value.</p> <figure> <picture> <img src="/assets/img/loss-lines.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The two most common methods to remove the sign are the following:</p> <ul> <li>Take the absolute value of the difference between the actual value and the prediction.</li> <li>Square the difference between the actual value and the prediction.</li> </ul> <p>There are four main types of loss:</p> <ul> <li><strong>L1 loss</strong>: The sum of the absolute values of the difference between the predicted values and the actual values.</li> <li><strong>Mean absolute error (MAE)</strong>: The average of L1 losses across a set of examples.</li> <li><strong>L2 loss</strong>: The sum of the squared difference between the predicted values and the actual values.</li> <li><strong>Mean squared error (MSE)</strong>: The average of L2 losses across a set of examples.</li> </ul> <h3 id="choosing-a-loss">Choosing a loss</h3> <p>In training, model will try to get the best bias and weights according to the LOSS. So choosing a loss is matter.</p> <p>When choosing the best loss function, also consider how you want the model to treat outliers. The outliers are closer to the model trained with MSE than to the model trained with MAE.</p> <figure> <picture> <img src="/assets/img/model-mse.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A model trained with MSE moves the model closer to the outliers.</p> <figure> <picture> <img src="/assets/img/model-mae.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A model trained with MAE is farther from the outliers.</p> <h2 id="gradient-descent">Gradient descent</h2> <p>Gradient descent is a <strong>mathematical technique</strong> that iteratively finds the weights and bias that produce the model with the lowest loss.</p> <p>Gradient descent finds the best weight and bias by repeating the following process for a number of user-defined iterations. The model begins training with randomized weights and biases near zero, and then repeats the following steps:</p> <ul> <li>Calculate the loss with the current weight and bias.</li> <li>Determine the direction to move the weights and bias that reduce loss.</li> <li>Move the weight and bias values a small amount in the direction that reduces loss.</li> <li>Return to step one and repeat the process until the model can‚Äôt reduce the loss any further.</li> </ul> <p>This is typical loss curve, Loss is on the y-axis and iterations are on the x-axis:</p> <figure> <picture> <img src="/assets/img/loss-convergence.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Loss surface showing the weight and bias values that produce the lowest loss.</p> <figure> <picture> <img src="/assets/img/loss-surface-points.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="hyperparameters">Hyperparameters</h2> <p>Hyperparameters are variables that control different aspects of training. Three common hyperparameters are:</p> <ul> <li>Learning rate</li> <li>Batch size</li> <li>Epochs</li> </ul> <p>In contrast, parameters are the variables, like the weights and bias, that are part of the model itself. In other words, hyperparameters are values that you control; parameters are values that the model calculates during training.</p> <h3 id="learning-rate">Learning rate</h3> <p>The learning rate determines the magnitude of the changes to make to the weights and bias during each step of the gradient descent process.</p> <p>The model multiplies the gradient by the learning rate to determine the model‚Äôs parameters (weight and bias values) for the next iteration. For example, if the <strong>gradient‚Äôs magnitude</strong> is 2.5 and the learning rate is 0.01, then the model will change the parameter by 0.025.</p> <p>Learning rate is a floating point number you set that influences how quickly the model converges.</p> <p>The ideal learning rate helps the model to converge within a reasonable number of iterations.</p> <p>What do it means when Learning Rate is 1? A learning rate of 1 means that the model updates its weights by the full amount of the calculated gradient. This almost inevitably leads to highly unstable training and the model failing to converge to a good solution.</p> <h3 id="batch-size">Batch size</h3> <p>Batch size is a hyperparameter that refers to the number of examples the model processes before updating its weights and bias.</p> <p>You might think that the model should do Full Batch, means calculating the loss for every example in the dataset before updating the weights and bias. However, when a dataset contains hundreds of thousands or even millions of examples, using the full batch isn‚Äôt practical.</p> <p>Two common techniques to get the right gradient on average without needing to look at every example in the dataset before updating the weights and bias:</p> <ul> <li>Stochastic gradient descent (SGD): Stochastic gradient descent uses only a single example (a batch size of one) per iteration. The term ‚Äústochastic‚Äù indicates that the one example comprising each batch is chosen at random. (Note that using stochastic gradient descent can produce noise throughout the entire loss curve, not just near convergence.)</li> <li>Mini-batch stochastic gradient descent (mini-batch SGD): Mini-batch stochastic gradient descent is a compromise between full-batch and SGD. The model chooses the examples included in each batch at random, averages their gradients, and then updates the weights and bias once per iteration.</li> </ul> <h3 id="epochs">Epochs</h3> <p>During training, an epoch means that the model has processed every example in the training set once. For example, given a training set with 1,000 examples and a mini-batch size of 100 examples, it will take the model 10 iterations to complete one epoch.</p> <p>Training typically requires many epochs. In general, more epochs produces a better model, but also takes more time to train.</p> <p>Here is an example to tell the difference:</p> <ul> <li>Full batch: After the model looks at all the examples in the dataset. For instance, if a dataset contains 1,000 examples and the model trains for 20 epochs, the model updates the weights and bias 20 times, once per epoch.</li> <li>Stochastic gradient descent: After the model looks at a single example from the dataset. For instance, if a dataset contains 1,000 examples and trains for 20 epochs, the model updates the weights and bias 20,000 times.</li> <li>Mini-batch stochastic gradient descent: After the model looks at the examples in each batch. For instance, if a dataset contains 1,000 examples, and the batch size is 100, and the model trains for 20 epochs, the model updates the weights and bias 200 times.</li> </ul> <h2 id="generate-a-correlation-matrix">Generate a correlation matrix</h2> <p>An important part of machine learning is determining which features correlate with the label. you can use a correlation matrix to identify features whose values correlate well with the label.</p> <p>Correlation values have the following meanings:</p> <ul> <li>1.0: perfect positive correlation; that is, when one attribute rises, the other attribute rises.</li> <li>-1.0: perfect negative correlation; that is, when one attribute rises, the other attribute falls.</li> <li>0.0: no correlation; the two columns are not linearly related. In general, the higher the absolute value of a correlation value, the greater its predictive power.</li> </ul> <p>dataframe can provide such function: <code class="language-plaintext highlighter-rouge">training_df.corr(numeric_only = True)</code></p> <h2 id="visualize-relationships-in-dataset">Visualize relationships in dataset</h2> <p>dataframe provide such function: <code class="language-plaintext highlighter-rouge">sns.pairplot(training_df, x_vars=["FARE", "TRIP_MILES", "TRIP_SECONDS"], y_vars=["FARE", "TRIP_MILES", "TRIP_SECONDS"])</code></p> <h2 id="references">References</h2> <ul> <li><a href="https://developers.google.com/machine-learning/crash-course/linear-regression">Google crash course</a></li> </ul>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[What is Linear Regression]]></summary></entry><entry><title type="html">Pandas Library</title><link href="https://benwzj.github.io/blog/2025/pandas/" rel="alternate" type="text/html" title="Pandas Library"/><published>2025-05-18T00:00:00+00:00</published><updated>2025-05-18T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/pandas</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/pandas/"><![CDATA[<p>Pandas is a powerful Python library for data manipulation and analysis.</p> <h2 id="basic-terms">Basic Terms</h2> <h3 id="axis">Axis</h3> <p>In Pandas, <code class="language-plaintext highlighter-rouge">axis</code> refers to the direction along which an operation is applied:</p> <table> <thead> <tr> <th>Axis</th> <th>Direction</th> <th>Refers To</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">0</code></td> <td><strong>Vertical</strong></td> <td><strong>Rows</strong> (downward)</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">1</code></td> <td><strong>Horizontal</strong></td> <td><strong>Columns</strong> (across)</td> </tr> </tbody> </table> <p>axis=0 means ‚Äúgo down the rows‚Äù (operate column-wise) axis=1 means ‚Äúgo across the columns‚Äù (operate row-wise)</p> <h2 id="core-data-structures">Core Data Structures</h2> <h3 id="series">Series</h3> <p>A one-dimensional labeled array holding data of any type such as integers, strings, Python objects etc.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="sh">'</span><span class="s">c</span><span class="sh">'</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <h3 id="dataframe">DataFrame</h3> <p>A two-dimensional data structure that holds data like a two-dimension array or a table with rows and columns. Think of it like a spreadsheet or SQL table.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">Name</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">Alice</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Bob</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Charlie</span><span class="sh">'</span><span class="p">],</span>
        <span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">28</span><span class="p">],</span>
        <span class="sh">'</span><span class="s">City</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">New York</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">London</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Paris</span><span class="sh">'</span><span class="p">]}</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div> <h2 id="reading-and-writing-data">Reading and Writing Data</h2> <p>Pandas can read and write data from various formats like CSV, Excel, JSON, SQL databases, and more.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Reading from CSV
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">data.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Writing to CSV
</span><span class="n">df</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">output.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># index=False prevents writing row indices
</span>
<span class="c1"># Other formats:
# pd.read_excel('data.xlsx')
# pd.read_json('data.json')
# ...
</span></code></pre></div></div> <h2 id="accessing-data">Accessing Data</h2> <h3 id="columns">Columns</h3> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">names</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">Name</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># Access the 'Name' column as a Series
</span><span class="n">ages</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">Age</span>       <span class="c1"># Alternative way to access a column
</span></code></pre></div></div> <h3 id="rows">Rows</h3> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">first_row</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Access the first row by label
</span><span class="n">second_row</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># Access the second row by integer position
</span></code></pre></div></div> <h3 id="slicing">Slicing</h3> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">subset</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>   <span class="c1"># Rows 2 and 3
</span></code></pre></div></div> <h2 id="data-manipulation">Data Manipulation</h2> <h3 id="filtering">Filtering:</h3> <p><code class="language-plaintext highlighter-rouge">young_people = df[df['Age'] &lt; 30]</code></p> <h3 id="sorting">Sorting:</h3> <p><code class="language-plaintext highlighter-rouge">df_sorted = df.sort_values(by='Age', ascending=False)</code></p> <h3 id="adding-columns">Adding Columns:</h3> <p><code class="language-plaintext highlighter-rouge">df['NewColumn'] = df['Age'] * 2</code></p> <h3 id="deleting-columns">deleting Columns:</h3> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">DataFrame</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="sh">'</span><span class="s">raise</span><span class="sh">'</span><span class="p">)[</span><span class="n">source</span><span class="p">]</span>
</code></pre></div></div> <p>Remove rows or columns by specifying label names and corresponding axis, or by directly specifying index or column names.</p> <h3 id="applying-functions">Applying Functions:</h3> <p><code class="language-plaintext highlighter-rouge">df['NameLength'] = df['Name'].apply(len)</code></p> <h3 id="grouping">Grouping:</h3> <p><code class="language-plaintext highlighter-rouge">grouped = df.groupby('City')['Age'].mean()</code></p> <h3 id="combining-dataframes">Combining DataFrames:</h3> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">merged_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="sh">'</span><span class="s">KeyColumn</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># Merge two DataFrames based on a common column
</span><span class="n">concatenated_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">])</span>        <span class="c1"># Concatenate DataFrames
</span></code></pre></div></div> <h2 id="handling-missing-data">Handling Missing Data</h2> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="nf">dropna</span><span class="p">()</span>       <span class="c1"># Remove rows with missing values
</span><span class="n">df</span><span class="p">.</span><span class="nf">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>      <span class="c1"># Fill missing values with 0
</span></code></pre></div></div> <h2 id="dataframe-describe">DataFrame describe</h2> <p>Understand the return from describe()</p> <table> <thead> <tr> <th>Statistic</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">count</code></td> <td>Number of non-missing values</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">mean</code></td> <td>Average value</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">std</code></td> <td>Standard deviation (spread)</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">min</code></td> <td>Minimum value</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">25%</code></td> <td>1st quartile (25th percentile)</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">50%</code></td> <td>2nd quartile = <strong>median</strong></td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">75%</code></td> <td>3rd quartile (75th percentile)</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">max</code></td> <td>Maximum value</td> </tr> </tbody> </table> <h4 id="mean-vs-50">mean vs. 50%</h4> <table> <thead> <tr> <th>Feature</th> <th><strong>Mean</strong></th> <th><strong>Median (50%)</strong></th> </tr> </thead> <tbody> <tr> <td>What it is</td> <td>The <strong>average</strong> of all values</td> <td>The <strong>middle</strong> value when sorted</td> </tr> <tr> <td>Formula</td> <td>Sum of values √∑ number of values</td> <td>Middle value (or average of two middle ones)</td> </tr> <tr> <td>Sensitive to outliers?</td> <td>‚úÖ Yes ‚Äì pulled by extreme values</td> <td>‚ùå No ‚Äì more resistant to outliers</td> </tr> <tr> <td>Good for</td> <td>Symmetrical distributions</td> <td>Skewed distributions (with outliers)</td> </tr> </tbody> </table> <ul> <li><code class="language-plaintext highlighter-rouge">Mean</code>: looks at the arithmetic average ‚Äî good for clean, normal data</li> <li><code class="language-plaintext highlighter-rouge">50%</code> (median): finds the middle value ‚Äî better for messy or skewed data. The <code class="language-plaintext highlighter-rouge">median</code> value tell that half the data is below this number, half above.</li> <li><code class="language-plaintext highlighter-rouge">25%</code> value is the 25% item value (or average of two ones)!</li> </ul> <h2 id="dataframe-sample">DataFrame sample</h2> <p><code class="language-plaintext highlighter-rouge">DataFrame.sample(n=None, frac=None, replace=False, weights=None, random_state=None, axis=None, ignore_index=False)</code>:</p> <p>Return a random sample of items from an axis of object.</p> <ul> <li><code class="language-plaintext highlighter-rouge">n</code>: Number of items from axis to return.</li> <li><code class="language-plaintext highlighter-rouge">frac</code>: Fraction of axis items to return. Cannot be used with <code class="language-plaintext highlighter-rouge">n</code>.</li> <li><code class="language-plaintext highlighter-rouge">axis</code>: Default is row for DataFrame type.</li> <li><code class="language-plaintext highlighter-rouge">random_state</code>: is used to control the randomness of an operation so that you get the same result every time you run your code. Just like a seed value passed to the internal random number generator. For example, by setting <code class="language-plaintext highlighter-rouge">random_state</code> (e.g., to 42), you ensure the same output every time your code runs.</li> </ul> <h2 id="references">References</h2> <ul> <li><a href="https://pandas.pydata.org/docs">Pandas website</a></li> </ul>]]></content><author><name></name></author><category term="Python"/><category term="ML"/><category term="Python"/><summary type="html"><![CDATA[Pandas is a powerful Python library for data manipulation and analysis.]]></summary></entry><entry><title type="html">Core concepts behind ML</title><link href="https://benwzj.github.io/blog/2025/ml-core-concept/" rel="alternate" type="text/html" title="Core concepts behind ML"/><published>2025-05-14T00:00:00+00:00</published><updated>2025-05-14T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/ml-core-concept</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/ml-core-concept/"><![CDATA[<p>ML is the <strong>process</strong> of training a piece of software, called a model, to make useful predictions or generate content (like text, images, audio, or video) from data.</p> <h2 id="types-of-ml-systems">Types of ML Systems</h2> <p>ML systems fall into one or more of the following categories based on how they learn to make predictions or generate content:</p> <ul> <li>Supervised learning</li> <li>Unsupervised learning</li> <li>Reinforcement learning</li> <li>Generative AI</li> </ul> <h3 id="supervised-learning">Supervised learning</h3> <p>Supervised learning models can make predictions after seeing lots of data with the correct answers and then discovering the connections between the elements in the data that produce the correct answers.</p> <p>Supervised ML models are trained using datasets with labeled examples.</p> <p>Two of the most common use cases for supervised learning are regression and classification.</p> <h4 id="regression">Regression</h4> <p>A regression model predicts <strong>a numeric value</strong>. For example, a weather model that predicts the amount of rain, in inches or millimeters, is a regression model.</p> <h4 id="classification">Classification</h4> <p>Classification models predict the likelihood that something belongs to a category.</p> <p>Classification models are divided into two groups: binary classification and multiclass classification.</p> <h3 id="unsupervised-learning">Unsupervised learning</h3> <p>Unsupervised learning models make predictions by being given data that does not contain any correct answers. An unsupervised learning model‚Äôs goal is to identify meaningful patterns among the data. In other words, the model has no hints on how to categorize each piece of data, but instead it must infer its own rules.</p> <p>A commonly used unsupervised learning model employs a technique called clustering. The model finds data points that demarcate natural groupings.</p> <p>Clustering differs from classification because the categories aren‚Äôt defined by you.</p> <h3 id="reinforcement-learning">Reinforcement learning</h3> <p>Reinforcement learning models make predictions by getting rewards or penalties based on actions performed within an environment. A reinforcement learning system generates a policy that defines the best strategy for getting the most rewards.</p> <p>Reinforcement learning is used to train robots to perform tasks, like walking around a room, and software programs like AlphaGo to play the game of Go.</p> <h3 id="generative-ai">Generative AI</h3> <p>Generative AI is a class of models that creates content from user input.</p> <p>At a high-level, generative models learn patterns in data with the goal to produce new but similar data. Generative models are like the following:</p> <ul> <li>Comedians who learn to imitate others by observing people‚Äôs behaviors and style of speaking</li> <li>Artists who learn to paint in a particular style by studying lots of paintings in that style</li> <li>Cover bands that learn to sound like a specific music group by listening to lots of music by that group</li> </ul> <p>To produce unique and creative outputs, generative models are initially trained using an unsupervised approach, where the model learns to mimic the data it‚Äôs trained on. The model is sometimes trained further using supervised or reinforcement learning on specific data related to tasks the model might be asked to perform, for example, summarize an article or edit a photo.</p> <h2 id="ml-model-types">ML Model types</h2> <p>Linear Regression Logistic Regression Classification</p>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[ML is the process of training a piece of software, called a model, to make useful predictions or generate content (like text, images, audio, or video) from data.]]></summary></entry><entry><title type="html">Embedding</title><link href="https://benwzj.github.io/blog/2025/embedding/" rel="alternate" type="text/html" title="Embedding"/><published>2025-05-06T00:00:00+00:00</published><updated>2025-05-06T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/embedding</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/embedding/"><![CDATA[<p>Embeddings is the basic key point to understand Machine Learning. And it is Machine Learning‚Äôs Most Useful Multitool.</p> <p>Embeddings are a powerful technique in AI, enabling machines to understand and work with complex data in a more meaningful way. They play a crucial role in various applications, from natural language processing to computer vision and recommendation systems.</p> <p>In machine learning, an embedding is a way of representing data as points in n-dimensional space so that similar data points cluster together.</p> <h2 id="what-are-embeddings">What are Embeddings</h2> <p>Abstractively, embeddings allow us to find similar data points. The machine can understand things because of embedding. Realistically, embeddings store semantically meaning in arrays of number.</p> <p>What kinds of things can be embedded? All The Things! Text, Images, Videos, Music.</p> <p>Embeddings are typically learned from large datasets using machine learning techniques.</p> <h3 id="understand-embeddings">Understand Embeddings</h3> <p>When talk about embeddings, i though it is just lots of array of numbers which telling about the things. But how to define this dimensions? what algorithm to operate this dimensions? Embedding should be a whole system, it including the start and the end. it include algorithms, data training, etc. It can embed your input into arrays, it can understand those arrays, it can capture semantic relationships, and it can inference.</p> <p>So Creating embeddings starts with a large dataset. For example word embeddings, this is a text corpus. We define a model, often a <strong>shallow neural network</strong> like in Word2Vec. Word2Vec‚Äôs skip-gram architecture, for instance, predicts surrounding context words given a target word. The model‚Äôs hidden layer weights become the word embeddings.</p> <h2 id="word-embedding">Word Embedding</h2> <p>Word embeddings are extremely useful in natural language processing. They can be used to find synonyms (‚Äúsemantic similarity‚Äù), to do clustering, or as a preprocessing step for a more complicated nlp model.</p> <p>Example: Imagine the words ‚Äúcat,‚Äù ‚Äúdog,‚Äù and ‚Äúcar.‚Äù Their embeddings might look like this (simplified):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat: [0.8, 0.2, 0.1]
dog: [0.7, 0.3, 0.2]
car: [0.1, 0.1, 0.9]
</code></pre></div></div> <p>Notice that ‚Äúcat‚Äù and ‚Äúdog‚Äù have similar embeddings, reflecting their semantic similarity, while ‚Äúcar‚Äù has a very different embedding.</p> <h2 id="pre-trained-models">Pre-trained Models</h2> <p>This pre-trained models also called Techniques which are used to create embeddings.</p> <p>A pre-trained embedding model is a model that has already been trained on a large dataset and can be used to generate embeddings for new data without further training (or with minimal fine-tuning). Think of it as a ready-to-use embedding generator.</p> <h3 id="some-famous-ones-are">Some famous ones are:</h3> <ul> <li>Word2Vec: a technique invented by Google in 2013. It Learns word embeddings by predicting surrounding words given a target word (or vice versa).</li> <li>GloVe (Global Vectors for Word Representation): Learns word embeddings by capturing global word co-occurrence statistics.</li> <li>FastText: An extension of Word2Vec that considers subword information, allowing it to generate embeddings for out-of-vocabulary words.</li> <li>Sentence Transformers: Generate embeddings for entire sentences or paragraphs.</li> <li>Graph Embeddings: Represent nodes in a graph as vectors.</li> </ul> <h3 id="how-to-use-a-pre-trained-embedding-model">How to use a pre-trained embedding model:</h3> <ul> <li>Choose a Model: Select a pre-trained model that suits your task and data. Popular choices include Word2Vec, GloVe, FastText, and sentence transformers like BERT. Consider factors like the size of the vocabulary, the dimensionality of the embeddings, and the type of data the model was trained on.</li> <li>Load the Model: Use a library like <code class="language-plaintext highlighter-rouge">Gensim</code> or <code class="language-plaintext highlighter-rouge">Hugging Face Transformers</code> to load the pre-trained model.</li> <li>Generate Embeddings: Input your data (e.g., words, sentences) into the loaded model to generate embeddings.</li> <li>Use the Embeddings: Use the generated embeddings as input features for your machine learning model or for other tasks like semantic search.</li> </ul> <p>Let‚Äôs say you want to build a sentiment analysis model. You could use pre-trained word embeddings from <strong>GloVe</strong>. You would load the GloVe model, then for each word in your input text, you would retrieve its corresponding pre-trained embedding vector. These vectors would then be used as input features for your sentiment analysis model.</p> <h3 id="example">Example</h3> <p>Using <code class="language-plaintext highlighter-rouge">google/universal-sentence-encoder</code> pre-trained model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow_hub</span> <span class="k">as</span> <span class="n">hub</span>

<span class="n">embed</span> <span class="o">=</span> <span class="n">hub</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">https://tfhub.dev/google/universal-sentence-encoder/4</span><span class="sh">"</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="nf">embed</span><span class="p">([</span>
    <span class="sh">"</span><span class="s">The quick brown fox jumps over the lazy dog.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">I am a sentence for which I would like to get its embedding</span><span class="sh">"</span><span class="p">])</span>

<span class="nf">print</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

<span class="c1"># Response looks like: [[0.001, 0.201, ...]]
# i.e., an array of vectors
</span></code></pre></div></div> <h2 id="what-is-word2vec-exactly">What is Word2vec exactly</h2> <p>Word2Vec is categorize as embedding model while BERT as a Language Model. Word2Vec provide methods, algorithms, neural network for embedding purpose. BERT can work to embedding, also can do much more various NLP tasks like generating contextualized word and sentence embeddings.</p> <h3 id="technique">Technique</h3> <p>You can say Word2vec is a technique. This technique is used for obtaining vector representations of words in natural language processing (NLP). These vectors capture information about the meaning of the word based on the surrounding words.</p> <h3 id="model">Model</h3> <p>You can say Word2vec is a group of models.<br/> Word2vec is composed of a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words.</p> <h3 id="how-word2vec-approach">How Word2vec approach</h3> <p>Word2vec takes as its input a large corpus of text and produces a mapping of the set of words to a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a vector in the space. (Word2vec Â∞ÜÂ§ßÈáèÊñáÊú¨‰Ωú‰∏∫ËæìÂÖ•ÔºåÂπ∂Â∞ÜËØçÈõÜÊò†Â∞ÑÂà∞ÂêëÈáèÁ©∫Èó¥ÔºàÈÄöÂ∏∏ÊúâÂá†ÁôæÁª¥ÔºâÔºåËØ≠ÊñôÂ∫ì‰∏≠ÊØè‰∏™ÂîØ‰∏ÄÁöÑËØçÈÉΩ‰ºöÂú®Á©∫Èó¥‰∏≠ÂàÜÈÖç‰∏Ä‰∏™ÂêëÈáè„ÄÇ)</p> <p>Here are a little bit more detail: Word2vec can use either of two model architectures to produce these distributed representations of words: continuous bag of words (CBOW) or continuously sliding skip-gram. In both architectures, word2vec considers both individual words and a sliding context window as it iterates over the corpus.</p> <ul> <li>The CBOW can be viewed as a ‚Äòfill in the blank‚Äô task.</li> <li>In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words.</li> <li>CBOW is faster while skip-gram does a better job for infrequent words.</li> <li>A corpus is a sequence of words. Both CBOW and skip-gram are methods to learn one vector per word appearing in the corpus.</li> </ul> <h3 id="how-to-use-word2vec">How to use Word2Vec</h3> <p>Using Word2Vec involves two main steps: training the model (optional, as pre-trained models are available) and then using the trained model to generate word embeddings.</p> <h4 id="training-optional">Training (Optional)</h4> <p>This is optional, as pre-trained models are available. But you can still train it with your own data.</p> <h4 id="using-the-model">Using the Model</h4> <p>Load the Model:</p> <pre><code class="language-Python">from gensim.models import Word2Vec
model = Word2Vec.load("word2vec.model") # Or load a pre-trained model
</code></pre> <p>Get Word Embeddings:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vector</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">[</span><span class="sh">'</span><span class="s">cat</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># Get embedding for 'cat'
</span></code></pre></div></div> <p>Find Similar Words:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">similar_words</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="nf">most_similar</span><span class="p">(</span><span class="sh">'</span><span class="s">cat</span><span class="sh">'</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># Find the 5 most similar words to 'cat'
</span></code></pre></div></div> <p>Perform Word Arithmetic (Analogy):</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="nf">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">king</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">woman</span><span class="sh">'</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">man</span><span class="sh">'</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># king - man + woman = ? (queen)
</span></code></pre></div></div> <p>Use in Downstream Tasks: Use the embeddings as features in machine learning models for tasks like:</p> <ul> <li>Text Classification: Sentiment analysis, spam detection.</li> <li>Machine Translation: Encoding and decoding text in different languages.</li> <li>Information Retrieval: Semantic search.</li> <li>Recommendation Systems: Recommending similar items.</li> </ul> <h2 id="create-your-own-embeddings">Create Your Own Embeddings</h2> <p>Usually, when we are talking about Creating Our Own Embeddings, usually refer to fine-tuning some pre-trained model.</p> <p>Actually, you can creat one from scratch. Or saying train one. Here‚Äôs what you‚Äôll need:</p> <ul> <li>Familiarity with Python and a deep learning framework like TensorFlow or PyTorch is <strong>essential</strong>.</li> <li>Data: A sufficiently large and relevant dataset is crucial.</li> <li>Algorithm: Choose an appropriate embedding algorithm. Word2Vec, GloVe, and FastText are good starting points for word embeddings.</li> <li>Computational Resources Access to a decent CPU, and ideally a GPU, will significantly speed up the process.</li> <li>Patience and Experimentation: requires patience and a willingness to experiment with different hyperparameters and architectures to achieve optimal results.</li> </ul> <p>Here are the highly simplified example:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Sample vocabulary and data
</span><span class="n">vocabulary</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">the</span><span class="sh">"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">cat</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">"</span><span class="s">sat</span><span class="sh">"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="sh">"</span><span class="s">on</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="sh">"</span><span class="s">mat</span><span class="sh">"</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="sh">"</span><span class="s">the</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">cat</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sat</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">on</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">the</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">mat</span><span class="sh">"</span><span class="p">]]</span>

<span class="c1"># Hyperparameters
</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">window_size</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Initialize embeddings randomly
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>

<span class="c1"># Training loop (simplified)
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>  <span class="c1"># Example number of epochs
</span>    <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)):</span>
            <span class="n">target_word</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">context_words</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">[</span><span class="nf">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">window_size</span><span class="p">):</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">sentence</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="nf">min</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">),</span> <span class="n">i</span> <span class="o">+</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>

            <span class="c1"># Simplified training logic (replace with actual gradient updates)
</span>            <span class="k">for</span> <span class="n">context_word</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">:</span>
                <span class="n">target_index</span> <span class="o">=</span> <span class="n">vocabulary</span><span class="p">[</span><span class="n">target_word</span><span class="p">]</span>
                <span class="n">context_index</span> <span class="o">=</span> <span class="n">vocabulary</span><span class="p">[</span><span class="n">context_word</span><span class="p">]</span>
                <span class="c1"># ... (Calculate gradients and update embeddings) ...
</span>
<span class="nf">print</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span> <span class="c1"># Your trained embeddings
</span></code></pre></div></div> <p>But A real implementation, it would involve things like</p> <ul> <li>A neural network: For predicting context words.</li> <li>Backpropagation: For calculating gradients.</li> <li>An optimization algorithm: Like SGD for updating embeddings.</li> </ul> <h2 id="faq">FAQ</h2> <ul> <li>When talking embedding, is it refer to an array of number?</li> <li>Who define the dimensions?</li> <li>Word2Vec is a pre-trained model or just a Techniques which difine demensions?</li> <li>How to create our own embedding model?</li> <li>What is the difference between text and image embedding?</li> <li>What is the relationship of Word2vec and Transformer?</li> <li>Can say BERT is a embedding model?</li> </ul>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="Embedding"/><category term="Vector"/><category term="ML"/><summary type="html"><![CDATA[Embeddings is the basic key point to understand Machine Learning. And it is Machine Learning‚Äôs Most Useful Multitool.]]></summary></entry><entry><title type="html">What is Transformer</title><link href="https://benwzj.github.io/blog/2025/transformer/" rel="alternate" type="text/html" title="What is Transformer"/><published>2025-05-05T00:00:00+00:00</published><updated>2025-05-05T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/transformer</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/transformer/"><![CDATA[<h2 id="what-is-transformer">What is Transformer</h2> <p>Transformer is a type of neural network architecture.</p> <ul> <li>Transformers were initially designed for translation, superseded RNN.</li> <li>Unlike traditional recurrent or convolutional models that process data sequentially, the Transformer leverages a mechanism called <strong>self-attention</strong> to process all input data simultaneously. This allows for much greater parallelization, leading to faster training and the ability to handle longer sequences of data effectively.</li> <li>Transformers are widely used in various fields, including natural language processing (NLP) for tasks like translation, text generation, and question answering, as well as computer vision.</li> <li>In essence, transformer models have revolutionized the way we process and understand sequential data by leveraging the power of attention and parallel processing.</li> <li>Like GPT(Generative Pre-trained Transformer), BERT(Bidirectional Encoder Representations from Transformers), they are based on Transformers.</li> <li>Self-Attention and Positional Encoding are the main innovations.</li> </ul> <h2 id="key-concepts">Key Concepts</h2> <ul> <li>Self-Attention: The core innovation of Transformers. It allows the model to weigh the importance of different parts of the input when generating an output. For example, in a sentence, the word ‚Äúit‚Äù might refer to different things depending on the context. Self-attention helps the model understand these relationships. It does this by calculating relationships between every word in a sequence and every other word, creating a weighted representation of the input.</li> <li>Attention Mechanism: A more general concept that allows the model to focus on specific parts of the input when generating an output. Self-attention is a specific type of attention.</li> <li>Encoder-Decoder Architecture: Many Transformers follow this structure. The encoder processes the input sequence and generates a contextualized representation. The decoder then uses this representation to generate the output sequence.</li> <li>Parallelization: Unlike recurrent networks that process input sequentially, Transformers can process all input tokens simultaneously, significantly speeding up training.</li> <li>Positional Encoding: Because Transformers don‚Äôt process sequentially, positional information of words in a sentence is lost. Positional encodings are added to the input embeddings to provide information about the position of each word.</li> <li>Feedforward Networks: Fully connected layers within each encoder and decoder layer that further process the information from the attention mechanism.</li> <li>Layer Normalization: A normalization technique used to stabilize training and improve performance.</li> </ul> <h2 id="how-a-transformer-works-simplified">How a Transformer works (simplified)</h2> <ul> <li>Input Embedding: The input sequence (e.g., a sentence) is converted into numerical representations called embeddings.</li> <li>Positional Encoding: Positional information is added to the embeddings.</li> <li>Encoder: Multiple encoder layers process the embeddings using self-attention and feedforward networks. Each encoder layer produces a set of encoded representations.</li> <li>Decoder: The decoder takes the encoded representations from the encoder and, using self-attention and feedforward networks, generates the output sequence (e.g., a translation, a summary, or the next word in a sentence). The decoder also uses attention mechanisms to focus on relevant parts of the encoded input.</li> <li>Output: The final decoder layer produces the output.</li> </ul> <h2 id="why-are-transformers-important">Why are Transformers important</h2> <p>Improved Performance: They have achieved state-of-the-art results in various NLP tasks. Parallelization: They train much faster than recurrent models. Handling Long Sequences: They can effectively process long sequences of data.</p> <h2 id="rnn">RNN</h2> <p>A recurrent neural network (RNN) is a type of neural network architecture specifically designed to process <strong>sequential</strong> data. it have many problems. Like:</p> <ul> <li>it struggle to learn long-range dependencies.</li> <li>Because sequential, it can‚Äôt be parallelized training. it is very slow.</li> </ul> <p>RNNs have been largely superseded by Transformer networks.</p>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="Transformer"/><summary type="html"><![CDATA[What is Transformer]]></summary></entry></feed>
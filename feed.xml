<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://benwzj.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://benwzj.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-02T04:28:38+00:00</updated><id>https://benwzj.github.io/feed.xml</id><title type="html">BEN WEN</title><subtitle>A website to show the world of Ben Wen </subtitle><entry><title type="html">Basic Math concepts in Machine Learning</title><link href="https://benwzj.github.io/blog/2025/math-in-ml/" rel="alternate" type="text/html" title="Basic Math concepts in Machine Learning"/><published>2025-06-18T00:00:00+00:00</published><updated>2025-06-18T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/math-in-ml</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/math-in-ml/"><![CDATA[<h2 id="exponent">Exponent</h2> <table> <thead> <tr> <th>Expression</th> <th>Meaning</th> <th>Result</th> <th>Explanation</th> </tr> </thead> <tbody> <tr> <td>\(2^3\)</td> <td>\(2 \times 2 \times 2\)</td> <td>8</td> <td>Multiply 2 three times</td> </tr> <tr> <td>\(5^2\)</td> <td>\(5 \times 5\)</td> <td>25</td> <td>Square of 5</td> </tr> <tr> <td>\(10^0\)</td> <td>‚Äî</td> <td>1</td> <td>Any non-zero number to the 0 power is 1</td> </tr> <tr> <td>\(2^{-3}\)</td> <td>\(\frac{1}{2^3} = \frac{1}{8}\)</td> <td>0.125</td> <td>Negative = reciprocal</td> </tr> <tr> <td>\(4^{-1}\)</td> <td>\(\frac{1}{4}\)</td> <td>0.25</td> <td>Negative exponent = 1 over base</td> </tr> <tr> <td>\(9^{1/2}\)</td> <td>\(\sqrt{9}\)</td> <td>3</td> <td>Fractional = root</td> </tr> <tr> <td>\(27^{1/3}\)</td> <td>\(\sqrt[3]{27}\)</td> <td>3</td> <td>Cube root</td> </tr> <tr> <td>\(16^{3/4}\)</td> <td>\(\left(\sqrt[4]{16}\right)^3\)</td> <td>8</td> <td>Root first, then power</td> </tr> </tbody> </table> <h3 id="negative-exponents">Negative Exponents</h3> <p>Represents the <strong>reciprocal</strong> (1 divided by the base raised to a positive power) For example: \(2^{-3}\) = 0.125</p> <h3 id="fractional-exponents">Fractional Exponents</h3> <p>Represents <strong>roots</strong>. For example Cube root: \(\sqrt[3]{27}\) = 3</p> <h2 id="logarithm">Logarithm</h2> <p>A logarithm is the inverse of an exponent.</p> \[\log_b(y) = x \quad \text{means} \quad b^x = y\] <ul> <li><code class="language-plaintext highlighter-rouge">b</code> = base</li> <li><code class="language-plaintext highlighter-rouge">x</code> = exponent</li> <li><code class="language-plaintext highlighter-rouge">y</code> = result</li> </ul> <p>For Example:</p> \[\log_2(8) = 3 \quad \text{because} \quad 2^3 = 8\] <h3 id="common-logarithm-types">Common Logarithm Types</h3> <table> <thead> <tr> <th>Notation</th> <th>Name</th> <th>Base</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">log</code></td> <td>Common logarithm</td> <td>10</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">ln</code></td> <td>Natural logarithm</td> <td>e ‚âà 2.718</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">log‚ÇÇ</code>, <code class="language-plaintext highlighter-rouge">log‚ÇÅ‚ÇÄ</code>, etc.</td> <td>Custom base logarithm</td> <td>any base</td> </tr> </tbody> </table> <h3 id="root-vs-logarithm">root vs logarithm</h3> <ul> <li>A root asks: ‚ÄúWhat number, when raised to a certain power (like 2 or 3), gives this result?‚Äù</li> <li>A logarithm asks: ‚ÄúTo what power must I raise a base to get this number?‚Äù</li> </ul> <table> <thead> <tr> <th>Concept</th> <th>Root (‚àö)</th> <th>Logarithm (log)</th> </tr> </thead> <tbody> <tr> <td>Question it asks</td> <td>What number to raise to <strong>n</strong>?</td> <td>What <strong>power</strong> gives this number?</td> </tr> <tr> <td>Example</td> <td>\(\sqrt{16} = 4 ‚Üí 4^2 = 16\)</td> <td>\(\log_2(16) = 4 ‚Üí 2^4 = 16\)</td> </tr> <tr> <td>Fixed part</td> <td>The <strong>power</strong> (e.g., square = 2)</td> <td>The <strong>base</strong> (e.g., base 2 or base 10)</td> </tr> <tr> <td>Output</td> <td>The <strong>number</strong> itself</td> <td>The <strong>exponent</strong></td> </tr> </tbody> </table> <ul> <li>A root finds the <strong>base</strong>, given the result and the power</li> <li>A logarithm finds the <strong>exponent</strong>, given the base and the result</li> </ul> <h2 id="logistic-function">Logistic function</h2> <p>There‚Äôs a family of functions called logistic functions whose output represents a probability, always outputting a value between 0 and 1. The standard logistic function, also known as the sigmoid function (sigmoid means ‚Äús-shaped‚Äù), has the formula:</p> \[f(x) = \frac{1}{1 + e^{-x}}\] <ul> <li>The ‚Äòe‚Äô is Euler‚Äôs number, a fundamental mathematical constant. <code class="language-plaintext highlighter-rouge">e‚âà2.71828...</code></li> <li>Logistic functions is one of The most important exponential function.</li> </ul> <p>Here are the classic corresponding graph of the sigmoid function:</p> <figure> <picture> <img src="/assets/img/sigmoid_function_with_axes.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>As the input, x, increases, the output of the sigmoid function approaches but never reaches 1.</li> <li>Similarly, as the input decreases, the sigmoid function‚Äôs output approaches but never reaches 0.</li> <li>The sigmoid function will bend the linear equation straight line into an s-shape.</li> </ul> <h2 id="quantile">Quantile</h2> <p>What Is a Quantile? A quantile is a statistical term that refers to dividing a dataset into equal-sized intervals based on the values in the data. Quantiles help us understand the distribution of data and where values fall in relation to the whole set.</p> <p>‚úÖ Basic Idea Quantiles split data into parts of equal probability. For example:</p> <ul> <li>If you divide data into 4 parts, you get quartiles</li> <li>If you divide it into 100 parts, you get percentiles</li> <li>If you divide it into 10 parts, you get deciles</li> </ul>]]></content><author><name></name></author><category term="AI"/><category term="Python"/><category term="ML"/><summary type="html"><![CDATA[Exponent]]></summary></entry><entry><title type="html">Handle Dataset in ML</title><link href="https://benwzj.github.io/blog/2025/ml-data/" rel="alternate" type="text/html" title="Handle Dataset in ML"/><published>2025-06-16T00:00:00+00:00</published><updated>2025-06-16T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/ml-data</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/ml-data/"><![CDATA[<p>ML practitioners spend far more time evaluating, cleaning, and transforming data than building models. Here introduce how to handle dataset before training a model.</p> <p>We devide data into tow types:</p> <ul> <li>numerical data</li> <li>categorical data</li> </ul> <h2 id="numerical-data">Numerical Data</h2> <p>This unit focuses on numerical data, meaning integers or floating-point values that behave like numbers. That is, they are additive, countable, ordered, and so on.</p> <h3 id="feature-vector">Feature Vector</h3> <p>The feature vector is input during training and during inference. A feature Vector is an array of feature values comprising an example.</p> <p>Feature Vectors seldom use the dataset‚Äôs raw values. Instead, you must typically process the dataset‚Äôs values into representations that your model can better learn from. This process is called feature engineering.</p> <p>Every value in a feature vector must be a <strong>floating-point</strong> value. However, many features are naturally strings or other non-numerical values. Consequently, a large part of feature engineering is representing non-numerical values as numerical values.</p> <p>The most common feature engineering techniques are:</p> <ul> <li>Normalization: Converting numerical values into a standard range.</li> <li>Binning (also referred to as bucketing): Converting numerical values into buckets of ranges.</li> </ul> <h3 id="overview-data">Overview Data</h3> <h4 id="visualize-your-data">Visualize your data</h4> <p>Visualizations help you continually check your assumptions. Use Pandas for visualization:</p> <ul> <li>Working with Missing Data: <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html">pandas Documentation</a></li> <li>Visualizations <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html">pandas Documentation</a></li> </ul> <h4 id="statistically-evaluate-your-data">Statistically evaluate your data</h4> <p>Use Pandas <code class="language-plaintext highlighter-rouge">describe()</code></p> <h4 id="find-outliers">Find outliers</h4> <p>There are some common rule to find outliers. For example: When the delta between the 0th and 25th percentiles differs significantly from the delta between the 75th and 100th percentiles, the dataset probably contains outliers.</p> <ul> <li>The outlier can be due to a mistake.</li> <li>The outlier is a legitimate data point, you can keep these outliers in your training set. But extreme outliers can still hurt your model. You can delete the outliers or apply more invasive feature engineering techniques, such as clipping.</li> </ul> <h3 id="feature-engineering-technique">Feature engineering technique</h3> <h4 id="normalization">Normalization</h4> <p>The goal of normalization is to transform features to be on a similar scale. Normalization methods:</p> <ul> <li>linear scaling</li> <li>Z-score scaling</li> <li>log scaling</li> <li>Clipping</li> </ul> <h4 id="binning">Binning</h4> <p>Binning (also called bucketing) is a feature engineering technique that groups different numerical subranges into bins or buckets. In many cases, binning turns numerical data into categorical data.</p> <p>When a feature appears more clumpy than linear, binning is a much better way to represent the data. When using Binning, the model can learn separate weights for each bin.</p> <p>Binning is a good alternative to scaling or clipping when either of the following conditions is met:</p> <ul> <li>The overall linear relationship between the feature and the label is weak or nonexistent.</li> <li>When the feature values are clustered.</li> </ul> <p>Binning example: Suppose you are creating a model that predicts the number of shoppers by the outside temperature for that day. The number of shoppers was highest when the temperature was most comfortable. So, the plot doesn‚Äôt really show any sort of linear relationship between the label and the feature value.</p> <figure> <picture> <img src="/assets/img/binning_temperature_vs_shoppers_divided_into_3_bins.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The graph suggests three clusters in the following subranges:</p> <ul> <li>Bin 1 is the temperature range 4-11.</li> <li>Bin 2 is the temperature range 12-26.</li> <li>Bin 3 is the temperature range 27-36.</li> </ul> <p>The model learns separate weights for each bin.</p> <h5 id="quantile-bucketing">Quantile bucketing</h5> <p><strong>Quantile bucketing</strong> creates bucketing boundaries such that the number of examples in each bucket is exactly or nearly equal.</p> <h4 id="scrubbing">Scrubbing</h4> <p>Many examples in datasets are unreliable. You can write a program or script to detect any of the following problems:</p> <ul> <li>Omitted values</li> <li>Duplicate examples</li> <li>Out-of-range feature values</li> </ul> <p>This is Scrubbing.</p> <h4 id="qualities-of-good-numerical-features">Qualities of good numerical features</h4> <ul> <li>Clearly named: Not recommended: <code class="language-plaintext highlighter-rouge">house_age: 851472000</code>; Recommended: <code class="language-plaintext highlighter-rouge">house_age_years: 27</code></li> <li>Checked or tested before training: Bad <code class="language-plaintext highlighter-rouge">user_age_in_years: 224</code>; OK <code class="language-plaintext highlighter-rouge">user_age_in_years: 24</code></li> <li>Sensible: A ‚Äúmagic value‚Äù is a purposeful discontinuity in an otherwise continuous feature.</li> </ul> <h4 id="polynomial-transforms">Polynomial transforms</h4> <p>A Polynomial Transform is a technique used to map your original input features into a higher-dimensional space by generating polynomial combinations of the original features.</p> <h3 id="numerical-data-best-practices">Numerical data Best practices</h3> <p>Best practices for working with numerical data:</p> <ul> <li>Remember that your ML model interacts with the data in the feature vector, not the data in the dataset.</li> <li>Normalize most numerical features.</li> <li>If your first normalization strategy doesn‚Äôt succeed, consider a different way to normalize your data.</li> <li>Binning, also referred to as bucketing, is sometimes better than normalizing.</li> <li>Considering what your data should look like, write verification tests to validate those expectations. For example: <ul> <li>The absolute value of latitude should never exceed 90. You can write a test to check if a latitude value greater than 90 appears in your data.</li> <li>If your data is restricted to the state of Florida, you can write tests to check that the latitudes fall between 24 through 31, inclusive.</li> </ul> </li> <li>Visualize your data with scatter plots and histograms. Look for anomalies.</li> <li>Gather statistics not only on the entire dataset but also on smaller subsets of the dataset. That‚Äôs because aggregate statistics sometimes obscure problems in smaller sections of a dataset.</li> <li>Document all your data transformations.</li> </ul> <h2 id="categorical-data">Categorical Data</h2> <p>Categorical Data is the Data which behave like categories. The numerical data can be Categorical Data. Because models can only train on floating-point values, Categorical Data need to be <strong>Encoded</strong>.</p> <h3 id="vocabulary-and-one-hot-encoding">Vocabulary and one-hot encoding</h3> <p>When a categorical feature has a low number of possible categories, you can encode it as a <strong>vocabulary</strong>. With a vocabulary encoding, the model treats each possible categorical value as a separate feature. During training, the model learns different weights for each category.</p> <h4 id="index-numbers">Index numbers</h4> <p>First you must convert each category string to a unique index number.</p> <h4 id="one-hot-encoding">One-hot encoding</h4> <p>The next step in building a vocabulary is to convert each index number to its one-hot encoding.</p> <p>In a one-hot encoding:</p> <ul> <li>Each category is represented by a vector (array) of N elements, where N is the number of categories. For example, if car_color has eight possible categories, then the one-hot vector representing will have eight elements.</li> <li>Exactly one of the elements in a one-hot vector has the value 1.0; all the remaining elements have the value 0.0.</li> </ul> <blockquote class="block-warning"> <p>It is the one-hot vector, not the string or the index number, that gets passed to the feature vector. The model learns a separate weight for each element of the feature vector.</p> </blockquote> <h4 id="sparse-representation">sparse representation</h4> <p>Sparse representation means storing the position of the 1.0 in a sparse vector. For example, the one-hot vector for ‚ÄúBlue‚Äù is: <code class="language-plaintext highlighter-rouge">[0, 0, 1, 0, 0, 0, 0, 0]</code> Since the <code class="language-plaintext highlighter-rouge">1</code> is in position <code class="language-plaintext highlighter-rouge">2</code> (when starting the count at 0), the sparse representation for the preceding one-hot vector is: <code class="language-plaintext highlighter-rouge">2</code></p> <p>Sparse representation consumes far less memory than the eight-element one-hot vector.</p> <blockquote class="block-warning"> <p>Importantly, the model must train on the one-hot vector, not the sparse representation.</p> </blockquote> <h3 id="encoding-high-dimensional-categorical-features">Encoding high-dimensional categorical features</h3> <p>Some categorical features have a high number of dimensions, like <code class="language-plaintext highlighter-rouge">words_in_english</code>.</p> <p>When the number of categories is high, one-hot encoding is usually a bad choice.</p> <ul> <li>Embeddings are usually a much better choice. Embeddings substantially reduce the number of dimensions, which benefits models in two important ways: <ul> <li>The model typically trains faster.</li> <li>The built model typically infers predictions more quickly. That is, the model has lower latency.</li> </ul> </li> <li>Hashing (also called the hashing trick) is a less common way to reduce the number of dimensions.</li> </ul>]]></content><author><name></name></author><category term="AI"/><category term="Python"/><category term="ML"/><summary type="html"><![CDATA[ML practitioners spend far more time evaluating, cleaning, and transforming data than building models. Here introduce how to handle dataset before training a model.]]></summary></entry><entry><title type="html">Important Concepts in ML</title><link href="https://benwzj.github.io/blog/2025/ml-concepts/" rel="alternate" type="text/html" title="Important Concepts in ML"/><published>2025-06-09T00:00:00+00:00</published><updated>2025-06-09T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/ml-concepts</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/ml-concepts/"><![CDATA[<h2 id="standard-deviation">Standard Deviation</h2> <p>Standard deviation measures how spread out the values in a dataset are from the mean (average).</p> <h3 id="example">Example:</h3> <p>Let‚Äôs say you have test scores from two classes:</p> <p>Class A: <code class="language-plaintext highlighter-rouge">[78, 80, 82, 81, 79] ‚Üí Avg = 80</code> Standard deviation is low: everyone scored close to the mean.</p> <p>Class B: <code class="language-plaintext highlighter-rouge">[50, 60, 80, 95, 100] ‚Üí Avg = 77</code> Standard deviation is high: big spread from the mean.</p> <h3 id="formula-simplified">Formula (simplified):</h3> <p>For a dataset with values x1, x2,‚Ä¶,xn:</p> \[\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2}\] <ul> <li>\(\bar{x}\) is the mean.</li> <li>\(x_i\) are the data values.</li> <li>The expression inside the square root is called the variance</li> </ul> <h2 id="data-distribution">Data Distribution</h2> <p>Understanding data distribution is key in statistics, data analysis, and machine learning. Data distribution describes how values in a dataset are spread out or arranged. It tells you:</p> <ul> <li>Which values occur most often</li> <li>How the values are grouped</li> <li>Whether the data is symmetrical, skewed, or has outliers</li> </ul> <h3 id="example-1">Example:</h3> <p>Imagine test scores from a class:</p> <p><code class="language-plaintext highlighter-rouge">[70, 72, 75, 75, 76, 78, 80, 85, 90, 100]</code></p> <p>You can see most scores are around 75‚Äì85, with one outlier (100). That pattern of how often each score occurs is its distribution.</p> <h3 id="random-module-example">Random module Example:</h3> <p>Use Random to analog Data Distribution, it is a list of all possible values, which shows how often each value occurs.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">numpy</span> <span class="kn">import</span> <span class="n">random</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">p</code> is the Data Distribution. The sum of all probability numbers should be 1. Here the <code class="language-plaintext highlighter-rouge">9</code> never show up because of probability is 0.</p> <p>Data Distribution Concepts:</p> <ul> <li><strong>Frequency</strong>: How often each value appears</li> <li><strong>Mean (average)</strong>: Where the center of the data is</li> <li><strong>Spread</strong>: How far the values are from each other</li> <li><strong>Shape</strong>: Overall pattern (normal, skewed, etc.)</li> </ul> <p>Data Distribution helps answer: üìå What‚Äôs common? üìå What‚Äôs rare? üìå How is it spread out?</p> <p>Such lists are important when working with statistics and data science. The random module offer methods that returns randomly generated data distributions.</p> <h3 id="numpy-offers-many-distribution-functions">NumPy Offers Many Distribution Functions</h3> <p>Why we need different distribution? Different types of randomness behave differently in nature. Each distribution models a specific type of uncertainty, For example:</p> <table> <thead> <tr> <th>Distribution</th> <th>Common Use Case</th> </tr> </thead> <tbody> <tr> <td><strong>Uniform</strong></td> <td>Equal chance outcomes (e.g., rolling a die)</td> </tr> <tr> <td><strong>Normal (Gaussian)</strong></td> <td>Natural measurements (e.g., height, test scores)</td> </tr> <tr> <td><strong>Binomial</strong></td> <td>Repeated yes/no trials (e.g., coin flips)</td> </tr> <tr> <td><strong>Poisson</strong></td> <td>Counting rare events (e.g., calls per hour)</td> </tr> <tr> <td><strong>Exponential</strong></td> <td>Time between events (e.g., waiting time)</td> </tr> <tr> <td><strong>Beta / Gamma / Chi-squared</strong></td> <td>Advanced statistical modeling</td> </tr> </tbody> </table> <h4 id="example-using-normal-distribution---human-heights">Example: Using Normal Distribution - Human Heights</h4> <p>Because Human traits like height follow a bell-shaped (normal) distribution. So use normal distribution:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Simulate 1000 people's heights (mean=170cm, std=10cm)
</span><span class="n">heights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">170</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">skyblue</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Simulated Human Heights (Normal Distribution)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Height (cm)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Frequency</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div> <h4 id="example-binomial-distribution--coin-tosses">Example: Binomial Distribution ‚Äì Coin Tosses</h4> <p>Models ‚Äúyes/no‚Äù outcomes like coin tosses, quiz answers, etc.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simulate flipping a fair coin 10 times, repeat 1000 experiments
</span><span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">orange</span><span class="sh">'</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="sh">'</span><span class="s">left</span><span class="sh">'</span><span class="p">,</span> <span class="n">rwidth</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">10 Coin Flips per Trial (Binomial Distribution)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Number of Heads</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Frequency</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h4 id="exponential-distribution--wait-times">Exponential Distribution ‚Äì Wait Times</h4> <p>Time between events like arrivals, failures, or clicks.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simulate wait times between buses (mean wait = 10 minutes)
</span><span class="n">wait_times</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">exponential</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">wait_times</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">purple</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Wait Time Between Buses (Exponential Distribution)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Minutes</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Frequency</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h4 id="poisson-distribution--call-center-events">Poisson Distribution ‚Äì Call Center Events</h4> <p>Models how often rare events happen in a fixed time (calls, emails, etc.)</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simulate number of calls per minute (avg = 3 calls/min)
</span><span class="n">calls</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">poisson</span><span class="p">(</span><span class="n">lam</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">calls</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="sh">'</span><span class="s">left</span><span class="sh">'</span><span class="p">,</span> <span class="n">rwidth</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Number of Calls Per Minute (Poisson Distribution)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Calls</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Frequency</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h2 id="normalize-data">Normalize data</h2> <p>When creating a model with multiple features, the values of each feature should span roughly the same range. If one feature‚Äôs values range from 500 to 100,000 and another feature‚Äôs values range from 2 to 12, the model will need to have weights of extremely low or extremely high values to be able to combine these features effectively. This could result in a low quality model. To avoid this, normalize features in a multi-feature model.</p> <p>The goal of normalization is to transform features to be on a similar scale.</p> <p>Three popular normalization methods:</p> <ul> <li>linear scaling</li> <li>Z-score scaling</li> <li>log scaling</li> </ul> <blockquote class="block-warning"> <p>The distribution of data should decide which method is going to be used.</p> </blockquote> <ul> <li>The technique of Normalize data: Clipping</li> </ul> <h3 id="linear-scaling">Linear scaling</h3> <p>Linear scaling (more commonly shortened to just scaling) means converting floating-point values from their natural range into a standard range‚Äîusually 0 to 1 or -1 to +1.</p> <p>Linear scaling is a good choice when all of the following conditions are met:</p> <ul> <li>Stable range:The lower and upper bounds of your data don‚Äôt change much over time.</li> <li>No Outliers: The feature contains few or no outliers, and those outliers aren‚Äôt extreme.</li> <li><strong>uniformly distributed</strong>: The feature is approximately uniformly distributed across its range. That is, a histogram would show roughly even bars for most values.</li> </ul> <p>Here are the examples:</p> <ul> <li>If human age is a feature, Linear scaling is a good normalization technique for age. because: <ul> <li>lower and upper bounds are 0 to 100.</li> <li>age contains a relatively small percentage of outliers. Only about 0.3% of the population is over 100.</li> <li></li> </ul> </li> <li>if net_worth is a feature that holds the net worth of different people. Linear scaling would be a poor choice, because: <ul> <li>This feature contains many outliers.</li> <li>the values are not uniformly distributed across its primary range. Most people would be squeezed within a very narrow band of the overall range.</li> </ul> </li> </ul> <h3 id="z-score-scaling">Z-score scaling</h3> <blockquote> <p>The Z-score for a given value is how many standard deviations away from the mean the value is.</p> </blockquote> <p>Consider a feature with a <strong>mean</strong> of 60 and a <strong>standard deviation</strong> of 10.</p> <ul> <li>The raw value 75 would have a Z-score of +1.5: <code class="language-plaintext highlighter-rouge">Z-score = (75 - 60) / 10 = +1.5</code></li> <li>The raw value 38 would have a Z-score of -2.2: <code class="language-plaintext highlighter-rouge">Z-score = (38 - 60) / 10 = -2.2</code></li> </ul> <p>Z-score is a good choice when the data follows a <strong>normal distribution</strong> or a distribution somewhat like a normal distribution.</p> <blockquote> <p>What is normal distribution? a classic normal distribution:</p> <ul> <li>At least 68.27% of data has a Z-score between -1.0 and +1.0.</li> <li>At least 95.45% of data has a Z-score between -2.0 and +2.0.</li> <li>At least 99.73% of data has a Z-score between -3.0 and +3.0.</li> <li>At least 99.994% of data has a Z-score between -4.0 and +4.0.</li> </ul> </blockquote> <p>Suppose your model trains on a feature named height that holds the adult heights of ten million women. Z-score scaling is a good normalization technique. Because this feature conforms to a normal distribution.</p> <h3 id="log-scaling">Log scaling</h3> <p>Log scaling computes the logarithm of the raw value. In practice, log scaling usually calculates the natural logarithm (ln).</p> <p>Log scaling is helpful when the data conforms to a <strong>power law distribution</strong>. Casually speaking, a power law distribution looks as follows:</p> <ul> <li>Low values of X have very high values of Y.</li> <li>As the values of X increase, the values of Y quickly decrease. Consequently, high values of X have very low values of Y.</li> </ul> <h4 id="understand-log-scaling">Understand Log scaling</h4> <p>Book sales conform to a power law distribution because:</p> <ul> <li>Most published books sell a tiny number of copies, maybe one or two hundred.</li> <li>Some books sell a moderate number of copies, in the thousands.</li> <li>Only a few bestsellers will sell more than a million copies.</li> </ul> <p>Suppose you are training a linear model to find the relationship of, say, book covers to book sales. A linear model training on raw values would have to find something about book covers on books that sell a million copies that is 10,000 more powerful than book covers that sell only 100 copies. However, log scaling all the sales figures makes the task far more feasible. For example,</p> <ul> <li>the log of 100 is:<code class="language-plaintext highlighter-rouge">~4.6 = ln(100)</code></li> <li>while the log of 1,000,000 is:<code class="language-plaintext highlighter-rouge">~13.8 = ln(1,000,000)</code></li> </ul> <p>So, the log of 1,000,000 is only about three times larger than the log of 100. You probably could imagine a bestseller book cover being about three times more powerful (in some way) than a tiny-selling book cover.</p> <p>Log scaling changes the distribution, which helps train a model that will make better predictions.</p> <h3 id="clipping">Clipping</h3> <p>Clipping is a technique to minimize the influence of extreme outliers.</p> <blockquote> <p>In brief, clipping usually caps (reduces) the value of outliers to a specific maximum value.</p> </blockquote> <p>Clipping is a strange idea, and yet, it can be very effective.</p> <p>You can also clip values after applying other forms of normalization.</p> <p>Clipping prevents your model from overindexing on unimportant data. However, some outliers are actually important, so clip values carefully.</p> <h3 id="references">References</h3> <ul> <li><a href="https://developers.google.com/machine-learning/crash-course/numerical-data/normalization">Google doc</a></li> </ul> <h2 id="polynomial-transforms">Polynomial transforms</h2> <p>A Polynomial Transform is a technique used to map your original input features into a higher-dimensional space by generating polynomial combinations of the original features.</p> <p>Sometimes, when the ML practitioner has domain knowledge suggesting that one variable is related to the square, cube, or other power of another variable, it‚Äôs useful to create a synthetic feature from one of the existing numerical features.</p> <p>Sometime, it‚Äôs not possible to draw a straight line that cleanly separates the two classes, but it is possible to draw a curve that does so:</p> <figure> <picture> <img src="/assets/img/ft_cross1.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Gradient descent finds the weight (or weights <code class="language-plaintext highlighter-rouge">w1</code>, <code class="language-plaintext highlighter-rouge">w2</code>, <code class="language-plaintext highlighter-rouge">w3</code>, in the case of additional features) that minimizes the loss of the model. But the data points shown cannot be separated by a line. What can be done?</p> <p>It‚Äôs possible to keep both the linear equation and allow nonlinearity by defining a new term, <code class="language-plaintext highlighter-rouge">x2</code>, that is simply <code class="language-plaintext highlighter-rouge">x1</code> squared:</p> \[x_2 = x_1^2\] <p>This synthetic feature, called a <strong>polynomial transform</strong>, is treated like any other feature. The previous linear formula becomes:</p> \[y = b + w_1x_1 + w_2x_2\] <p>This can still be treated like a linear regression problem, and the weights determined through gradient descent, as usual, despite containing a hidden squared term, the polynomial transform.</p> <blockquote> <p>Usually the numerical feature of interest is multiplied by itself, that is, raised to some power. Sometimes an ML practitioner can make an informed guess about the appropriate exponent. For example, many relationships in the physical world are related to squared terms, including acceleration due to gravity, the attenuation of light or sound over distance, and elastic potential energy.</p> </blockquote> <h2 id="label-leakage">Label Leakage</h2> <p>It‚Äôs important to prevent the model from getting the label as input during training, which is called label leakage.</p> <h3 id="why">Why?</h3> <p>If you include the label in your input features, the model <strong>cheats</strong> ‚Äî it learns the answer directly rather than learning to predict it from the actual data.</p> <h3 id="example-2">Example</h3> <p>Imagine you‚Äôre training a model to predict house prices, and you accidentally include the actual sale price (price) as a feature:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">location</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">size</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">price</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># üö´ WRONG
</span><span class="n">target</span> <span class="o">=</span> <span class="sh">'</span><span class="s">price</span><span class="sh">'</span>
</code></pre></div></div> <p>The model would just learn to copy the price column instead of learning how location and size affect price.</p> <h3 id="training-a-model-to-predict-dog-or-cat">training a model to predict dog or cat</h3> <p>When training a model to predict something (like ‚Äúdog or cat‚Äù), you must provide both:</p> <ul> <li>Inputs: the images themselves (the pixel data ‚Äî e.g. arrays, tensors, etc.)</li> <li>Labels: the correct answer (e.g. ‚Äúdog‚Äù or ‚Äúcat‚Äù) ‚Äî this is what the model tries to learn to predict</li> </ul> <p>In this case, You do provide labels during training, but You do not include the label as part of the <strong>input</strong> features(like the image containing ‚Äòdog‚Äô letters).</p> <p>Let‚Äôs say you have:</p> <ul> <li>Image 1: üê∂ (dog) ‚Üí label: 0</li> <li>Image 2: üê± (cat) ‚Üí label: 1</li> </ul> <p>Training Code (simplified):</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">image_1</span><span class="p">,</span> <span class="n">image_2</span><span class="p">,</span> <span class="p">...]</span>   <span class="c1"># Input: pixel data only
</span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">...]</span>               <span class="c1"># Target: labels (dog = 0, cat = 1)
</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="AI"/><category term="Python"/><category term="ML"/><category term="Data-Distribution"/><summary type="html"><![CDATA[Standard Deviation]]></summary></entry><entry><title type="html">What is Keras</title><link href="https://benwzj.github.io/blog/2025/Keras/" rel="alternate" type="text/html" title="What is Keras"/><published>2025-06-07T00:00:00+00:00</published><updated>2025-06-07T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/Keras</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/Keras/"><![CDATA[<h2 id="what-is-keras">What is Keras</h2> <p>Keras is an open-source, <strong>high-level</strong> deep learning API written in Python. It is designed to simplify the process of building and training neural networks. Keras acts as a user-friendly interface for complex machine learning libraries like TensorFlow, Theano, or CNTK, though today it is fully integrated into TensorFlow 2.x.</p> <p>Keras allows developers and researchers to build deep learning models with just a few lines of code. It supports common neural network layers (dense, convolutional, recurrent, etc.) and techniques like dropout, batch normalization, and activation functions. Its modular design makes it easy to build sequential models (a stack of layers) or more complex models using its Functional API.</p> <p>Keras has Built-in tools for model training, evaluation, and prediction.</p> <h2 id="example">Example</h2> <h3 id="creates-a-simple-feedforward-neural-network">Creates a simple feedforward neural network</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">from</span> <span class="n">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="n">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">([</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,)),</span>
    <span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div> <p>This example creates a simple feedforward neural network.</p> <h3 id="set_random_seed-function">set_random_seed Function</h3> <p><code class="language-plaintext highlighter-rouge">keras.utils.set_random_seed(42)</code> is used to make your machine learning experiment <strong>reproducible</strong> by setting a random seed.</p> <p>üîç What It Does: It sets the random seed for:</p> <ul> <li>NumPy</li> <li>Python‚Äôs built-in random module</li> <li>TensorFlow</li> </ul> <p>This ensures that all random operations (like weight initialization, data shuffling, dropout, etc.) behave the same way every time you run the code.</p> <p>üîí Why Use It? Machine learning involves randomness ‚Äî for example:</p> <ul> <li>Initializing model weights</li> <li>Shuffling data during training</li> <li>Splitting datasets randomly</li> </ul> <p>Without a fixed seed, you‚Äôll get slightly different results each time. Setting the seed makes results reproducible, which is critical for:</p> <ul> <li>Debugging</li> <li>Experiment comparison</li> <li>Scientific reliability</li> </ul> <p>üß† What is 42? The number 42 is just a commonly used seed value (a nod to The Hitchhiker‚Äôs Guide to the Galaxy). You can use any integer.</p> <p>‚úÖ Summary: Ensures consistent behavior across runs by fixing the random seed used by Keras/TensorFlow and related libraries.</p>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[What is Keras]]></summary></entry><entry><title type="html">Random in Coding</title><link href="https://benwzj.github.io/blog/2025/py-random/" rel="alternate" type="text/html" title="Random in Coding"/><published>2025-06-07T00:00:00+00:00</published><updated>2025-06-07T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/py-random</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/py-random/"><![CDATA[<h2 id="about-random-number">About Random Number</h2> <p>Random number does NOT mean a different number every time. Random means something that can not be predicted logically.</p> <h3 id="pseudo-random-and-true-random">Pseudo Random and True Random.</h3> <p>Computers work on programs, and programs are definitive set of instructions. So it means there must be some algorithm to generate a random number as well.</p> <p>Random numbers generated through a generation algorithm are called <strong>pseudo random</strong>. It can be predicted.</p> <p>In order to generate a truly random number on our computers we need to get the random data from some outside source. This outside source is generally our keystrokes, mouse movements, data on network etc.</p> <p>We do not need truly random numbers, unless it is related to security (e.g. encryption keys) or the basis of application is the randomness (e.g. Digital roulette wheels).</p> <h2 id="how-languages-implement-randomness">How languages implement randomness</h2> <p>Most programming languages (like Python, Java, C++) implement pseudo-random number generators (PRNGs).</p> <ul> <li>PRNGs are algorithms that produce sequences of numbers that appear random, but are completely determined by an initial value called the <strong>seed</strong>.</li> <li>Once you set a seed, the sequence of random numbers is repeatable.</li> </ul> <h3 id="algorithm-behind-prngs">Algorithm Behind PRNGs</h3> <table> <thead> <tr> <th>Algorithm</th> <th>Used In</th> </tr> </thead> <tbody> <tr> <td>Linear Congruential Generator (LCG)</td> <td>C, early Java, Python (older versions)</td> </tr> <tr> <td>Mersenne Twister</td> <td>Python <code class="language-plaintext highlighter-rouge">random</code>, NumPy (default before v1.17)</td> </tr> <tr> <td>PCG (Permuted Congruential Generator)</td> <td><code class="language-plaintext highlighter-rouge">numpy.random.default_rng()</code></td> </tr> <tr> <td>Xoroshiro/Xoshiro</td> <td>Modern C/C++ libraries and game engines</td> </tr> </tbody> </table> <h3 id="seed">Seed</h3> <p>Seed: Controlling the Sequence Setting a seed ensures repeatability (useful for debugging, testing, reproducibility). Without a seed, most languages auto-generate one using something like the current time or system entropy.</p> <h3 id="state">State</h3> <p>A pseudo-random number generator (PRNG) doesn‚Äôt truly generate random numbers, it produces a long, predictable sequence of numbers based on:</p> <ul> <li>A starting point (seed)</li> <li>An internal state that evolves as numbers are generated</li> </ul> <p>So, the state is like a snapshot of the PRNG‚Äôs memory at a moment in time. If you save it, you can pause and resume the sequence exactly where you left off.</p> <h3 id="true-randomness-trngs">True Randomness (TRNGs)</h3> <p>Most of time we only need PRNG, but some applications (e.g. cryptography) need true randomness: Collected from physical sources like:</p> <ul> <li>Mouse movement</li> <li>Atmospheric noise</li> <li>Hardware random number generators</li> </ul> <h2 id="python-random-module">Python Random module</h2> <p>Python Random module implements pseudo-random number generators for various distributions. (Other Languea, Library will do smiliar things for Random)</p> <ul> <li>For <strong>integers</strong>, there is uniform selection from a range.</li> <li>For <strong>sequences</strong>, there is uniform selection of a random element, a function to generate a random permutation of a list in-place, and a function for random sampling without replacement.</li> <li>Almost all module functions depend on the basic function <code class="language-plaintext highlighter-rouge">random()</code>, which generates a random float uniformly in the half-open range 0.0 &lt;= X &lt; 1.0.</li> <li>Python uses the <strong>Mersenne Twister</strong> as the core generator.</li> <li>It produces 53-bit precision floats and has a period of 2**19937-1.</li> <li>It is completely unsuitable for cryptographic purposes.</li> <li>For security or cryptographic uses, see the secrets module.</li> <li>The functions supplied by this module are actually bound methods of a hidden instance of the <code class="language-plaintext highlighter-rouge">random.Random</code> class. You can instantiate your own instances of Random to get generators that don‚Äôt share state.</li> </ul> <h3 id="bookkeeping">Bookkeeping</h3> <h4 id="randomseedanone-version2"><code class="language-plaintext highlighter-rouge">random.seed(a=None, version=2)</code></h4> <p>Initialize the random number generator.</p> <ul> <li>If <code class="language-plaintext highlighter-rouge">a</code> is omitted or None, the current system time is used.</li> <li>If <code class="language-plaintext highlighter-rouge">a</code> is an int, it is used directly.</li> </ul> <h4 id="randomgetstate"><code class="language-plaintext highlighter-rouge">random.getstate()</code></h4> <p>Return an object capturing the current internal state of the generator. This object can be passed to <code class="language-plaintext highlighter-rouge">setstate()</code> to restore the state.</p> <h4 id="randomsetstatestate"><code class="language-plaintext highlighter-rouge">random.setstate(state)</code></h4> <p>It can restore the state for the random generator.</p> <h3 id="for-sequences-random-selection">For Sequences random selection</h3> <h4 id="randomchoiceseq">random.choice(seq)</h4> <p>Return a random element from the non-empty sequence <code class="language-plaintext highlighter-rouge">seq</code>.</p> <h4 id="randomshufflex">random.shuffle(x)</h4> <p>Shuffle the sequence <code class="language-plaintext highlighter-rouge">x</code> in place. (please note that, it will mutate the sequence). To shuffle an immutable sequence and return a new shuffled list, use <code class="language-plaintext highlighter-rouge">sample(x, k=len(x))</code> instead.</p> <h4 id="randomsamplepopulation-k--countsnone">random.sample(population, k, *, counts=None)</h4> <p>Return a <code class="language-plaintext highlighter-rouge">k</code> length, new list of unique elements chosen from the <code class="language-plaintext highlighter-rouge">population</code> sequence or set. Used for random sampling without replacement.</p> <h2 id="numpy-random-module">NumPy Random module</h2> <p>Python built-in random module vs. numpy.random</p> <table> <thead> <tr> <th>Feature</th> <th><code class="language-plaintext highlighter-rouge">random</code> (Python built-in)</th> <th><code class="language-plaintext highlighter-rouge">numpy.random</code> (NumPy)</th> </tr> </thead> <tbody> <tr> <td><strong>Library</strong></td> <td>Built-in (<code class="language-plaintext highlighter-rouge">import random</code>)</td> <td>Requires NumPy (<code class="language-plaintext highlighter-rouge">import numpy as np</code>)</td> </tr> <tr> <td><strong>Speed</strong></td> <td>Slower</td> <td>Much faster (optimized with C under the hood)</td> </tr> <tr> <td><strong>Data Types</strong></td> <td>Works with basic types (int, float)</td> <td>Works with NumPy arrays</td> </tr> <tr> <td><strong>Best For</strong></td> <td>Simple randomness (games, small apps)</td> <td>Scientific computing, machine learning</td> </tr> <tr> <td><strong>Random Arrays</strong></td> <td>Not supported</td> <td>Easily generates arrays of random numbers</td> </tr> <tr> <td><strong>Distributions</strong></td> <td>Basic (uniform, normal)</td> <td>Extensive (normal, binomial, Poisson, etc.)</td> </tr> <tr> <td><strong>Seeding</strong></td> <td><code class="language-plaintext highlighter-rouge">random.seed()</code></td> <td><code class="language-plaintext highlighter-rouge">np.random.seed()</code> or <code class="language-plaintext highlighter-rouge">np.random.default_rng()</code></td> </tr> <tr> <td><strong>Modern Interface</strong></td> <td>Basic only</td> <td><code class="language-plaintext highlighter-rouge">np.random.default_rng()</code> (recommended in NumPy ‚â• 1.17)</td> </tr> </tbody> </table> <p>Python random:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">random</span>
<span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>  <span class="c1"># Random int between 1 and 10
</span></code></pre></div></div> <p>NumPy random:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">default_rng</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">rng</span><span class="p">.</span><span class="nf">integers</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">))</span>    <span class="c1"># Same, but faster and better control
</span></code></pre></div></div>]]></content><author><name></name></author><category term="Python"/><category term="Python"/><category term="ML"/><category term="Data-Distribution"/><summary type="html"><![CDATA[About Random Number]]></summary></entry><entry><title type="html">Logistic Regression</title><link href="https://benwzj.github.io/blog/2025/logistic-regresion/" rel="alternate" type="text/html" title="Logistic Regression"/><published>2025-05-22T00:00:00+00:00</published><updated>2025-05-22T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/logistic-regresion</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/logistic-regresion/"><![CDATA[<h2 id="what-is-logistic-regression">What is Logistic Regression</h2> <p>Logistic regression is designed to predict the probability of a given outcome.</p> <p>Classic example is spam-prediction model.</p> <p>Simply saying, Logistic regression = Logistic function + linear regression.</p> <h2 id="sigmoid-function">Sigmoid function</h2> <p>There‚Äôs a family of functions called logistic functions whose output represents a probability, always outputting a value between 0 and 1. The standard logistic function, also known as the sigmoid function (sigmoid means ‚Äús-shaped‚Äù), has the formula:</p> \[f(x) = \frac{1}{1 + e^{-x}}\] <ul> <li>The ‚Äòe‚Äô is Euler‚Äôs number, a fundamental mathematical constant. <code class="language-plaintext highlighter-rouge">e‚âà2.71828...</code></li> <li>Logistic functions is one of The most important exponential function.</li> </ul> <p>Here are the classic corresponding graph of the sigmoid function:</p> <figure> <picture> <img src="/assets/img/sigmoid_function_with_axes.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>As the input, x, increases, the output of the sigmoid function approaches but never reaches 1.</li> <li>Similarly, as the input decreases, the sigmoid function‚Äôs output approaches but never reaches 0.</li> <li>The sigmoid function will bend the linear equation straight line into an s-shape.</li> </ul> <h3 id="transforming-linear-output-using-the-sigmoid-function">Transforming linear output using the sigmoid function</h3> <p>Left: graph of the linear function z = 2x + 5, with three points highlighted. Right: Sigmoid curve with the same three points highlighted after being transformed by the sigmoid function:</p> <figure> <picture> <img src="/assets/img/linear_to_logistic.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="logistic-regression-trainning">Logistic regression trainning</h2> <p>Logistic regression models are trained using the same process as linear regression models, with two key distinctions:</p> <ul> <li>Logistic regression models use Log Loss as the loss function instead of squared loss.</li> <li>Applying regularization is critical to prevent overfitting.</li> </ul> <h3 id="log-loss">Log Loss</h3> <p>In the Linear regression module, you used squared loss (also called L2 loss) as the loss function. loss function for logistic regression is Log Loss.</p> <figure> <picture> <img src="/assets/img/logloss-func.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="regularization">Regularization</h3> <p>Regularization, a mechanism for penalizing model complexity during training.</p> <p>Regularization is extremely important in logistic regression modeling. Without regularization, the asymptotic nature of logistic regression would keep driving loss towards 0 in cases where the model has a large number of features.</p> <p>Consequently, most logistic regression models use one of the following two strategies to decrease model complexity:</p> <ul> <li>L2 regularization</li> <li>Early stopping: Limiting the number of training steps to halt training while loss is still decreasing.</li> </ul>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[What is Logistic Regression]]></summary></entry><entry><title type="html">Classification in ML</title><link href="https://benwzj.github.io/blog/2025/classification/" rel="alternate" type="text/html" title="Classification in ML"/><published>2025-05-22T00:00:00+00:00</published><updated>2025-05-22T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/classification</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/classification/"><![CDATA[<p>Classification is the task of predicting which of a set of classes (categories) an example belongs to. Classification is converting a <strong>logistic regression model</strong> that predicts a probability into a binary classification model that predicts one of two classes.</p> <p>There are some terms you need to be understood and remembered:</p> <ul> <li>TP, FP, TN, FN</li> <li>Threshold</li> <li>Accuracy</li> <li>Recall, or true positive rate</li> <li>False positive rate</li> <li>Precision</li> </ul> <h2 id="threshold">Threshold</h2> <p>Choosing a threshold is very important for Classification. You can use tool like Confusion matrix to understand more about threshold in your model.</p> <h3 id="confusion-matrix">Confusion matrix</h3> <table> <thead> <tr> <th>¬†</th> <th>Actual positive</th> <th>Actual negative</th> </tr> </thead> <tbody> <tr> <td>Predicted positive</td> <td>True positive (TP): A spam email correctly classified as a spam email. These are the spam messages automatically sent to the spam folder.</td> <td>False positive (FP): A not-spam email misclassified as spam. These are the legitimate emails that wind up in the spam folder.</td> </tr> <tr> <td>Predicted negative</td> <td>False negative (FN): A spam email misclassified as not-spam. These are spam emails that aren‚Äôt caught by the spam filter and make their way into the inbox.</td> <td>True negative (TN): A not-spam email correctly classified as not-spam. These are the legitimate emails that are sent directly to the inbox.</td> </tr> </tbody> </table> <p>For example it can look like this:</p> <figure> <picture> <img src="/assets/img/confusion-matrix.png" class="img-fluid rounded z-depth-1" width="50%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>TP = it is Spam and labled as Spam FP = ir is not spam but labled as spam TN = it is not spam and labled as not spam FN = it is spam but labled as not spam</p> <p>When the classification threshold increases, both true and false positives decrease. Becuase the model will likely predict fewer positives overall.</p> <h2 id="measure-model-metrics">Measure Model Metrics</h2> <p>They are Accuracy, TPR, FPR, Precision.</p> <p>TPR and FPR will be used more in Measurement.</p> <h3 id="accuracy">Accuracy</h3> <p>Accuracy is the proportion of all classifications that were correct, whether positive or negative. It is mathematically defined as:</p> \[\text{Accuracy} = \frac{\text{correct classifications}}{\text{total classifications}} = \frac{TP + TN}{TP + TN + FP + FN}\] <h3 id="recall-or-true-positive-rate">Recall, or true positive rate</h3> <p>The true positive rate (TPR), or the proportion of all actual positives that were classified correctly as positives, is also known as recall.</p> <p>Recall is mathematically defined as:</p> \[\text{Recall (or TPR)} = \frac{\text{correctly classified actual positives}}{\text{all actual positives}} = \frac{TP}{TP + FN}\] <p>A hypothetical perfect model would have zero false negatives and therefore a recall (TPR) of 1.0, which is to say, a 100% detection rate.</p> <h3 id="false-positive-rate">False positive rate</h3> <p>The false positive rate (FPR) is the proportion of all actual negatives that were classified incorrectly as positives, also known as the probability of false alarm. It is mathematically defined as:</p> \[FPR = \frac{\text{incorrectly classified actual negatives}}{\text{all actual negatives}} = \frac{FP}{FP + TN}\] <p>A perfect model would have zero false positives and therefore a FPR of 0.0, which is to say, a 0% false alarm rate.</p> <h3 id="precision">Precision</h3> <p>Precision is the proportion of all the model‚Äôs positive classifications that are actually positive. It is mathematically defined as:</p> \[\text{Precision} = \frac{\text{correctly classified actual positives}}{\text{everything classified as positive}} = \frac{TP}{TP + FP}\] <p>Precision improves as false positives decrease, while recall improves when false negatives decrease.</p> <h3 id="choice-of-metric-and-tradeoffs">Choice of metric and tradeoffs</h3> <table> <thead> <tr> <th>Metric</th> <th>Guidance</th> </tr> </thead> <tbody> <tr> <td>Accuracy</td> <td>Use as a rough indicator of model training progress/convergence for balanced datasets. <br/> For model performance, use only in combination with other metrics. <br/> Avoid for imbalanced datasets. Consider using another metric.</td> </tr> <tr> <td>Recall (True positive rate)</td> <td>Use when false negatives are more expensive than false positives.</td> </tr> <tr> <td>False positive rate</td> <td>Use when false positives are more expensive than false negatives.</td> </tr> <tr> <td>Precision</td> <td>Use when it‚Äôs very important for positive predictions to be accurate.</td> </tr> </tbody> </table> <p>Accoring to my understanding, Spam email model should use FPR matric, because I can‚Äôt accept labling legitimate email as spam.</p> <h2 id="roc-and-auc">ROC and AUC</h2> <h3 id="roc">ROC</h3> <p>ROC curve, short for Receiver-operating characteristic curve. The long version of the name, receiver operating characteristic, is a holdover from WWII radar detection.</p> <p>The ROC curve is a visual representation of model performance across all thresholds.</p> <p>The ROC curve is drawn by calculating the true positive rate (TPR) and false positive rate (FPR) at every possible threshold (in practice, at selected intervals), then graphing TPR over FPR.</p> <h3 id="auc">AUC</h3> <p>AUC, short for Area under the curve.</p> <p>The area under the ROC curve (AUC) represents the probability that the model, if given a randomly chosen positive and negative example, will rank the positive higher than the negative.</p> <p>AUC is a useful measure for comparing the performance of two different models, as long as the dataset is roughly balanced. The model with greater area under the curve is generally the better one.</p> <h4 id="understand-auc">Understand AUC</h4> <p>For a binary classifier, a model that does exactly as well as random guesses or coin flips has a ROC that is a diagonal line from (0,0) to (1,1). The AUC is 0.5, representing a 50% probability of correctly ranking a random positive and negative example.</p> <h3 id="auc-and-roc-for-choosing-model-and-threshold">AUC and ROC for choosing model and threshold</h3> <p>AUC is a useful measure for comparing the performance of two different models, as long as the dataset is roughly balanced. The model with greater area under the curve is generally the better one.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/auc_0-65.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/auc_0-93.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The points on a ROC curve closest to (0,1) represent a range of the best-performing thresholds for the given model.</p> <figure> <picture> <img src="/assets/img/auc_abc.png" class="img-fluid rounded z-depth-1" width="50%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>If false positives (false alarms) are highly costly, it may make sense to choose a threshold that gives a lower FPR, like the one at point A, even if TPR is reduced. Conversely, if false positives are cheap and false negatives (missed true positives) highly costly, the threshold for point C, which maximizes TPR, may be preferable. If the costs are roughly equivalent, point B may offer the best balance between TPR and FPR.</p> <p>Understand AUC and ROC:</p> <figure> <picture> <img src="/assets/img/AUC.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="some-other-concepts">Some other concepts</h2> <h3 id="prediction-bias">Prediction bias</h3> <p>Prediction bias is the difference between the mean of a model‚Äôs predictions and the mean of ground-truth labels in the data. A model trained on a dataset where 5% of the emails are spam should predict, on average, that 5% of the emails it classifies are spam. In other words, the mean of the labels in the ground-truth dataset is 0.05, and the mean of the model‚Äôs predictions should also be 0.05. If this is the case, the model has zero prediction bias. Of course, the model might still have other problems.</p> <h3 id="multi-class-classification">Multi-class classification</h3> <p>Multi-class classification can be treated as an extension of binary classification to more than two classes.</p> <p>For example, in a three-class multi-class classification problem, where you‚Äôre classifying examples with the labels A, B, and C, you could turn the problem into two separate binary classification problems. First, you might create a binary classifier that categorizes examples using the label A+B and the label C. Then, you could create a second binary classifier that reclassifies the examples that are labeled A+B using the label A and the label B.</p> <h2 id="conclusion">Conclusion</h2> <h3 id="clear-tp-tn-fp-fn-terms">Clear TP, TN, FP, FN terms.</h3> <p>All these terms are connected to threshold. When threshold change, they are changed. For example, when threshold increase, FN should increase.</p> <h3 id="the-core-metrics-are-tpr-and-fpr">The core Metrics are TPR and FPR.</h3> \[\text{TPR} = \frac{TP}{\text{all actual positives}}\] <p>Use when false negatives are more expensive than false positives.</p> \[FPR = \frac{FP}{\text{all actual negatives}}\] <p>Use when false positives are more expensive than false negatives.</p> <h3 id="roc-and-auc-1">ROC and AUC</h3> <p>ROC and AUC are metrics which using TPR and FPR to train model.</p> <h2 id="references">References</h2> <ul> <li><a href="https://developers.google.com/machine-learning/crash-course/classification">Google crash course</a></li> </ul>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[Classification is the task of predicting which of a set of classes (categories) an example belongs to. Classification is converting a logistic regression model that predicts a probability into a binary classification model that predicts one of two classes.]]></summary></entry><entry><title type="html">Linear Regression</title><link href="https://benwzj.github.io/blog/2025/linear-regression/" rel="alternate" type="text/html" title="Linear Regression"/><published>2025-05-21T00:00:00+00:00</published><updated>2025-05-21T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/linear-regression</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/linear-regression/"><![CDATA[<h2 id="what-is-linear-regression">What is Linear Regression</h2> <p>Training data to form a model, simply say, it is to find the bias and weights among the data. linear regression is one of the <strong>methods</strong> that find the relationship between features and a label to get the bias and weights.</p> <figure> <picture> <img src="/assets/img/car-data-points-with-model.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>During training, the model calculates the <strong>weight</strong> and <strong>bias</strong> that produce the best model.</p> <figure> <picture> <img src="/assets/img/linear-regression-equation.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="loss">Loss</h2> <p>Loss is a numerical metric that describes how wrong a model‚Äôs predictions are. Loss measures the distance between the model‚Äôs predictions and the actual labels. The goal of training a model is to minimize the loss, reducing it to its lowest possible value.</p> <figure> <picture> <img src="/assets/img/loss-lines.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The two most common methods to remove the sign are the following:</p> <ul> <li>Take the absolute value of the difference between the actual value and the prediction.</li> <li>Square the difference between the actual value and the prediction.</li> </ul> <p>There are four main types of loss:</p> <ul> <li><strong>L1 loss</strong>: The sum of the absolute values of the difference between the predicted values and the actual values.</li> <li><strong>Mean absolute error (MAE)</strong>: The average of L1 losses across a set of examples.</li> <li><strong>L2 loss</strong>: The sum of the squared difference between the predicted values and the actual values.</li> <li><strong>Mean squared error (MSE)</strong>: The average of L2 losses across a set of examples.</li> </ul> <h3 id="choosing-a-loss">Choosing a loss</h3> <p>In training, model will try to get the best bias and weights according to the LOSS. So choosing a loss is matter.</p> <p>When choosing the best loss function, also consider how you want the model to treat outliers. The outliers are closer to the model trained with MSE than to the model trained with MAE.</p> <figure> <picture> <img src="/assets/img/model-mse.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A model trained with MSE moves the model closer to the outliers.</p> <figure> <picture> <img src="/assets/img/model-mae.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A model trained with MAE is farther from the outliers.</p> <h2 id="gradient-descent">Gradient descent</h2> <p>Gradient descent is a <strong>mathematical technique</strong> that iteratively finds the weights and bias that produce the model with the lowest loss.</p> <p>Gradient descent finds the best weight and bias by repeating the following process for a number of user-defined iterations. The model begins training with randomized weights and biases near zero, and then repeats the following steps:</p> <ul> <li>Calculate the loss with the current weight and bias.</li> <li>Determine the direction to move the weights and bias that reduce loss.</li> <li>Move the weight and bias values a small amount in the direction that reduces loss.</li> <li>Return to step one and repeat the process until the model can‚Äôt reduce the loss any further.</li> </ul> <p>This is typical loss curve, Loss is on the y-axis and iterations are on the x-axis:</p> <figure> <picture> <img src="/assets/img/loss-convergence.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Loss surface showing the weight and bias values that produce the lowest loss.</p> <figure> <picture> <img src="/assets/img/loss-surface-points.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="hyperparameters">Hyperparameters</h2> <p>Hyperparameters are variables that control different aspects of training. Three common hyperparameters are:</p> <ul> <li>Learning rate</li> <li>Batch size</li> <li>Epochs</li> </ul> <p>In contrast, parameters are the variables, like the weights and bias, that are part of the model itself. In other words, hyperparameters are values that you control; parameters are values that the model calculates during training.</p> <h3 id="learning-rate">Learning rate</h3> <p>The learning rate determines the magnitude of the changes to make to the weights and bias during each step of the gradient descent process.</p> <p>The model multiplies the gradient by the learning rate to determine the model‚Äôs parameters (weight and bias values) for the next iteration. For example, if the <strong>gradient‚Äôs magnitude</strong> is 2.5 and the learning rate is 0.01, then the model will change the parameter by 0.025.</p> <p>Learning rate is a floating point number you set that influences how quickly the model converges.</p> <p>The ideal learning rate helps the model to converge within a reasonable number of iterations.</p> <p>What do it means when Learning Rate is 1? A learning rate of 1 means that the model updates its weights by the full amount of the calculated gradient. This almost inevitably leads to highly unstable training and the model failing to converge to a good solution.</p> <h3 id="batch-size">Batch size</h3> <p>Batch size is a hyperparameter that refers to the number of examples the model processes before updating its weights and bias.</p> <p>You might think that the model should do Full Batch, means calculating the loss for every example in the dataset before updating the weights and bias. However, when a dataset contains hundreds of thousands or even millions of examples, using the full batch isn‚Äôt practical.</p> <p>Two common techniques to get the right gradient on average without needing to look at every example in the dataset before updating the weights and bias:</p> <ul> <li>Stochastic gradient descent (SGD): Stochastic gradient descent uses only a single example (a batch size of one) per iteration. The term ‚Äústochastic‚Äù indicates that the one example comprising each batch is chosen at random. (Note that using stochastic gradient descent can produce noise throughout the entire loss curve, not just near convergence.)</li> <li>Mini-batch stochastic gradient descent (mini-batch SGD): Mini-batch stochastic gradient descent is a compromise between full-batch and SGD. The model chooses the examples included in each batch at random, averages their gradients, and then updates the weights and bias once per iteration.</li> </ul> <h3 id="epochs">Epochs</h3> <p>During training, an epoch means that the model has processed every example in the training set once. For example, given a training set with 1,000 examples and a mini-batch size of 100 examples, it will take the model 10 iterations to complete one epoch.</p> <p>Training typically requires many epochs. In general, more epochs produces a better model, but also takes more time to train.</p> <p>Here is an example to tell the difference:</p> <ul> <li>Full batch: After the model looks at all the examples in the dataset. For instance, if a dataset contains 1,000 examples and the model trains for 20 epochs, the model updates the weights and bias 20 times, once per epoch.</li> <li>Stochastic gradient descent: After the model looks at a single example from the dataset. For instance, if a dataset contains 1,000 examples and trains for 20 epochs, the model updates the weights and bias 20,000 times.</li> <li>Mini-batch stochastic gradient descent: After the model looks at the examples in each batch. For instance, if a dataset contains 1,000 examples, and the batch size is 100, and the model trains for 20 epochs, the model updates the weights and bias 200 times.</li> </ul> <h2 id="generate-a-correlation-matrix">Generate a correlation matrix</h2> <p>An important part of machine learning is determining which features correlate with the label. you can use a correlation matrix to identify features whose values correlate well with the label.</p> <p>Correlation values have the following meanings:</p> <ul> <li>1.0: perfect positive correlation; that is, when one attribute rises, the other attribute rises.</li> <li>-1.0: perfect negative correlation; that is, when one attribute rises, the other attribute falls.</li> <li>0.0: no correlation; the two columns are not linearly related. In general, the higher the absolute value of a correlation value, the greater its predictive power.</li> </ul> <p>dataframe can provide such function: <code class="language-plaintext highlighter-rouge">training_df.corr(numeric_only = True)</code></p> <h2 id="visualize-relationships-in-dataset">Visualize relationships in dataset</h2> <p>dataframe provide such function: <code class="language-plaintext highlighter-rouge">sns.pairplot(training_df, x_vars=["FARE", "TRIP_MILES", "TRIP_SECONDS"], y_vars=["FARE", "TRIP_MILES", "TRIP_SECONDS"])</code></p> <h2 id="references">References</h2> <ul> <li><a href="https://developers.google.com/machine-learning/crash-course/linear-regression">Google crash course</a></li> </ul>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[What is Linear Regression]]></summary></entry><entry><title type="html">Pandas Library</title><link href="https://benwzj.github.io/blog/2025/pandas/" rel="alternate" type="text/html" title="Pandas Library"/><published>2025-05-18T00:00:00+00:00</published><updated>2025-05-18T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/pandas</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/pandas/"><![CDATA[<p>Pandas is a powerful Python library for data manipulation and analysis.</p> <h2 id="basic-terms">Basic Terms</h2> <h3 id="axis">Axis</h3> <p>In Pandas, <code class="language-plaintext highlighter-rouge">axis</code> refers to the direction along which an operation is applied:</p> <table> <thead> <tr> <th>Axis</th> <th>Direction</th> <th>Refers To</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">0</code></td> <td><strong>Vertical</strong></td> <td><strong>Rows</strong> (downward)</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">1</code></td> <td><strong>Horizontal</strong></td> <td><strong>Columns</strong> (across)</td> </tr> </tbody> </table> <p>axis=0 means ‚Äúgo down the rows‚Äù (operate column-wise) axis=1 means ‚Äúgo across the columns‚Äù (operate row-wise)</p> <h2 id="core-data-structures">Core Data Structures</h2> <h3 id="series">Series</h3> <p>A one-dimensional labeled array holding data of any type such as integers, strings, Python objects etc.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="sh">'</span><span class="s">c</span><span class="sh">'</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <h3 id="dataframe">DataFrame</h3> <p>A two-dimensional data structure that holds data like a two-dimension array or a table with rows and columns. Think of it like a spreadsheet or SQL table.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">Name</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">Alice</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Bob</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Charlie</span><span class="sh">'</span><span class="p">],</span>
        <span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">28</span><span class="p">],</span>
        <span class="sh">'</span><span class="s">City</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">New York</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">London</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Paris</span><span class="sh">'</span><span class="p">]}</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div> <h2 id="reading-and-writing-data">Reading and Writing Data</h2> <p>Pandas can read and write data from various formats like CSV, Excel, JSON, SQL databases, and more.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Reading from CSV
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">data.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Writing to CSV
</span><span class="n">df</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">output.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># index=False prevents writing row indices
</span>
<span class="c1"># Other formats:
# pd.read_excel('data.xlsx')
# pd.read_json('data.json')
# ...
</span></code></pre></div></div> <h2 id="accessing-data">Accessing Data</h2> <h3 id="columns">Columns</h3> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">names</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">Name</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># Access the 'Name' column as a Series
</span><span class="n">ages</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">Age</span>       <span class="c1"># Alternative way to access a column
</span></code></pre></div></div> <h3 id="rows">Rows</h3> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">first_row</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Access the first row by label
</span><span class="n">second_row</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># Access the second row by integer position
</span></code></pre></div></div> <h3 id="slicing">Slicing</h3> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">subset</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>   <span class="c1"># Rows 2 and 3
</span></code></pre></div></div> <h2 id="data-manipulation">Data Manipulation</h2> <h3 id="filtering">Filtering:</h3> <p><code class="language-plaintext highlighter-rouge">young_people = df[df['Age'] &lt; 30]</code></p> <h3 id="sorting">Sorting:</h3> <p><code class="language-plaintext highlighter-rouge">df_sorted = df.sort_values(by='Age', ascending=False)</code></p> <h3 id="adding-columns">Adding Columns:</h3> <p><code class="language-plaintext highlighter-rouge">df['NewColumn'] = df['Age'] * 2</code></p> <h3 id="deleting-columns">deleting Columns:</h3> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">DataFrame</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="sh">'</span><span class="s">raise</span><span class="sh">'</span><span class="p">)[</span><span class="n">source</span><span class="p">]</span>
</code></pre></div></div> <p>Remove rows or columns by specifying label names and corresponding axis, or by directly specifying index or column names.</p> <h3 id="applying-functions">Applying Functions:</h3> <p><code class="language-plaintext highlighter-rouge">df['NameLength'] = df['Name'].apply(len)</code></p> <h3 id="grouping">Grouping:</h3> <p><code class="language-plaintext highlighter-rouge">grouped = df.groupby('City')['Age'].mean()</code></p> <h3 id="combining-dataframes">Combining DataFrames:</h3> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">merged_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="sh">'</span><span class="s">KeyColumn</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># Merge two DataFrames based on a common column
</span><span class="n">concatenated_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">])</span>        <span class="c1"># Concatenate DataFrames
</span></code></pre></div></div> <h2 id="handling-missing-data">Handling Missing Data</h2> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="nf">dropna</span><span class="p">()</span>       <span class="c1"># Remove rows with missing values
</span><span class="n">df</span><span class="p">.</span><span class="nf">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>      <span class="c1"># Fill missing values with 0
</span></code></pre></div></div> <h2 id="dataframe-describe">DataFrame describe</h2> <p>Understand the return from describe()</p> <table> <thead> <tr> <th>Statistic</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">count</code></td> <td>Number of non-missing values</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">mean</code></td> <td>Average value</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">std</code></td> <td>Standard deviation (spread)</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">min</code></td> <td>Minimum value</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">25%</code></td> <td>1st quartile (25th percentile)</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">50%</code></td> <td>2nd quartile = <strong>median</strong></td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">75%</code></td> <td>3rd quartile (75th percentile)</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">max</code></td> <td>Maximum value</td> </tr> </tbody> </table> <h4 id="mean-vs-50">mean vs. 50%</h4> <table> <thead> <tr> <th>Feature</th> <th><strong>Mean</strong></th> <th><strong>Median (50%)</strong></th> </tr> </thead> <tbody> <tr> <td>What it is</td> <td>The <strong>average</strong> of all values</td> <td>The <strong>middle</strong> value when sorted</td> </tr> <tr> <td>Formula</td> <td>Sum of values √∑ number of values</td> <td>Middle value (or average of two middle ones)</td> </tr> <tr> <td>Sensitive to outliers?</td> <td>‚úÖ Yes ‚Äì pulled by extreme values</td> <td>‚ùå No ‚Äì more resistant to outliers</td> </tr> <tr> <td>Good for</td> <td>Symmetrical distributions</td> <td>Skewed distributions (with outliers)</td> </tr> </tbody> </table> <ul> <li><code class="language-plaintext highlighter-rouge">Mean</code>: looks at the arithmetic average ‚Äî good for clean, normal data</li> <li><code class="language-plaintext highlighter-rouge">50%</code> (median): finds the middle value ‚Äî better for messy or skewed data. The <code class="language-plaintext highlighter-rouge">median</code> value tell that half the data is below this number, half above.</li> <li><code class="language-plaintext highlighter-rouge">25%</code> value is the 25% item value (or average of two ones)!</li> </ul> <h2 id="dataframe-sample">DataFrame sample</h2> <p><code class="language-plaintext highlighter-rouge">DataFrame.sample(n=None, frac=None, replace=False, weights=None, random_state=None, axis=None, ignore_index=False)</code>:</p> <p>Return a random sample of items from an axis of object.</p> <ul> <li><code class="language-plaintext highlighter-rouge">n</code>: Number of items from axis to return.</li> <li><code class="language-plaintext highlighter-rouge">frac</code>: Fraction of axis items to return. Cannot be used with <code class="language-plaintext highlighter-rouge">n</code>.</li> <li><code class="language-plaintext highlighter-rouge">axis</code>: Default is row for DataFrame type.</li> <li><code class="language-plaintext highlighter-rouge">random_state</code>: is used to control the randomness of an operation so that you get the same result every time you run your code. Just like a seed value passed to the internal random number generator. For example, by setting <code class="language-plaintext highlighter-rouge">random_state</code> (e.g., to 42), you ensure the same output every time your code runs.</li> </ul> <h2 id="references">References</h2> <ul> <li><a href="https://pandas.pydata.org/docs">Pandas website</a></li> </ul>]]></content><author><name></name></author><category term="Python"/><category term="ML"/><category term="Python"/><summary type="html"><![CDATA[Pandas is a powerful Python library for data manipulation and analysis.]]></summary></entry><entry><title type="html">Machine Learning Overview</title><link href="https://benwzj.github.io/blog/2025/ml-overview/" rel="alternate" type="text/html" title="Machine Learning Overview"/><published>2025-05-14T00:00:00+00:00</published><updated>2025-05-14T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/ml-overview</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/ml-overview/"><![CDATA[<p>ML is the <strong>process</strong> of training a piece of software, called a model, to make useful predictions or generate content (like text, images, audio, or video) from data.</p> <h2 id="types-of-ml-systems">Types of ML Systems</h2> <p>ML systems fall into one or more of the following categories based on how they learn to make predictions or generate content:</p> <ul> <li>Supervised learning</li> <li>Unsupervised learning</li> <li>Reinforcement learning</li> <li>Generative AI</li> </ul> <h3 id="supervised-learning">Supervised learning</h3> <p>Supervised learning models can make predictions after seeing lots of data with the correct answers and then discovering the connections between the elements in the data that produce the correct answers.</p> <p>Supervised ML models are trained using datasets with labeled examples.</p> <p>Two of the most common use cases for supervised learning are regression and classification.</p> <h4 id="regression">Regression</h4> <p>A regression model predicts <strong>a numeric value</strong>. For example, a weather model that predicts the amount of rain, in inches or millimeters, is a regression model.</p> <h4 id="classification">Classification</h4> <p>Classification models predict the likelihood that something belongs to a category.</p> <p>Classification models are divided into two groups: binary classification and multiclass classification.</p> <h3 id="unsupervised-learning">Unsupervised learning</h3> <p>Unsupervised learning models make predictions by being given data that does not contain any correct answers. An unsupervised learning model‚Äôs goal is to identify meaningful patterns among the data. In other words, the model has no hints on how to categorize each piece of data, but instead it must infer its own rules.</p> <p>A commonly used unsupervised learning model employs a technique called clustering. The model finds data points that demarcate natural groupings.</p> <p>Clustering differs from classification because the categories aren‚Äôt defined by you.</p> <h3 id="reinforcement-learning">Reinforcement learning</h3> <p>Reinforcement learning models make predictions by getting rewards or penalties based on actions performed within an environment. A reinforcement learning system generates a policy that defines the best strategy for getting the most rewards.</p> <p>Reinforcement learning is used to train robots to perform tasks, like walking around a room, and software programs like AlphaGo to play the game of Go.</p> <h3 id="generative-ai">Generative AI</h3> <p>Generative AI is a class of models that creates content from user input.</p> <p>At a high-level, generative models learn patterns in data with the goal to produce new but similar data. Generative models are like the following:</p> <ul> <li>Comedians who learn to imitate others by observing people‚Äôs behaviors and style of speaking</li> <li>Artists who learn to paint in a particular style by studying lots of paintings in that style</li> <li>Cover bands that learn to sound like a specific music group by listening to lots of music by that group</li> </ul> <p>To produce unique and creative outputs, generative models are initially trained using an unsupervised approach, where the model learns to mimic the data it‚Äôs trained on. The model is sometimes trained further using supervised or reinforcement learning on specific data related to tasks the model might be asked to perform, for example, summarize an article or edit a photo.</p> <h2 id="ml-model-types">ML Model types</h2> <p>Linear Regression Logistic Regression Classification</p>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[ML is the process of training a piece of software, called a model, to make useful predictions or generate content (like text, images, audio, or video) from data.]]></summary></entry></feed>
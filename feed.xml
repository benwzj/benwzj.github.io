<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://benwzj.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://benwzj.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-05T14:20:45+00:00</updated><id>https://benwzj.github.io/feed.xml</id><title type="html">BEN WEN</title><subtitle>A website to show the world of Ben Wen </subtitle><entry><title type="html">Logistic Regression</title><link href="https://benwzj.github.io/blog/2025/logistic-regresion/" rel="alternate" type="text/html" title="Logistic Regression"/><published>2025-05-22T00:00:00+00:00</published><updated>2025-05-22T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/logistic-regresion</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/logistic-regresion/"><![CDATA[<h2 id="what-is-logistic-regression">What is Logistic Regression</h2> <p>Logistic regression is designed to predict the probability of a given outcome.</p> <p>Classic example is spam-prediction model.</p> <h2 id="sigmoid-function">Sigmoid function</h2> <p>There’s a family of functions called logistic functions whose output represents a probability, always outputting a value between 0 and 1. The standard logistic function, also known as the sigmoid function (sigmoid means “s-shaped”), has the formula:</p> \[f(x) = \frac{1}{1 + e^{-x}}\] <ul> <li>The ‘e’ is Euler’s number, a fundamental mathematical constant. <code class="language-plaintext highlighter-rouge">e≈2.71828...</code></li> <li>Logistic functions is one of The most important exponential function.</li> </ul> <p>Here are the classic corresponding graph of the sigmoid function:</p> <figure> <picture> <img src="/assets/img/sigmoid_function_with_axes.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>As the input, x, increases, the output of the sigmoid function approaches but never reaches 1.</li> <li>Similarly, as the input decreases, the sigmoid function’s output approaches but never reaches 0.</li> <li>The sigmoid function will bend the linear equation straight line into an s-shape.</li> </ul> <h3 id="what-is-exponent">What is Exponent</h3> <table> <thead> <tr> <th>Expression</th> <th>Meaning</th> <th>Result</th> <th>Explanation</th> </tr> </thead> <tbody> <tr> <td>\(2^3\)</td> <td>\(2 \times 2 \times 2\)</td> <td>8</td> <td>Multiply 2 three times</td> </tr> <tr> <td>\(5^2\)</td> <td>\(5 \times 5\)</td> <td>25</td> <td>Square of 5</td> </tr> <tr> <td>\(10^0\)</td> <td>—</td> <td>1</td> <td>Any non-zero number to the 0 power is 1</td> </tr> <tr> <td>\(2^{-3}\)</td> <td>\(\frac{1}{2^3} = \frac{1}{8}\)</td> <td>0.125</td> <td>Negative = reciprocal</td> </tr> <tr> <td>\(4^{-1}\)</td> <td>\(\frac{1}{4}\)</td> <td>0.25</td> <td>Negative exponent = 1 over base</td> </tr> <tr> <td>\(9^{1/2}\)</td> <td>\(\sqrt{9}\)</td> <td>3</td> <td>Fractional = root</td> </tr> <tr> <td>\(27^{1/3}\)</td> <td>\(\sqrt[3]{27}\)</td> <td>3</td> <td>Cube root</td> </tr> <tr> <td>\(16^{3/4}\)</td> <td>\(\left(\sqrt[4]{16}\right)^3\)</td> <td>8</td> <td>Root first, then power</td> </tr> </tbody> </table> <h3 id="transforming-linear-output-using-the-sigmoid-function">Transforming linear output using the sigmoid function</h3> <p>Left: graph of the linear function z = 2x + 5, with three points highlighted. Right: Sigmoid curve with the same three points highlighted after being transformed by the sigmoid function:</p> <figure> <picture> <img src="/assets/img/linear_to_logistic.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="logistic-regression-trainning">Logistic regression trainning</h2> <p>Logistic regression models are trained using the same process as linear regression models, with two key distinctions:</p> <ul> <li>Logistic regression models use Log Loss as the loss function instead of squared loss.</li> <li>Applying regularization is critical to prevent overfitting.</li> </ul> <h3 id="log-loss">Log Loss</h3> <p>In the Linear regression module, you used squared loss (also called L2 loss) as the loss function. loss function for logistic regression is Log Loss.</p> <figure> <picture> <img src="/assets/img/logloss-func.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="regularization">Regularization</h3> <p>Regularization, a mechanism for penalizing model complexity during training.</p> <p>Regularization is extremely important in logistic regression modeling. Without regularization, the asymptotic nature of logistic regression would keep driving loss towards 0 in cases where the model has a large number of features.</p> <p>Consequently, most logistic regression models use one of the following two strategies to decrease model complexity:</p> <ul> <li>L2 regularization</li> <li>Early stopping: Limiting the number of training steps to halt training while loss is still decreasing.</li> </ul>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[What is Logistic Regression]]></summary></entry><entry><title type="html">Classification</title><link href="https://benwzj.github.io/blog/2025/classification/" rel="alternate" type="text/html" title="Classification"/><published>2025-05-22T00:00:00+00:00</published><updated>2025-05-22T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/classification</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/classification/"><![CDATA[<p>Classification is the task of predicting which of a set of classes (categories) an example belongs to. Classification is converting a logistic regression model that predicts a probability into a binary classification model that predicts one of two classes.</p> <p>There are some terms you need to be understood and clear:</p> <ul> <li>TP, FP, TN, FN</li> <li>Threshold</li> <li>Accuracy</li> <li>Recall, or true positive rate</li> <li>False positive rate</li> <li>Precision</li> </ul> <h2 id="threshold">Threshold</h2> <p>Choosing a threshold is very important for Classification. You can use tool like Confusion matrix to understand more about threshold in your model.</p> <h3 id="confusion-matrix">Confusion matrix</h3> <table> <thead> <tr> <th> </th> <th>Actual positive</th> <th>Actual negative</th> </tr> </thead> <tbody> <tr> <td>Predicted positive</td> <td>True positive (TP): A spam email correctly classified as a spam email. These are the spam messages automatically sent to the spam folder.</td> <td>False positive (FP): A not-spam email misclassified as spam. These are the legitimate emails that wind up in the spam folder.</td> </tr> <tr> <td>Predicted negative</td> <td>False negative (FN): A spam email misclassified as not-spam. These are spam emails that aren’t caught by the spam filter and make their way into the inbox.</td> <td>True negative (TN): A not-spam email correctly classified as not-spam. These are the legitimate emails that are sent directly to the inbox.</td> </tr> </tbody> </table> <p>For example it can look like this:</p> <figure> <picture> <img src="/assets/img/confusion-matrix.png" class="img-fluid rounded z-depth-1" width="50%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>TP = it is Spam and labled as Spam FP = ir is not spam but labled as spam TN = it is not spam and labled as not spam FN = it is spam but labled as not spam</p> <p>When the classification threshold increases, both true and false positives decrease. Becuase the model will likely predict fewer positives overall.</p> <h2 id="measure-model-metrics">Measure Model Metrics</h2> <p>They are Accuracy, TPR, FPR, Precision.</p> <p>TPR and FPR will be used more in Measurement.</p> <h3 id="accuracy">Accuracy</h3> <p>Accuracy is the proportion of all classifications that were correct, whether positive or negative. It is mathematically defined as:</p> \[\text{Accuracy} = \frac{\text{correct classifications}}{\text{total classifications}} = \frac{TP + TN}{TP + TN + FP + FN}\] <h3 id="recall-or-true-positive-rate">Recall, or true positive rate</h3> <p>The true positive rate (TPR), or the proportion of all actual positives that were classified correctly as positives, is also known as recall.</p> <p>Recall is mathematically defined as:</p> \[\text{Recall (or TPR)} = \frac{\text{correctly classified actual positives}}{\text{all actual positives}} = \frac{TP}{TP + FN}\] <p>A hypothetical perfect model would have zero false negatives and therefore a recall (TPR) of 1.0, which is to say, a 100% detection rate.</p> <h3 id="false-positive-rate">False positive rate</h3> <p>The false positive rate (FPR) is the proportion of all actual negatives that were classified incorrectly as positives, also known as the probability of false alarm. It is mathematically defined as:</p> \[FPR = \frac{\text{incorrectly classified actual negatives}}{\text{all actual negatives}} = \frac{FP}{FP + TN}\] <p>A perfect model would have zero false positives and therefore a FPR of 0.0, which is to say, a 0% false alarm rate.</p> <h3 id="precision">Precision</h3> <p>Precision is the proportion of all the model’s positive classifications that are actually positive. It is mathematically defined as:</p> \[\text{Precision} = \frac{\text{correctly classified actual positives}}{\text{everything classified as positive}} = \frac{TP}{TP + FP}\] <p>Precision improves as false positives decrease, while recall improves when false negatives decrease.</p> <h3 id="choice-of-metric-and-tradeoffs">Choice of metric and tradeoffs</h3> <table> <thead> <tr> <th>Metric</th> <th>Guidance</th> </tr> </thead> <tbody> <tr> <td>Accuracy</td> <td>Use as a rough indicator of model training progress/convergence for balanced datasets. <br/> For model performance, use only in combination with other metrics. <br/> Avoid for imbalanced datasets. Consider using another metric.</td> </tr> <tr> <td>Recall (True positive rate)</td> <td>Use when false negatives are more expensive than false positives.</td> </tr> <tr> <td>False positive rate</td> <td>Use when false positives are more expensive than false negatives.</td> </tr> <tr> <td>Precision</td> <td>Use when it’s very important for positive predictions to be accurate.</td> </tr> </tbody> </table> <p>Accoring to my understanding, Spam email model should use FPR matric, because I can’t accept labling legitimate email as spam.</p> <h2 id="roc-and-auc">ROC and AUC</h2> <h3 id="roc">ROC</h3> <p>ROC curve, short for Receiver-operating characteristic curve. The long version of the name, receiver operating characteristic, is a holdover from WWII radar detection.</p> <p>The ROC curve is a visual representation of model performance across all thresholds.</p> <p>The ROC curve is drawn by calculating the true positive rate (TPR) and false positive rate (FPR) at every possible threshold (in practice, at selected intervals), then graphing TPR over FPR.</p> <h3 id="auc">AUC</h3> <p>AUC, short for Area under the curve.</p> <p>The area under the ROC curve (AUC) represents the probability that the model, if given a randomly chosen positive and negative example, will rank the positive higher than the negative.</p> <p>AUC is a useful measure for comparing the performance of two different models, as long as the dataset is roughly balanced. The model with greater area under the curve is generally the better one.</p> <h4 id="understand-auc">Understand AUC</h4> <p>For a binary classifier, a model that does exactly as well as random guesses or coin flips has a ROC that is a diagonal line from (0,0) to (1,1). The AUC is 0.5, representing a 50% probability of correctly ranking a random positive and negative example.</p> <h3 id="auc-and-roc-for-choosing-model-and-threshold">AUC and ROC for choosing model and threshold</h3> <p>AUC is a useful measure for comparing the performance of two different models, as long as the dataset is roughly balanced. The model with greater area under the curve is generally the better one.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/auc_0-65.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/auc_0-93.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The points on a ROC curve closest to (0,1) represent a range of the best-performing thresholds for the given model.</p> <figure> <picture> <img src="/assets/img/auc_abc.png" class="img-fluid rounded z-depth-1" width="50%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>If false positives (false alarms) are highly costly, it may make sense to choose a threshold that gives a lower FPR, like the one at point A, even if TPR is reduced. Conversely, if false positives are cheap and false negatives (missed true positives) highly costly, the threshold for point C, which maximizes TPR, may be preferable. If the costs are roughly equivalent, point B may offer the best balance between TPR and FPR.</p> <p>Understand AUC and ROC:</p> <figure> <picture> <img src="/assets/img/AUC.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="prediction-bias">Prediction bias</h2> <p>Prediction bias is the difference between the mean of a model’s predictions and the mean of ground-truth labels in the data. A model trained on a dataset where 5% of the emails are spam should predict, on average, that 5% of the emails it classifies are spam. In other words, the mean of the labels in the ground-truth dataset is 0.05, and the mean of the model’s predictions should also be 0.05. If this is the case, the model has zero prediction bias. Of course, the model might still have other problems.</p> <h2 id="multi-class-classification">Multi-class classification</h2> <p>Multi-class classification can be treated as an extension of binary classification to more than two classes.</p> <p>For example, in a three-class multi-class classification problem, where you’re classifying examples with the labels A, B, and C, you could turn the problem into two separate binary classification problems. First, you might create a binary classifier that categorizes examples using the label A+B and the label C. Then, you could create a second binary classifier that reclassifies the examples that are labeled A+B using the label A and the label B.</p> <h2 id="references">References</h2> <ul> <li><a href="https://developers.google.com/machine-learning/crash-course/classification">Google crash course</a></li> </ul>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[Classification is the task of predicting which of a set of classes (categories) an example belongs to. Classification is converting a logistic regression model that predicts a probability into a binary classification model that predicts one of two classes.]]></summary></entry><entry><title type="html">Linear Regression</title><link href="https://benwzj.github.io/blog/2025/linear-regression/" rel="alternate" type="text/html" title="Linear Regression"/><published>2025-05-21T00:00:00+00:00</published><updated>2025-05-21T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/linear-regression</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/linear-regression/"><![CDATA[<h2 id="what-is-linear-regression">What is Linear Regression</h2> <p>Training data to form a model, simply say, it is to find the bias and weights among the data. linear regression is one of the <strong>methods</strong> that find the relationship between features and a label to get the bias and weights.</p> <figure> <picture> <img src="/assets/img/car-data-points-with-model.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>During training, the model calculates the <strong>weight</strong> and <strong>bias</strong> that produce the best model.</p> <figure> <picture> <img src="/assets/img/linear-regression-equation.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="loss">Loss</h2> <p>Loss is a numerical metric that describes how wrong a model’s predictions are. Loss measures the distance between the model’s predictions and the actual labels. The goal of training a model is to minimize the loss, reducing it to its lowest possible value.</p> <figure> <picture> <img src="/assets/img/loss-lines.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The two most common methods to remove the sign are the following:</p> <ul> <li>Take the absolute value of the difference between the actual value and the prediction.</li> <li>Square the difference between the actual value and the prediction.</li> </ul> <p>There are four main types of loss:</p> <ul> <li><strong>L1 loss</strong>: The sum of the absolute values of the difference between the predicted values and the actual values.</li> <li><strong>Mean absolute error (MAE)</strong>: The average of L1 losses across a set of examples.</li> <li><strong>L2 loss</strong>: The sum of the squared difference between the predicted values and the actual values.</li> <li><strong>Mean squared error (MSE)</strong>: The average of L2 losses across a set of examples.</li> </ul> <h3 id="choosing-a-loss">Choosing a loss</h3> <p>In training, model will try to get the best bias and weights according to the LOSS. So choosing a loss is matter.</p> <p>When choosing the best loss function, also consider how you want the model to treat outliers. The outliers are closer to the model trained with MSE than to the model trained with MAE.</p> <figure> <picture> <img src="/assets/img/model-mse.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A model trained with MSE moves the model closer to the outliers.</p> <figure> <picture> <img src="/assets/img/model-mae.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A model trained with MAE is farther from the outliers.</p> <h2 id="gradient-descent">Gradient descent</h2> <p>Gradient descent is a <strong>mathematical technique</strong> that iteratively finds the weights and bias that produce the model with the lowest loss.</p> <p>Gradient descent finds the best weight and bias by repeating the following process for a number of user-defined iterations. The model begins training with randomized weights and biases near zero, and then repeats the following steps:</p> <ul> <li>Calculate the loss with the current weight and bias.</li> <li>Determine the direction to move the weights and bias that reduce loss.</li> <li>Move the weight and bias values a small amount in the direction that reduces loss.</li> <li>Return to step one and repeat the process until the model can’t reduce the loss any further.</li> </ul> <p>This is typical loss curve, Loss is on the y-axis and iterations are on the x-axis:</p> <figure> <picture> <img src="/assets/img/loss-convergence.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Loss surface showing the weight and bias values that produce the lowest loss.</p> <figure> <picture> <img src="/assets/img/loss-surface-points.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="hyperparameters">Hyperparameters</h2> <p>Hyperparameters are variables that control different aspects of training. Three common hyperparameters are:</p> <ul> <li>Learning rate</li> <li>Batch size</li> <li>Epochs</li> </ul> <p>In contrast, parameters are the variables, like the weights and bias, that are part of the model itself. In other words, hyperparameters are values that you control; parameters are values that the model calculates during training.</p> <h3 id="learning-rate">Learning rate</h3> <p>The learning rate determines the magnitude of the changes to make to the weights and bias during each step of the gradient descent process.</p> <p>The model multiplies the gradient by the learning rate to determine the model’s parameters (weight and bias values) for the next iteration. For example, if the <strong>gradient’s magnitude</strong> is 2.5 and the learning rate is 0.01, then the model will change the parameter by 0.025.</p> <p>Learning rate is a floating point number you set that influences how quickly the model converges.</p> <p>The ideal learning rate helps the model to converge within a reasonable number of iterations.</p> <p>What do it means when Learning Rate is 1? A learning rate of 1 means that the model updates its weights by the full amount of the calculated gradient. This almost inevitably leads to highly unstable training and the model failing to converge to a good solution.</p> <h3 id="batch-size">Batch size</h3> <p>Batch size is a hyperparameter that refers to the number of examples the model processes before updating its weights and bias.</p> <p>You might think that the model should do Full Batch, means calculating the loss for every example in the dataset before updating the weights and bias. However, when a dataset contains hundreds of thousands or even millions of examples, using the full batch isn’t practical.</p> <p>Two common techniques to get the right gradient on average without needing to look at every example in the dataset before updating the weights and bias:</p> <ul> <li>Stochastic gradient descent (SGD): Stochastic gradient descent uses only a single example (a batch size of one) per iteration. The term “stochastic” indicates that the one example comprising each batch is chosen at random. (Note that using stochastic gradient descent can produce noise throughout the entire loss curve, not just near convergence.)</li> <li>Mini-batch stochastic gradient descent (mini-batch SGD): Mini-batch stochastic gradient descent is a compromise between full-batch and SGD. The model chooses the examples included in each batch at random, averages their gradients, and then updates the weights and bias once per iteration.</li> </ul> <h3 id="epochs">Epochs</h3> <p>During training, an epoch means that the model has processed every example in the training set once. For example, given a training set with 1,000 examples and a mini-batch size of 100 examples, it will take the model 10 iterations to complete one epoch.</p> <p>Training typically requires many epochs. In general, more epochs produces a better model, but also takes more time to train.</p> <p>Here is an example to tell the difference:</p> <ul> <li>Full batch: After the model looks at all the examples in the dataset. For instance, if a dataset contains 1,000 examples and the model trains for 20 epochs, the model updates the weights and bias 20 times, once per epoch.</li> <li>Stochastic gradient descent: After the model looks at a single example from the dataset. For instance, if a dataset contains 1,000 examples and trains for 20 epochs, the model updates the weights and bias 20,000 times.</li> <li>Mini-batch stochastic gradient descent: After the model looks at the examples in each batch. For instance, if a dataset contains 1,000 examples, and the batch size is 100, and the model trains for 20 epochs, the model updates the weights and bias 200 times.</li> </ul> <h2 id="generate-a-correlation-matrix">Generate a correlation matrix</h2> <p>An important part of machine learning is determining which features correlate with the label. you can use a correlation matrix to identify features whose values correlate well with the label.</p> <p>Correlation values have the following meanings:</p> <ul> <li>1.0: perfect positive correlation; that is, when one attribute rises, the other attribute rises.</li> <li>-1.0: perfect negative correlation; that is, when one attribute rises, the other attribute falls.</li> <li>0.0: no correlation; the two columns are not linearly related. In general, the higher the absolute value of a correlation value, the greater its predictive power.</li> </ul> <p>dataframe can provide such function: <code class="language-plaintext highlighter-rouge">training_df.corr(numeric_only = True)</code></p> <h2 id="visualize-relationships-in-dataset">Visualize relationships in dataset</h2> <p>dataframe provide such function: <code class="language-plaintext highlighter-rouge">sns.pairplot(training_df, x_vars=["FARE", "TRIP_MILES", "TRIP_SECONDS"], y_vars=["FARE", "TRIP_MILES", "TRIP_SECONDS"])</code></p> <h2 id="references">References</h2> <ul> <li><a href="https://developers.google.com/machine-learning/crash-course/linear-regression">Google crash course</a></li> </ul>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[What is Linear Regression]]></summary></entry><entry><title type="html">Pandas main points</title><link href="https://benwzj.github.io/blog/2025/pandas/" rel="alternate" type="text/html" title="Pandas main points"/><published>2025-05-18T00:00:00+00:00</published><updated>2025-05-18T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/pandas</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/pandas/"><![CDATA[<p>Pandas is a powerful Python library for data manipulation and analysis.</p> <h2 id="core-data-structures">Core Data Structures</h2> <h3 id="series">Series</h3> <p>A one-dimensional labeled array holding data of any type such as integers, strings, Python objects etc.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="sh">'</span><span class="s">c</span><span class="sh">'</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div> <h3 id="dataframe">DataFrame</h3> <p>A two-dimensional data structure that holds data like a two-dimension array or a table with rows and columns. Think of it like a spreadsheet or SQL table.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">Name</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">Alice</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Bob</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Charlie</span><span class="sh">'</span><span class="p">],</span>
        <span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">28</span><span class="p">],</span>
        <span class="sh">'</span><span class="s">City</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">New York</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">London</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Paris</span><span class="sh">'</span><span class="p">]}</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</code></pre></div></div> <h2 id="reading-and-writing-data">Reading and Writing Data</h2> <p>Pandas can read and write data from various formats like CSV, Excel, JSON, SQL databases, and more.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Reading from CSV
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">data.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Writing to CSV
</span><span class="n">df</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">output.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># index=False prevents writing row indices
</span>
<span class="c1"># Other formats:
# pd.read_excel('data.xlsx')
# pd.read_json('data.json')
# ...
</span></code></pre></div></div> <h2 id="accessing-data">Accessing Data:</h2> <h3 id="columns">Columns</h3> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">names</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">Name</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># Access the 'Name' column as a Series
</span><span class="n">ages</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">Age</span>       <span class="c1"># Alternative way to access a column
</span></code></pre></div></div> <h3 id="rows">Rows</h3> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">first_row</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Access the first row by label
</span><span class="n">second_row</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># Access the second row by integer position
</span></code></pre></div></div> <h3 id="slicing">Slicing</h3> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">subset</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>   <span class="c1"># Rows 2 and 3
</span></code></pre></div></div> <h2 id="data-manipulation">Data Manipulation:</h2> <h3 id="filtering">Filtering:</h3> <p><code class="language-plaintext highlighter-rouge">young_people = df[df['Age'] &lt; 30]</code></p> <h3 id="sorting">Sorting:</h3> <p><code class="language-plaintext highlighter-rouge">df_sorted = df.sort_values(by='Age', ascending=False)</code></p> <h3 id="adding-columns">Adding Columns:</h3> <p><code class="language-plaintext highlighter-rouge">df['NewColumn'] = df['Age'] * 2</code></p> <h3 id="applying-functions">Applying Functions:</h3> <p><code class="language-plaintext highlighter-rouge">df['NameLength'] = df['Name'].apply(len)</code></p> <h3 id="grouping">Grouping:</h3> <p><code class="language-plaintext highlighter-rouge">grouped = df.groupby('City')['Age'].mean()</code></p> <h2 id="handling-missing-data">Handling Missing Data:</h2> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="nf">dropna</span><span class="p">()</span>       <span class="c1"># Remove rows with missing values
</span><span class="n">df</span><span class="p">.</span><span class="nf">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>      <span class="c1"># Fill missing values with 0
</span></code></pre></div></div> <h2 id="data-analysis">Data Analysis:</h2> <p>Pandas provides various functions for data analysis, including statistical summaries, aggregations, and more.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="nf">describe</span><span class="p">()</span>   <span class="c1"># Statistical summary
</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">()</span> <span class="c1"># Mean age
</span></code></pre></div></div> <h2 id="combining-dataframes">Combining DataFrames:</h2> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">merged_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="sh">'</span><span class="s">KeyColumn</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># Merge two DataFrames based on a common column
</span><span class="n">concatenated_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">])</span>        <span class="c1"># Concatenate DataFrames
</span></code></pre></div></div> <h2 id="references">References</h2> <ul> <li><a href="https://pandas.pydata.org/docs">Pandas website</a></li> </ul>]]></content><author><name></name></author><category term="Python"/><category term="ML"/><category term="Python"/><summary type="html"><![CDATA[Pandas is a powerful Python library for data manipulation and analysis.]]></summary></entry><entry><title type="html">Core concepts behind ML</title><link href="https://benwzj.github.io/blog/2025/ml-core-concept/" rel="alternate" type="text/html" title="Core concepts behind ML"/><published>2025-05-14T00:00:00+00:00</published><updated>2025-05-14T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/ml-core-concept</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/ml-core-concept/"><![CDATA[<p>ML is the <strong>process</strong> of training a piece of software, called a model, to make useful predictions or generate content (like text, images, audio, or video) from data.</p> <h2 id="types-of-ml-systems">Types of ML Systems</h2> <p>ML systems fall into one or more of the following categories based on how they learn to make predictions or generate content:</p> <ul> <li>Supervised learning</li> <li>Unsupervised learning</li> <li>Reinforcement learning</li> <li>Generative AI</li> </ul> <h3 id="supervised-learning">Supervised learning</h3> <p>Supervised learning models can make predictions after seeing lots of data with the correct answers and then discovering the connections between the elements in the data that produce the correct answers.</p> <p>Supervised ML models are trained using datasets with labeled examples.</p> <p>Two of the most common use cases for supervised learning are regression and classification.</p> <h4 id="regression">Regression</h4> <p>A regression model predicts <strong>a numeric value</strong>. For example, a weather model that predicts the amount of rain, in inches or millimeters, is a regression model.</p> <h4 id="classification">Classification</h4> <p>Classification models predict the likelihood that something belongs to a category.</p> <p>Classification models are divided into two groups: binary classification and multiclass classification.</p> <h3 id="unsupervised-learning">Unsupervised learning</h3> <p>Unsupervised learning models make predictions by being given data that does not contain any correct answers. An unsupervised learning model’s goal is to identify meaningful patterns among the data. In other words, the model has no hints on how to categorize each piece of data, but instead it must infer its own rules.</p> <p>A commonly used unsupervised learning model employs a technique called clustering. The model finds data points that demarcate natural groupings.</p> <p>Clustering differs from classification because the categories aren’t defined by you.</p> <h3 id="reinforcement-learning">Reinforcement learning</h3> <p>Reinforcement learning models make predictions by getting rewards or penalties based on actions performed within an environment. A reinforcement learning system generates a policy that defines the best strategy for getting the most rewards.</p> <p>Reinforcement learning is used to train robots to perform tasks, like walking around a room, and software programs like AlphaGo to play the game of Go.</p> <h3 id="generative-ai">Generative AI</h3> <p>Generative AI is a class of models that creates content from user input.</p> <p>At a high-level, generative models learn patterns in data with the goal to produce new but similar data. Generative models are like the following:</p> <ul> <li>Comedians who learn to imitate others by observing people’s behaviors and style of speaking</li> <li>Artists who learn to paint in a particular style by studying lots of paintings in that style</li> <li>Cover bands that learn to sound like a specific music group by listening to lots of music by that group</li> </ul> <p>To produce unique and creative outputs, generative models are initially trained using an unsupervised approach, where the model learns to mimic the data it’s trained on. The model is sometimes trained further using supervised or reinforcement learning on specific data related to tasks the model might be asked to perform, for example, summarize an article or edit a photo.</p> <h2 id="ml-model-types">ML Model types</h2> <p>Linear Regression Logistic Regression Classification</p>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="ML"/><summary type="html"><![CDATA[ML is the process of training a piece of software, called a model, to make useful predictions or generate content (like text, images, audio, or video) from data.]]></summary></entry><entry><title type="html">Embedding</title><link href="https://benwzj.github.io/blog/2025/embedding/" rel="alternate" type="text/html" title="Embedding"/><published>2025-05-06T00:00:00+00:00</published><updated>2025-05-06T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/embedding</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/embedding/"><![CDATA[<p>Embeddings is the basic key point to understand Machine Learning. And it is Machine Learning’s Most Useful Multitool.</p> <p>Embeddings are a powerful technique in AI, enabling machines to understand and work with complex data in a more meaningful way. They play a crucial role in various applications, from natural language processing to computer vision and recommendation systems.</p> <p>In machine learning, an embedding is a way of representing data as points in n-dimensional space so that similar data points cluster together.</p> <h2 id="what-are-embeddings">What are Embeddings</h2> <p>Abstractively, embeddings allow us to find similar data points. The machine can understand things because of embedding. Realistically, embeddings store semantically meaning in arrays of number.</p> <p>What kinds of things can be embedded? All The Things! Text, Images, Videos, Music.</p> <p>Embeddings are typically learned from large datasets using machine learning techniques.</p> <h3 id="understand-embeddings">Understand Embeddings</h3> <p>When talk about embeddings, i though it is just lots of array of numbers which telling about the things. But how to define this dimensions? what algorithm to operate this dimensions? Embedding should be a whole system, it including the start and the end. it include algorithms, data training, etc. It can embed your input into arrays, it can understand those arrays, it can capture semantic relationships, and it can inference.</p> <p>So Creating embeddings starts with a large dataset. For example word embeddings, this is a text corpus. We define a model, often a <strong>shallow neural network</strong> like in Word2Vec. Word2Vec’s skip-gram architecture, for instance, predicts surrounding context words given a target word. The model’s hidden layer weights become the word embeddings.</p> <h2 id="word-embedding">Word Embedding</h2> <p>Word embeddings are extremely useful in natural language processing. They can be used to find synonyms (“semantic similarity”), to do clustering, or as a preprocessing step for a more complicated nlp model.</p> <p>Example: Imagine the words “cat,” “dog,” and “car.” Their embeddings might look like this (simplified):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat: [0.8, 0.2, 0.1]
dog: [0.7, 0.3, 0.2]
car: [0.1, 0.1, 0.9]
</code></pre></div></div> <p>Notice that “cat” and “dog” have similar embeddings, reflecting their semantic similarity, while “car” has a very different embedding.</p> <h2 id="pre-trained-models">Pre-trained Models</h2> <p>This pre-trained models also called Techniques which are used to create embeddings.</p> <p>A pre-trained embedding model is a model that has already been trained on a large dataset and can be used to generate embeddings for new data without further training (or with minimal fine-tuning). Think of it as a ready-to-use embedding generator.</p> <h3 id="some-famous-ones-are">Some famous ones are:</h3> <ul> <li>Word2Vec: a technique invented by Google in 2013. It Learns word embeddings by predicting surrounding words given a target word (or vice versa).</li> <li>GloVe (Global Vectors for Word Representation): Learns word embeddings by capturing global word co-occurrence statistics.</li> <li>FastText: An extension of Word2Vec that considers subword information, allowing it to generate embeddings for out-of-vocabulary words.</li> <li>Sentence Transformers: Generate embeddings for entire sentences or paragraphs.</li> <li>Graph Embeddings: Represent nodes in a graph as vectors.</li> </ul> <h3 id="how-to-use-a-pre-trained-embedding-model">How to use a pre-trained embedding model:</h3> <ul> <li>Choose a Model: Select a pre-trained model that suits your task and data. Popular choices include Word2Vec, GloVe, FastText, and sentence transformers like BERT. Consider factors like the size of the vocabulary, the dimensionality of the embeddings, and the type of data the model was trained on.</li> <li>Load the Model: Use a library like <code class="language-plaintext highlighter-rouge">Gensim</code> or <code class="language-plaintext highlighter-rouge">Hugging Face Transformers</code> to load the pre-trained model.</li> <li>Generate Embeddings: Input your data (e.g., words, sentences) into the loaded model to generate embeddings.</li> <li>Use the Embeddings: Use the generated embeddings as input features for your machine learning model or for other tasks like semantic search.</li> </ul> <p>Let’s say you want to build a sentiment analysis model. You could use pre-trained word embeddings from <strong>GloVe</strong>. You would load the GloVe model, then for each word in your input text, you would retrieve its corresponding pre-trained embedding vector. These vectors would then be used as input features for your sentiment analysis model.</p> <h3 id="example">Example</h3> <p>Using <code class="language-plaintext highlighter-rouge">google/universal-sentence-encoder</code> pre-trained model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow_hub</span> <span class="k">as</span> <span class="n">hub</span>

<span class="n">embed</span> <span class="o">=</span> <span class="n">hub</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">https://tfhub.dev/google/universal-sentence-encoder/4</span><span class="sh">"</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="nf">embed</span><span class="p">([</span>
    <span class="sh">"</span><span class="s">The quick brown fox jumps over the lazy dog.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">I am a sentence for which I would like to get its embedding</span><span class="sh">"</span><span class="p">])</span>

<span class="nf">print</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

<span class="c1"># Response looks like: [[0.001, 0.201, ...]]
# i.e., an array of vectors
</span></code></pre></div></div> <h2 id="what-is-word2vec-exactly">What is Word2vec exactly</h2> <p>Word2Vec is categorize as embedding model while BERT as a Language Model. Word2Vec provide methods, algorithms, neural network for embedding purpose. BERT can work to embedding, also can do much more various NLP tasks like generating contextualized word and sentence embeddings.</p> <h3 id="technique">Technique</h3> <p>You can say Word2vec is a technique. This technique is used for obtaining vector representations of words in natural language processing (NLP). These vectors capture information about the meaning of the word based on the surrounding words.</p> <h3 id="model">Model</h3> <p>You can say Word2vec is a group of models.<br/> Word2vec is composed of a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words.</p> <h3 id="how-word2vec-approach">How Word2vec approach</h3> <p>Word2vec takes as its input a large corpus of text and produces a mapping of the set of words to a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a vector in the space. (Word2vec 将大量文本作为输入，并将词集映射到向量空间（通常有几百维），语料库中每个唯一的词都会在空间中分配一个向量。)</p> <p>Here are a little bit more detail: Word2vec can use either of two model architectures to produce these distributed representations of words: continuous bag of words (CBOW) or continuously sliding skip-gram. In both architectures, word2vec considers both individual words and a sliding context window as it iterates over the corpus.</p> <ul> <li>The CBOW can be viewed as a ‘fill in the blank’ task.</li> <li>In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words.</li> <li>CBOW is faster while skip-gram does a better job for infrequent words.</li> <li>A corpus is a sequence of words. Both CBOW and skip-gram are methods to learn one vector per word appearing in the corpus.</li> </ul> <h3 id="how-to-use-word2vec">How to use Word2Vec</h3> <p>Using Word2Vec involves two main steps: training the model (optional, as pre-trained models are available) and then using the trained model to generate word embeddings.</p> <h4 id="training-optional">Training (Optional)</h4> <p>This is optional, as pre-trained models are available. But you can still train it with your own data.</p> <h4 id="using-the-model">Using the Model</h4> <p>Load the Model:</p> <pre><code class="language-Python">from gensim.models import Word2Vec
model = Word2Vec.load("word2vec.model") # Or load a pre-trained model
</code></pre> <p>Get Word Embeddings:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vector</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">[</span><span class="sh">'</span><span class="s">cat</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># Get embedding for 'cat'
</span></code></pre></div></div> <p>Find Similar Words:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">similar_words</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="nf">most_similar</span><span class="p">(</span><span class="sh">'</span><span class="s">cat</span><span class="sh">'</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># Find the 5 most similar words to 'cat'
</span></code></pre></div></div> <p>Perform Word Arithmetic (Analogy):</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="nf">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">king</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">woman</span><span class="sh">'</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">man</span><span class="sh">'</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># king - man + woman = ? (queen)
</span></code></pre></div></div> <p>Use in Downstream Tasks: Use the embeddings as features in machine learning models for tasks like:</p> <ul> <li>Text Classification: Sentiment analysis, spam detection.</li> <li>Machine Translation: Encoding and decoding text in different languages.</li> <li>Information Retrieval: Semantic search.</li> <li>Recommendation Systems: Recommending similar items.</li> </ul> <h2 id="create-your-own-embeddings">Create Your Own Embeddings</h2> <p>Usually, when we are talking about Creating Our Own Embeddings, usually refer to fine-tuning some pre-trained model.</p> <p>Actually, you can creat one from scratch. Or saying train one. Here’s what you’ll need:</p> <ul> <li>Familiarity with Python and a deep learning framework like TensorFlow or PyTorch is <strong>essential</strong>.</li> <li>Data: A sufficiently large and relevant dataset is crucial.</li> <li>Algorithm: Choose an appropriate embedding algorithm. Word2Vec, GloVe, and FastText are good starting points for word embeddings.</li> <li>Computational Resources Access to a decent CPU, and ideally a GPU, will significantly speed up the process.</li> <li>Patience and Experimentation: requires patience and a willingness to experiment with different hyperparameters and architectures to achieve optimal results.</li> </ul> <p>Here are the highly simplified example:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Sample vocabulary and data
</span><span class="n">vocabulary</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">the</span><span class="sh">"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">cat</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">"</span><span class="s">sat</span><span class="sh">"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="sh">"</span><span class="s">on</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="sh">"</span><span class="s">mat</span><span class="sh">"</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="sh">"</span><span class="s">the</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">cat</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sat</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">on</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">the</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">mat</span><span class="sh">"</span><span class="p">]]</span>

<span class="c1"># Hyperparameters
</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">window_size</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Initialize embeddings randomly
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>

<span class="c1"># Training loop (simplified)
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>  <span class="c1"># Example number of epochs
</span>    <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)):</span>
            <span class="n">target_word</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">context_words</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">[</span><span class="nf">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">window_size</span><span class="p">):</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">sentence</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="nf">min</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">),</span> <span class="n">i</span> <span class="o">+</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>

            <span class="c1"># Simplified training logic (replace with actual gradient updates)
</span>            <span class="k">for</span> <span class="n">context_word</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">:</span>
                <span class="n">target_index</span> <span class="o">=</span> <span class="n">vocabulary</span><span class="p">[</span><span class="n">target_word</span><span class="p">]</span>
                <span class="n">context_index</span> <span class="o">=</span> <span class="n">vocabulary</span><span class="p">[</span><span class="n">context_word</span><span class="p">]</span>
                <span class="c1"># ... (Calculate gradients and update embeddings) ...
</span>
<span class="nf">print</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span> <span class="c1"># Your trained embeddings
</span></code></pre></div></div> <p>But A real implementation, it would involve things like</p> <ul> <li>A neural network: For predicting context words.</li> <li>Backpropagation: For calculating gradients.</li> <li>An optimization algorithm: Like SGD for updating embeddings.</li> </ul> <h2 id="faq">FAQ</h2> <ul> <li>When talking embedding, is it refer to an array of number?</li> <li>Who define the dimensions?</li> <li>Word2Vec is a pre-trained model or just a Techniques which difine demensions?</li> <li>How to create our own embedding model?</li> <li>What is the difference between text and image embedding?</li> <li>What is the relationship of Word2vec and Transformer?</li> <li>Can say BERT is a embedding model?</li> </ul>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="Embedding"/><category term="Vector"/><category term="ML"/><summary type="html"><![CDATA[Embeddings is the basic key point to understand Machine Learning. And it is Machine Learning’s Most Useful Multitool.]]></summary></entry><entry><title type="html">What is Transformer</title><link href="https://benwzj.github.io/blog/2025/transformer/" rel="alternate" type="text/html" title="What is Transformer"/><published>2025-05-05T00:00:00+00:00</published><updated>2025-05-05T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/transformer</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/transformer/"><![CDATA[<h2 id="what-is-transformer">What is Transformer</h2> <p>Transformer is a type of neural network architecture.</p> <ul> <li>Transformers were initially designed for translation, superseded RNN.</li> <li>Unlike traditional recurrent or convolutional models that process data sequentially, the Transformer leverages a mechanism called <strong>self-attention</strong> to process all input data simultaneously. This allows for much greater parallelization, leading to faster training and the ability to handle longer sequences of data effectively.</li> <li>Transformers are widely used in various fields, including natural language processing (NLP) for tasks like translation, text generation, and question answering, as well as computer vision.</li> <li>In essence, transformer models have revolutionized the way we process and understand sequential data by leveraging the power of attention and parallel processing.</li> <li>Like GPT(Generative Pre-trained Transformer), BERT(Bidirectional Encoder Representations from Transformers), they are based on Transformers.</li> <li>Self-Attention and Positional Encoding are the main innovations.</li> </ul> <h2 id="key-concepts">Key Concepts</h2> <ul> <li>Self-Attention: The core innovation of Transformers. It allows the model to weigh the importance of different parts of the input when generating an output. For example, in a sentence, the word “it” might refer to different things depending on the context. Self-attention helps the model understand these relationships. It does this by calculating relationships between every word in a sequence and every other word, creating a weighted representation of the input.</li> <li>Attention Mechanism: A more general concept that allows the model to focus on specific parts of the input when generating an output. Self-attention is a specific type of attention.</li> <li>Encoder-Decoder Architecture: Many Transformers follow this structure. The encoder processes the input sequence and generates a contextualized representation. The decoder then uses this representation to generate the output sequence.</li> <li>Parallelization: Unlike recurrent networks that process input sequentially, Transformers can process all input tokens simultaneously, significantly speeding up training.</li> <li>Positional Encoding: Because Transformers don’t process sequentially, positional information of words in a sentence is lost. Positional encodings are added to the input embeddings to provide information about the position of each word.</li> <li>Feedforward Networks: Fully connected layers within each encoder and decoder layer that further process the information from the attention mechanism.</li> <li>Layer Normalization: A normalization technique used to stabilize training and improve performance.</li> </ul> <h2 id="how-a-transformer-works-simplified">How a Transformer works (simplified)</h2> <ul> <li>Input Embedding: The input sequence (e.g., a sentence) is converted into numerical representations called embeddings.</li> <li>Positional Encoding: Positional information is added to the embeddings.</li> <li>Encoder: Multiple encoder layers process the embeddings using self-attention and feedforward networks. Each encoder layer produces a set of encoded representations.</li> <li>Decoder: The decoder takes the encoded representations from the encoder and, using self-attention and feedforward networks, generates the output sequence (e.g., a translation, a summary, or the next word in a sentence). The decoder also uses attention mechanisms to focus on relevant parts of the encoded input.</li> <li>Output: The final decoder layer produces the output.</li> </ul> <h2 id="why-are-transformers-important">Why are Transformers important</h2> <p>Improved Performance: They have achieved state-of-the-art results in various NLP tasks. Parallelization: They train much faster than recurrent models. Handling Long Sequences: They can effectively process long sequences of data.</p> <h2 id="rnn">RNN</h2> <p>A recurrent neural network (RNN) is a type of neural network architecture specifically designed to process <strong>sequential</strong> data. it have many problems. Like:</p> <ul> <li>it struggle to learn long-range dependencies.</li> <li>Because sequential, it can’t be parallelized training. it is very slow.</li> </ul> <p>RNNs have been largely superseded by Transformer networks.</p>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="Transformer"/><summary type="html"><![CDATA[What is Transformer]]></summary></entry><entry><title type="html">Diffusion Model Concepts</title><link href="https://benwzj.github.io/blog/2025/diffusion-model/" rel="alternate" type="text/html" title="Diffusion Model Concepts"/><published>2025-05-04T00:00:00+00:00</published><updated>2025-05-04T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/diffusion-model</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/diffusion-model/"><![CDATA[<h2 id="what-is-diffusion-model">what is Diffusion Model</h2> <p>A diffusion model is a type of generative model, meaning it’s used to create new data instances that resemble the data it was trained on.</p> <p>There are two Steps:</p> <ul> <li>Model need to understand your input: it can be text or image. How?</li> <li>When Model know your meaning, it need to craft the image. How?</li> </ul> <p>At a high level, a diffusion model is a type of deep neural network what learn to add noise to a picture and then learn how to reverse that process to recontruct a clear image.</p> <p>The logic like this: the model have been trained lots of images, and store them as noises and embedding. when it is ask to generate a specific image, it can retrive noises and reverse back to image.</p> <h2 id="main-concepts">Main Concepts</h2> <h3 id="forward-diffusion-or-diffusion-process">Forward Diffusion (or Diffusion Process):</h3> <p>This process gradually adds Gaussian noise to a <strong>training image</strong> over a series of small timesteps until the image becomes pure noise, losing all its original information. This is a Markov chain process, meaning each step only depends on the previous step.</p> <h3 id="reverse-diffusion-or-denoising-process">Reverse Diffusion (or Denoising Process):</h3> <p>This is the heart of the diffusion model. It learns to reverse the diffusion process, effectively removing noise step-by-step to reconstruct the original image (or generate a new image based on a prompt). The model learns the conditional probability distribution of the image at each timestep, given the noisy image at the next timestep.</p> <h3 id="markov-chain">Markov Chain:</h3> <p>A sequence of events where the probability of each event depends only on the state attained in the previous event. Diffusion models use this concept for both the forward and reverse processes.</p> <h3 id="gaussian-noise">Gaussian Noise:</h3> <p>A type of random noise that follows a normal distribution (bell curve).</p> <h2 id="how-it-works">How it works</h2> <h3 id="training">Training:</h3> <p>The model is trained on a large dataset of images. During training, it learns how noise is added at each timestep in the forward diffusion process. Crucially, it also learns how to reverse this process, effectively denoising.</p> <h3 id="inference-generating-images">Inference (Generating Images):</h3> <p>Start with pure noise. Iteratively apply the reverse diffusion process, guided by the text prompt (often encoded by an LLM). At each timestep, the model predicts the slightly less noisy image based on the current noisy image and the text embedding. Repeat this process until a clean image is generated.</p> <h2 id="examples-of-diffusion-models">Examples of Diffusion Models</h2> <p>Stable Diffusion DALL-E 2 Imagen</p>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="Diffusion"/><summary type="html"><![CDATA[what is Diffusion Model]]></summary></entry><entry><title type="html">Google AI Studio</title><link href="https://benwzj.github.io/blog/2025/google-ai-studio/" rel="alternate" type="text/html" title="Google AI Studio"/><published>2025-03-26T00:00:00+00:00</published><updated>2025-03-26T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/google-ai-studio</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/google-ai-studio/"><![CDATA[<h2 id="what-is-function-calling">What is Function Calling</h2> <p>Function calling lets you connect models to external tools and APIs. Instead of generating text responses, the model understands when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the model to act as a bridge between natural language and real-world actions and data. Function calling has 3 primary use cases:</p> <ul> <li>Augment Knowledge: Access information from external sources like databases, APIs, and knowledge bases.</li> <li>Extend Capabilities: Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.</li> <li>Take Actions: Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices</li> </ul>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><category term="Prompt"/><summary type="html"><![CDATA[What is Function Calling]]></summary></entry><entry><title type="html">AI Agent</title><link href="https://benwzj.github.io/blog/2025/ai-agent/" rel="alternate" type="text/html" title="AI Agent"/><published>2025-03-05T00:00:00+00:00</published><updated>2025-03-05T00:00:00+00:00</updated><id>https://benwzj.github.io/blog/2025/ai-agent</id><content type="html" xml:base="https://benwzj.github.io/blog/2025/ai-agent/"><![CDATA[<h2 id="what-is-ai-agent">What is AI Agent</h2> <p>AI Agent is like an expert designed to help with tasks and answer questions. E.g. coding agent, marketing agent, learning agent or just a friend.</p> <p>An artificial intelligence (AI) agent refers to a system or program that is capable of autonomously performing tasks on behalf of a user or another system by designing its workflow and utilizing available tools.</p> <p>AI agents can encompass a wide range of functionalities beyond natural language processing including decision-making, problem-solving, interacting with external environments and executing actions.</p> <p>AI agents can encompass a wide range of functionalities beyond natural language processing including decision-making, problem-solving, interacting with external environments and executing actions.</p> <h2 id="what-is-not-ai-agent">What is NOT AI agent?</h2> <p>When you are using prompt to ask ChatGPT directly, for example, ask it ‘please write out an essay on Topic XXX from start to end in one go’. This is not AI Agent. Probably charGPT still give you some information, but They are possibly not the thing you are looking for. For the complicated topic, Agentic workflow can significantly improve the result. Basically, it is breaking the topic into many steps, it also include iteration, revise, etc.</p> <h3 id="important-terms">Important terms</h3> <ul> <li>circulation</li> <li>external tools</li> </ul> <p>LLM BERT Reflection</p> <p>OpenAI - GPT, DALL·E, Sora</p> <p>Gemini - formerly known as Bard, is a generative artificial intelligence chatbot developed by Google. Based on the large language model (LLM) of the same name, it was launched in 2023 in response to the rise of OpenAI’s ChatGPT. It was previously based on the LaMDA and PaLM LLMs.</p> <p>Anthropic - Claude xAI - Grok Qwen DeepSeek Llama</p>]]></content><author><name></name></author><category term="AI"/><category term="AI"/><summary type="html"><![CDATA[What is AI Agent]]></summary></entry></feed>
---
layout: post
title: Prompt for LLM
date: 2025-03-05
categories: AI
tags: AI Prompt
toc: 
  - name: Prompt design strategies
  - name: Useful tips
  - name: File prompting strategies
---

AI is a Revolution! Many part of activities in the society will shift to AI. Now Prompting is obviously a biggest capability, skill to stay informed.

> Prompt design is the process of creating prompts that elicit the desired response from language models. Writing well structured prompts is an essential part of ensuring accurate, high quality responses from a language model.

## Prompt design strategies

Here list some concepts:

### Clear and Specific instructions

- Give the models instructions on what to do.
- Make the instructions clear and specific.
- Specify any constraints or formatting requirements for the output.

### Include Few-shot Examples

We recommend to always include few-shot examples in your prompts. Prompts without few-shot examples are likely to be less effective. In fact, you can remove instructions from your prompt if your examples are clear enough in showing the task at hand.

### Add contextual information
- Include information (context) in the prompt that you want the model to use when generating a response.
- Give the model instructions on how to use the contextual information.

### Add prefixes

A prefix is a word or phrase that you add to the prompt content that can serve several purposes, depending on where you put the prefix:

- Input prefix: Adding a prefix to the input signals semantically meaningful parts of the input to the model. For example, the prefixes "English:" and "French:" demarcate two different languages.
- Output prefix: Even though the output is generated by the model, you can add a prefix for the output in the prompt. The output prefix gives the model information about what's expected as a response. For example, the output prefix "JSON:" signals to the model that the output should be in JSON format.
- Example prefix: In few-shot prompts, adding prefixes to the examples provides labels that the model can use when generating the output, which makes it easier to parse output content.

### Let the model complete partial input

- If you give the model a partial input, the model completes that input based on any available examples or context in the prompt.
- Having the model complete an input may sometimes be easier than describing the task in natural language.
- Adding a partial answer to a prompt can guide the model to follow a desired pattern or format.

### Break down prompts into simple components

- Break down complex instructions into a prompt for each instruction and decide which prompt to apply based on the user's input.
- Break down multiple sequential steps into separate prompts and chain them such that the output on the preceding prompt becomes the input of the following prompt.
- Break down parallel tasks and aggregate the responses to produce the final output.

### Experiment with different parameter values

There are some common parameters:

- Max output tokens: Maximum number of tokens that can be generated in the response.
- Temperature
- Top-K 
- Top-P

Temperature, Top-K and Top-P change how the model selects tokens for output. 

### Prompt iteration strategies

Using different words or phrasing in your prompts often yields different responses from the model even though they all mean the same thing.

### Fallback responses
A fallback response is a response returned by the model when either the prompt or the response triggers a safety filter. An example of a fallback response is "I'm not able to help with that, as I'm only a language model."

If the model responds with a fallback response, try increasing the temperature.

### Things to avoid
- Avoid relying on models to generate factual information.
- Use with care on math and logic problems.


## Useful tips

- Use persona, the role of AI itself. For example, 'act as anime expert to suggest a anime gife for my friend birthday gift'. 
- You can order the data into a table.
- If you can provide context, references, that will be better.
- Do Promting with one word: TCREI (Task Context Refercences Evaluate Iterate). 
- Iteration have some tips (RSTI): 
  - Revisit the prompt framework.
  - Separate prompt into shorter ones.
  - Trying different phrasing or switching to an analogous task.
  - Introduce constrain
- Prompt chaining
  - Chain of thought prompting
  - Tree of thought prompting
- if you stuck with prompt somewhere, you can ask AI to help with Prompt for you. This is call meta prompting.

## File prompting strategies

Like Gemini, ChatGPT, Clause models, Most LLMs are built to handle various types of input data, including text,files, images, and audio. 

AI can understand files, images, videos just like you! It can be even better than you. 

When you do this:
{% include figure.html path="assets/img/timetable.png" class="img-fluid rounded z-depth-1" width="60%" %}

```Parse the time and city from the airport board shown in this image into a list.```
AI can response this:
```
- 10:50 Moscow
- 11:05 Edinburgh
- 11:05 London
- 11:10 Bucharest
- 11:30 Kiev
- 11:35 Dublin
- 11:45 East Midlands
- 12:15 Sofia
- 12:30 London
- 12:30 Newcastle
- 12:40 St Petersburg
- 12:40 London
- 12:45 Manchester
```

## Imagen prompt guide 



---
layout: post
title: Handle Dataset in ML
date: 2025-06-16
categories: AI
tags: Python ML 
toc: 
  - name: Numerical Data 
    subsections:
      - name: Feature Vector
      - name: Overview Data
      - name: Feature engineering technique 
  - name: Categorical Data
---

ML practitioners spend far more time evaluating, cleaning, and transforming data than building models. 
Here introduce how to handle dataset before training a model.

We devide data into tow types:
- numerical data 
- categorical data

## Numerical Data 

This unit focuses on numerical data, meaning integers or floating-point values that behave like numbers. That is, they are additive, countable, ordered, and so on. 

### Feature Vector

The feature vector is input during training and during inference. A feature Vector is an array of feature values comprising an example. 

Feature Vectors seldom use the dataset's raw values. Instead, you must typically process the dataset's values into representations that your model can better learn from. This process is called feature engineering.

Every value in a feature vector must be a **floating-point** value. However, many features are naturally strings or other non-numerical values. Consequently, a large part of feature engineering is representing non-numerical values as numerical values. 

The most common feature engineering techniques are:
- Normalization: Converting numerical values into a standard range.
- Binning (also referred to as bucketing): Converting numerical values into buckets of ranges.

### Overview Data

#### Visualize your data
Visualizations help you continually check your assumptions. 
Use Pandas for visualization:
- Working with Missing Data: [pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html)
- Visualizations [pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html)

#### Statistically evaluate your data
Use Pandas `describe()`

#### Find outliers
There are some common rule to find outliers. For example: 
When the delta between the 0th and 25th percentiles differs significantly from the delta between the 75th and 100th percentiles, the dataset probably contains outliers.

- The outlier can be due to a mistake.
- The outlier is a legitimate data point, you can keep these outliers in your training set. But extreme outliers can still hurt your model. You can delete the outliers or apply more invasive feature engineering techniques, such as clipping.

### Feature engineering technique 

#### Normalization
The goal of normalization is to transform features to be on a similar scale.
Normalization methods:
- linear scaling
- Z-score scaling
- log scaling
- Clipping

#### Binning 

Binning (also called bucketing) is a feature engineering technique that groups different numerical subranges into bins or buckets. In many cases, binning turns numerical data into categorical data.

When a feature appears more clumpy than linear, binning is a much better way to represent the data. When using Binning, the model can learn separate weights for each bin. 

Binning is a good alternative to scaling or clipping when either of the following conditions is met:
- The overall linear relationship between the feature and the label is weak or nonexistent.
- When the feature values are clustered.

Binning example:
Suppose you are creating a model that predicts the number of shoppers by the outside temperature for that day.
The number of shoppers was highest when the temperature was most comfortable. So, the plot doesn't really show any sort of linear relationship between the label and the feature value.

{% include figure.html path="assets/img/binning_temperature_vs_shoppers_divided_into_3_bins.png" class="img-fluid rounded z-depth-1" width="80%" %}

The graph suggests three clusters in the following subranges:
- Bin 1 is the temperature range 4-11.
- Bin 2 is the temperature range 12-26.
- Bin 3 is the temperature range 27-36.

The model learns separate weights for each bin.

##### Quantile bucketing
**Quantile bucketing** creates bucketing boundaries such that the number of examples in each bucket is exactly or nearly equal.

#### Scrubbing

Many examples in datasets are unreliable.
You can write a program or script to detect any of the following problems:
- Omitted values
- Duplicate examples
- Out-of-range feature values

This is Scrubbing.

#### Qualities of good numerical features

- Clearly named: Not recommended: `house_age: 851472000`; Recommended: `house_age_years: 27`
- Checked or tested before training: Bad `user_age_in_years: 224`; OK `user_age_in_years: 24`
- Sensible: A "magic value" is a purposeful discontinuity in an otherwise continuous feature.

#### Polynomial transforms

A Polynomial Transform is a technique used to map your original input features into a higher-dimensional space by generating polynomial combinations of the original features.

### Numerical data Best practices

Best practices for working with numerical data:
- Remember that your ML model interacts with the data in the feature vector, not the data in the dataset.
- Normalize most numerical features.
- If your first normalization strategy doesn't succeed, consider a different way to normalize your data.
- Binning, also referred to as bucketing, is sometimes better than normalizing.
- Considering what your data should look like, write verification tests to validate those expectations. For example:
  - The absolute value of latitude should never exceed 90. You can write a test to check if a latitude value greater than 90 appears in your data.
  - If your data is restricted to the state of Florida, you can write tests to check that the latitudes fall between 24 through 31, inclusive.
- Visualize your data with scatter plots and histograms. Look for anomalies.
- Gather statistics not only on the entire dataset but also on smaller subsets of the dataset. That's because aggregate statistics sometimes obscure problems in smaller sections of a dataset.
- Document all your data transformations.

## Categorical Data

Categorical Data is the Data which behave like categories. The numerical data can be Categorical Data.
Because models can only train on floating-point values, Categorical Data need to be **Encoded**.

### Vocabulary and one-hot encoding

When a categorical feature has a low number of possible categories, you can encode it as a **vocabulary**. With a vocabulary encoding, the model treats each possible categorical value as a separate feature. During training, the model learns different weights for each category.






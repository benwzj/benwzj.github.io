<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Important Concepts in ML | BEN WEN</title> <meta name="author" content="Ben Wen"> <meta name="description" content="A website to show the world of Ben Wen "> <meta name="keywords" content="jekyll, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/mdb.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link defer rel="stylesheet" href="/assets/css/bootstrap-table.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/all.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/academicons.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/github.css?d41d8cd98f00b204e9800998ecf8427e" media="" id="highlight_theme_light"> <link rel="icon" href="/assets/img/favicon.png"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://benwzj.github.io/blog/2025/ml-concepts/"> <link rel="stylesheet" href="/assets/css/native.css?d41d8cd98f00b204e9800998ecf8427e" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <style>html{scroll-behavior:smooth}</style> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">BEN WEN</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/about/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume</a> </li> <li class="nav-item"> <a class="nav-link" href="https://github.com/benwzj" rel="external nofollow noopener" target="_blank"> GitHub <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" style="width:1rem;height:1rem;fill:currentColor"> <g data-name="Layer 2"><g data-name="external-link"> <rect width="24" height="24" opacity="0"></rect> <path d="M20 11a1 1 0 0 0-1 1v6a1 1 0 0 1-1 1H6a1 1 0 0 1-1-1V6a1 1 0 0 1 1-1h6a1 1 0 0 0 0-2H6a3 3 0 0 0-3 3v12a3 3 0 0 0 3 3h12a3 3 0 0 0 3-3v-6a1 1 0 0 0-1-1z"></path> <path d="M16 5h1.58l-6.29 6.28a1 1 0 0 0 0 1.42 1 1 0 0 0 1.42 0L19 6.42V8a1 1 0 0 0 1 1 1 1 0 0 0 1-1V4a1 1 0 0 0-1-1h-4a1 1 0 0 0 0 2z"></path> </g></g> </svg> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="post col-sm-9"> <h1 class="post-title">Important Concepts in ML</h1> <br> <br> <article class="post-content"> <div id="markdown-content"> <h2 id="standard-deviation">Standard Deviation</h2> <p>Standard deviation measures how spread out the values in a dataset are from the mean (average).</p> <h3 id="example">Example:</h3> <p>Let’s say you have test scores from two classes:</p> <p>Class A: <code class="language-plaintext highlighter-rouge">[78, 80, 82, 81, 79] → Avg = 80</code> Standard deviation is low: everyone scored close to the mean.</p> <p>Class B: <code class="language-plaintext highlighter-rouge">[50, 60, 80, 95, 100] → Avg = 77</code> Standard deviation is high: big spread from the mean.</p> <h3 id="formula-simplified">Formula (simplified):</h3> <p>For a dataset with values x1, x2,…,xn:</p> \[\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2}\] <ul> <li>\(\bar{x}\) is the mean.</li> <li>\(x_i\) are the data values.</li> <li>The expression inside the square root is called the variance</li> </ul> <h2 id="data-distribution">Data Distribution</h2> <p>Understanding data distribution is key in statistics, data analysis, and machine learning. Data distribution describes how values in a dataset are spread out or arranged. It tells you:</p> <ul> <li>Which values occur most often</li> <li>How the values are grouped</li> <li>Whether the data is symmetrical, skewed, or has outliers</li> </ul> <h3 id="example-1">Example:</h3> <p>Imagine test scores from a class:</p> <p><code class="language-plaintext highlighter-rouge">[70, 72, 75, 75, 76, 78, 80, 85, 90, 100]</code></p> <p>You can see most scores are around 75–85, with one outlier (100). That pattern of how often each score occurs is its distribution.</p> <h3 id="random-module-example">Random module Example:</h3> <p>Use Random to analog Data Distribution, it is a list of all possible values, which shows how often each value occurs.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">numpy</span> <span class="kn">import</span> <span class="n">random</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">p</code> is the Data Distribution. The sum of all probability numbers should be 1. Here the <code class="language-plaintext highlighter-rouge">9</code> never show up because of probability is 0.</p> <p>Data Distribution Concepts:</p> <ul> <li> <strong>Frequency</strong>: How often each value appears</li> <li> <strong>Mean (average)</strong>: Where the center of the data is</li> <li> <strong>Spread</strong>: How far the values are from each other</li> <li> <strong>Shape</strong>: Overall pattern (normal, skewed, etc.)</li> </ul> <p>Data Distribution helps answer: 📌 What’s common? 📌 What’s rare? 📌 How is it spread out?</p> <p>Such lists are important when working with statistics and data science. The random module offer methods that returns randomly generated data distributions.</p> <h3 id="numpy-offers-many-distribution-functions">NumPy Offers Many Distribution Functions</h3> <p>Why we need different distribution? Different types of randomness behave differently in nature. Each distribution models a specific type of uncertainty, For example:</p> <table> <thead> <tr> <th>Distribution</th> <th>Common Use Case</th> </tr> </thead> <tbody> <tr> <td><strong>Uniform</strong></td> <td>Equal chance outcomes (e.g., rolling a die)</td> </tr> <tr> <td><strong>Normal (Gaussian)</strong></td> <td>Natural measurements (e.g., height, test scores)</td> </tr> <tr> <td><strong>Binomial</strong></td> <td>Repeated yes/no trials (e.g., coin flips)</td> </tr> <tr> <td><strong>Poisson</strong></td> <td>Counting rare events (e.g., calls per hour)</td> </tr> <tr> <td><strong>Exponential</strong></td> <td>Time between events (e.g., waiting time)</td> </tr> <tr> <td><strong>Beta / Gamma / Chi-squared</strong></td> <td>Advanced statistical modeling</td> </tr> </tbody> </table> <h4 id="example-using-normal-distribution---human-heights">Example: Using Normal Distribution - Human Heights</h4> <p>Because Human traits like height follow a bell-shaped (normal) distribution. So use normal distribution:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Simulate 1000 people's heights (mean=170cm, std=10cm)
</span><span class="n">heights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">170</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">skyblue</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Simulated Human Heights (Normal Distribution)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Height (cm)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Frequency</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div> <h4 id="example-binomial-distribution--coin-tosses">Example: Binomial Distribution – Coin Tosses</h4> <p>Models “yes/no” outcomes like coin tosses, quiz answers, etc.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simulate flipping a fair coin 10 times, repeat 1000 experiments
</span><span class="n">results</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">orange</span><span class="sh">'</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="sh">'</span><span class="s">left</span><span class="sh">'</span><span class="p">,</span> <span class="n">rwidth</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">10 Coin Flips per Trial (Binomial Distribution)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Number of Heads</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Frequency</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h4 id="exponential-distribution--wait-times">Exponential Distribution – Wait Times</h4> <p>Time between events like arrivals, failures, or clicks.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simulate wait times between buses (mean wait = 10 minutes)
</span><span class="n">wait_times</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">exponential</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">wait_times</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">purple</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Wait Time Between Buses (Exponential Distribution)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Minutes</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Frequency</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h4 id="poisson-distribution--call-center-events">Poisson Distribution – Call Center Events</h4> <p>Models how often rare events happen in a fixed time (calls, emails, etc.)</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simulate number of calls per minute (avg = 3 calls/min)
</span><span class="n">calls</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">poisson</span><span class="p">(</span><span class="n">lam</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">calls</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="sh">'</span><span class="s">left</span><span class="sh">'</span><span class="p">,</span> <span class="n">rwidth</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Number of Calls Per Minute (Poisson Distribution)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Calls</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Frequency</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h2 id="normalize-data">Normalize data</h2> <p>When creating a model with multiple features, the values of each feature should span roughly the same range. If one feature’s values range from 500 to 100,000 and another feature’s values range from 2 to 12, the model will need to have weights of extremely low or extremely high values to be able to combine these features effectively. This could result in a low quality model. To avoid this, normalize features in a multi-feature model.</p> <p>The goal of normalization is to transform features to be on a similar scale.</p> <p>Three popular normalization methods:</p> <ul> <li>linear scaling</li> <li>Z-score scaling</li> <li>log scaling</li> </ul> <blockquote class="block-warning"> <p>The distribution of data should decide which method is going to be used. uniformly distributed -&gt; linear scaling ; normal distribution -&gt; Z-score scaling; power law distributiong -&gt; log scalin;</p> </blockquote> <ul> <li>The technique of Normalize data: Clipping</li> </ul> <h3 id="linear-scaling">Linear scaling</h3> <p>Linear scaling (more commonly shortened to just scaling) means converting floating-point values from their natural range into a standard range—usually 0 to 1 or -1 to +1.</p> <p>Linear scaling is a good choice when all of the following conditions are met:</p> <ul> <li>Stable range:The lower and upper bounds of your data don’t change much over time.</li> <li>No Outliers: The feature contains few or no outliers, and those outliers aren’t extreme.</li> <li> <strong>uniformly distributed</strong>: The feature is approximately uniformly distributed across its range. That is, a histogram would show roughly even bars for most values.</li> </ul> <p>Here are the examples:</p> <ul> <li>If human age is a feature, Linear scaling is a good normalization technique for age. because: <ul> <li>lower and upper bounds are 0 to 100.</li> <li>age contains a relatively small percentage of outliers. Only about 0.3% of the population is over 100.</li> </ul> </li> <li>if net_worth is a feature that holds the net worth of different people. Linear scaling would be a poor choice, because: <ul> <li>This feature contains many outliers.</li> <li>the values are not uniformly distributed across its primary range. Most people would be squeezed within a very narrow band of the overall range.</li> </ul> </li> </ul> <h3 id="z-score-scaling">Z-score scaling</h3> <blockquote> <p>The Z-score for a given value is how many standard deviations away from the mean the value is.</p> </blockquote> <p>Consider a feature with a <strong>mean</strong> of 60 and a <strong>standard deviation</strong> of 10.</p> <ul> <li>The raw value 75 would have a Z-score of +1.5: <code class="language-plaintext highlighter-rouge">Z-score = (75 - 60) / 10 = +1.5</code> </li> <li>The raw value 38 would have a Z-score of -2.2: <code class="language-plaintext highlighter-rouge">Z-score = (38 - 60) / 10 = -2.2</code> </li> </ul> <p>Z-score is a good choice when the data follows a <strong>normal distribution</strong> or a distribution somewhat like a normal distribution.</p> <blockquote> <p>What is normal distribution? a classic normal distribution:</p> <ul> <li>At least 68.27% of data has a Z-score between -1.0 and +1.0.</li> <li>At least 95.45% of data has a Z-score between -2.0 and +2.0.</li> <li>At least 99.73% of data has a Z-score between -3.0 and +3.0.</li> <li>At least 99.994% of data has a Z-score between -4.0 and +4.0.</li> </ul> </blockquote> <p>Suppose your model trains on a feature named height that holds the adult heights of ten million women. Z-score scaling is a good normalization technique. Because this feature conforms to a normal distribution.</p> <h3 id="log-scaling">Log scaling</h3> <p>Log scaling computes the logarithm of the raw value. In practice, log scaling usually calculates the natural logarithm (ln).</p> <p>Log scaling is helpful when the data conforms to a <strong>power law distribution</strong>. Casually speaking, a power law distribution looks as follows:</p> <ul> <li>Low values of X have very high values of Y.</li> <li>As the values of X increase, the values of Y quickly decrease. Consequently, high values of X have very low values of Y.</li> </ul> <h4 id="understand-log-scaling">Understand Log scaling</h4> <p>Book sales conform to a power law distribution because:</p> <ul> <li>Most published books sell a tiny number of copies, maybe one or two hundred.</li> <li>Some books sell a moderate number of copies, in the thousands.</li> <li>Only a few bestsellers will sell more than a million copies.</li> </ul> <p>Suppose you are training a linear model to find the relationship of, say, book covers to book sales. A linear model training on raw values would have to find something about book covers on books that sell a million copies that is 10,000 more powerful than book covers that sell only 100 copies. However, log scaling all the sales figures makes the task far more feasible. For example,</p> <ul> <li>the log of 100 is:<code class="language-plaintext highlighter-rouge">~4.6 = ln(100)</code> </li> <li>while the log of 1,000,000 is:<code class="language-plaintext highlighter-rouge">~13.8 = ln(1,000,000)</code> </li> </ul> <p>So, the log of 1,000,000 is only about three times larger than the log of 100. You probably could imagine a bestseller book cover being about three times more powerful (in some way) than a tiny-selling book cover.</p> <p>Log scaling changes the distribution, which helps train a model that will make better predictions.</p> <h3 id="clipping">Clipping</h3> <p>Clipping is a technique to minimize the influence of extreme outliers.</p> <blockquote> <p>In brief, clipping usually caps (reduces) the value of outliers to a specific maximum value.</p> </blockquote> <p>Clipping is a strange idea, and yet, it can be very effective.</p> <p>You can also clip values after applying other forms of normalization.</p> <p>Clipping prevents your model from overindexing on unimportant data. However, some outliers are actually important, so clip values carefully.</p> <h3 id="references">References</h3> <ul> <li><a href="https://developers.google.com/machine-learning/crash-course/numerical-data/normalization" rel="external nofollow noopener" target="_blank">Google doc</a></li> </ul> <h2 id="polynomial-transforms">Polynomial transforms</h2> <p>A Polynomial Transform is a technique used to map your original input features into a higher-dimensional space by generating polynomial combinations of the original features.</p> <p>Sometimes, when the ML practitioner has domain knowledge suggesting that one variable is related to the square, cube, or other power of another variable, it’s useful to create a synthetic feature from one of the existing numerical features.</p> <p>Sometime, it’s not possible to draw a straight line that cleanly separates the two classes, but it is possible to draw a curve that does so:</p> <figure> <picture> <img src="/assets/img/ft_cross1.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Gradient descent finds the weight (or weights <code class="language-plaintext highlighter-rouge">w1</code>, <code class="language-plaintext highlighter-rouge">w2</code>, <code class="language-plaintext highlighter-rouge">w3</code>, in the case of additional features) that minimizes the loss of the model. But the data points shown cannot be separated by a line. What can be done?</p> <p>It’s possible to keep both the linear equation and allow nonlinearity by defining a new term, <code class="language-plaintext highlighter-rouge">x2</code>, that is simply <code class="language-plaintext highlighter-rouge">x1</code> squared:</p> \[x_2 = x_1^2\] <p>This synthetic feature, called a <strong>polynomial transform</strong>, is treated like any other feature. The previous linear formula becomes:</p> \[y = b + w_1x_1 + w_2x_2\] <p>This can still be treated like a linear regression problem, and the weights determined through gradient descent, as usual, despite containing a hidden squared term, the polynomial transform.</p> <blockquote> <p>Usually the numerical feature of interest is multiplied by itself, that is, raised to some power. Sometimes an ML practitioner can make an informed guess about the appropriate exponent. For example, many relationships in the physical world are related to squared terms, including acceleration due to gravity, the attenuation of light or sound over distance, and elastic potential energy.</p> </blockquote> <h2 id="label-leakage">Label Leakage</h2> <p>It’s important to prevent the model from getting the label as input during training, which is called label leakage.</p> <h3 id="why">Why?</h3> <p>If you include the label in your input features, the model <strong>cheats</strong> — it learns the answer directly rather than learning to predict it from the actual data.</p> <h3 id="example-2">Example</h3> <p>Imagine you’re training a model to predict house prices, and you accidentally include the actual sale price (price) as a feature:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">location</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">size</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">price</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># 🚫 WRONG
</span><span class="n">target</span> <span class="o">=</span> <span class="sh">'</span><span class="s">price</span><span class="sh">'</span>
</code></pre></div></div> <p>The model would just learn to copy the price column instead of learning how location and size affect price.</p> <h3 id="training-a-model-to-predict-dog-or-cat">training a model to predict dog or cat</h3> <p>When training a model to predict something (like “dog or cat”), you must provide both:</p> <ul> <li>Inputs: the images themselves (the pixel data — e.g. arrays, tensors, etc.)</li> <li>Labels: the correct answer (e.g. “dog” or “cat”) — this is what the model tries to learn to predict</li> </ul> <p>In this case, You do provide labels during training, but You do not include the label as part of the <strong>input</strong> features(like the image containing ‘dog’ letters).</p> <p>Let’s say you have:</p> <ul> <li>Image 1: 🐶 (dog) → label: 0</li> <li>Image 2: 🐱 (cat) → label: 1</li> </ul> <p>Training Code (simplified):</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">image_1</span><span class="p">,</span> <span class="n">image_2</span><span class="p">,</span> <span class="p">...]</span>   <span class="c1"># Input: pixel data only
</span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">...]</span>               <span class="c1"># Target: labels (dog = 0, cat = 1)
</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div> </div> </article> </div> <div class="col-sm-3 post-side-container"> <div class="card hoverable post-side-sticky"> <div class="card-body post-side-detail"> <h5 class="card-title">Post Detail</h5> <div class="detail-item"> <div class="item-title">Published : </div> <div class="item-content">Jun 9, 2025</div> </div> <div class="detail-item"> <div class="item-title">Category : </div> <div class="item-content"> <a href="/blog/category/ai"> <i class="fas fa-tag fa-sm"></i> AI</a> </div> </div> <div class="detail-item"> <div class="item-title">Tags : </div> </div> <div class="tags"> <a href="/blog/tag/python"> <div class="tag"> Python </div> </a> <a href="/blog/tag/ml"> <div class="tag"> ML </div> </a> <a href="/blog/tag/data-distribution"> <div class="tag"> Data-Distribution </div> </a> </div> <div class="detail-item"> <div class="item-title">Content : </div> </div> <div class="toc-container"> <div class="toc-item"> <a href="#standard-deviation"> # Standard Deviation </a> </div> <div class="toc-item"> <a href="#data-distribution"> # Data Distribution </a> </div> <div class="toc-item"> <a href="#normalize-data"> # Normalize data </a> </div> <div class="toc-item"> <a href="#polynomial-transforms"> # Polynomial transforms </a> </div> <div class="toc-item"> <a href="#label-leakage"> # Label Leakage </a> </div> </div> </div> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Ben Wen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/jquery.min.js?d41d8cd98f00b204e9800998ecf8427e"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="/assets/js/tex-mml-chtml.js?d41d8cd98f00b204e9800998ecf8427e"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
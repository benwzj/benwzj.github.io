<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>What is Transformer | BEN WEN</title> <meta name="author" content="Ben Wen"> <meta name="description" content="A website to show the world of Ben Wen "> <meta name="keywords" content="jekyll, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/mdb.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link defer rel="stylesheet" href="/assets/css/bootstrap-table.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/all.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/academicons.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/github.css?d41d8cd98f00b204e9800998ecf8427e" media="" id="highlight_theme_light"> <link rel="icon" href="/assets/img/favicon.png"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://benwzj.github.io/blog/2025/transformer/"> <link rel="stylesheet" href="/assets/css/native.css?d41d8cd98f00b204e9800998ecf8427e" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <style>html{scroll-behavior:smooth}</style> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">BEN WEN</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/about/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume</a> </li> <li class="nav-item"> <a class="nav-link" href="https://github.com/benwzj" rel="external nofollow noopener" target="_blank"> GitHub <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" style="width:1rem;height:1rem;fill:currentColor"> <g data-name="Layer 2"><g data-name="external-link"> <rect width="24" height="24" opacity="0"></rect> <path d="M20 11a1 1 0 0 0-1 1v6a1 1 0 0 1-1 1H6a1 1 0 0 1-1-1V6a1 1 0 0 1 1-1h6a1 1 0 0 0 0-2H6a3 3 0 0 0-3 3v12a3 3 0 0 0 3 3h12a3 3 0 0 0 3-3v-6a1 1 0 0 0-1-1z"></path> <path d="M16 5h1.58l-6.29 6.28a1 1 0 0 0 0 1.42 1 1 0 0 0 1.42 0L19 6.42V8a1 1 0 0 0 1 1 1 1 0 0 0 1-1V4a1 1 0 0 0-1-1h-4a1 1 0 0 0 0 2z"></path> </g></g> </svg> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="post col-sm-9"> <h1 class="post-title">What is Transformer</h1> <br> <br> <article class="post-content"> <div id="markdown-content"> <h2 id="what-is-transformer">What is Transformer</h2> <p>Transformer is a type of neural network architecture.</p> <ul> <li>Transformers were initially designed for translation, superseded RNN.</li> <li>Unlike traditional recurrent or convolutional models that process data sequentially, the Transformer leverages a mechanism called <strong>self-attention</strong> to process all input data simultaneously. This allows for much greater parallelization, leading to faster training and the ability to handle longer sequences of data effectively.</li> <li>Transformers are widely used in various fields, including natural language processing (NLP) for tasks like translation, text generation, and question answering, as well as computer vision.</li> <li>In essence, transformer models have revolutionized the way we process and understand sequential data by leveraging the power of attention and parallel processing.</li> <li>Like GPT, BERT, they are based on Transformers.</li> <li>Self-Attention and Positional Encoding are the main innovations.</li> </ul> <h2 id="key-concepts">Key Concepts</h2> <ul> <li>Self-Attention: The core innovation of Transformers. It allows the model to weigh the importance of different parts of the input when generating an output. For example, in a sentence, the word “it” might refer to different things depending on the context. Self-attention helps the model understand these relationships. It does this by calculating relationships between every word in a sequence and every other word, creating a weighted representation of the input.</li> <li>Attention Mechanism: A more general concept that allows the model to focus on specific parts of the input when generating an output. Self-attention is a specific type of attention.</li> <li>Encoder-Decoder Architecture: Many Transformers follow this structure. The encoder processes the input sequence and generates a contextualized representation. The decoder then uses this representation to generate the output sequence.</li> <li>Parallelization: Unlike recurrent networks that process input sequentially, Transformers can process all input tokens simultaneously, significantly speeding up training.</li> <li>Positional Encoding: Because Transformers don’t process sequentially, positional information of words in a sentence is lost. Positional encodings are added to the input embeddings to provide information about the position of each word.</li> <li>Feedforward Networks: Fully connected layers within each encoder and decoder layer that further process the information from the attention mechanism.</li> <li>Layer Normalization: A normalization technique used to stabilize training and improve performance.</li> </ul> <h2 id="how-a-transformer-works-simplified">How a Transformer works (simplified)</h2> <ul> <li>Input Embedding: The input sequence (e.g., a sentence) is converted into numerical representations called embeddings.</li> <li>Positional Encoding: Positional information is added to the embeddings.</li> <li>Encoder: Multiple encoder layers process the embeddings using self-attention and feedforward networks. Each encoder layer produces a set of encoded representations.</li> <li>Decoder: The decoder takes the encoded representations from the encoder and, using self-attention and feedforward networks, generates the output sequence (e.g., a translation, a summary, or the next word in a sentence). The decoder also uses attention mechanisms to focus on relevant parts of the encoded input.</li> <li>Output: The final decoder layer produces the output.</li> </ul> <h2 id="why-are-transformers-important">Why are Transformers important</h2> <p>Improved Performance: They have achieved state-of-the-art results in various NLP tasks. Parallelization: They train much faster than recurrent models. Handling Long Sequences: They can effectively process long sequences of data.</p> <h2 id="rnn">RNN</h2> <p>A recurrent neural network (RNN) is a type of neural network architecture specifically designed to process <strong>sequential</strong> data. it have many problems. Like:</p> <ul> <li>it struggle to learn long-range dependencies.</li> <li>Because sequential, it can’t be parallelized training. it is very slow.</li> </ul> <p>RNNs have been largely superseded by Transformer networks.</p> </div> </article> </div> <div class="col-sm-3 post-side-container"> <div class="card hoverable post-side-sticky"> <div class="card-body post-side-detail"> <h5 class="card-title">Post Detail</h5> <div class="detail-item"> <div class="item-title">Published : </div> <div class="item-content">May 5, 2025</div> </div> <div class="detail-item"> <div class="item-title">Category : </div> <div class="item-content"> <a href="/blog/category/ai"> <i class="fas fa-tag fa-sm"></i> AI</a> </div> </div> <div class="detail-item"> <div class="item-title">Tags : </div> </div> <div class="tags"> <a href="/blog/tag/ai"> <div class="tag"> AI </div> </a> <a href="/blog/tag/transformer"> <div class="tag"> Transformer </div> </a> </div> </div> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Ben Wen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/jquery.min.js?d41d8cd98f00b204e9800998ecf8427e"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="/assets/js/tex-mml-chtml.js?d41d8cd98f00b204e9800998ecf8427e"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Linear Regression | BEN WEN</title> <meta name="author" content="Ben Wen"> <meta name="description" content="A website to show the world of Ben Wen "> <meta name="keywords" content="jekyll, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/mdb.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link defer rel="stylesheet" href="/assets/css/bootstrap-table.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/all.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/academicons.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/github.css?d41d8cd98f00b204e9800998ecf8427e" media="" id="highlight_theme_light"> <link rel="icon" href="/assets/img/favicon.png"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://benwzj.github.io/blog/2025/linear-regression/"> <link rel="stylesheet" href="/assets/css/native.css?d41d8cd98f00b204e9800998ecf8427e" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <style>html{scroll-behavior:smooth}</style> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">BEN WEN</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/about/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume</a> </li> <li class="nav-item"> <a class="nav-link" href="https://github.com/benwzj" rel="external nofollow noopener" target="_blank"> GitHub <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" style="width:1rem;height:1rem;fill:currentColor"> <g data-name="Layer 2"><g data-name="external-link"> <rect width="24" height="24" opacity="0"></rect> <path d="M20 11a1 1 0 0 0-1 1v6a1 1 0 0 1-1 1H6a1 1 0 0 1-1-1V6a1 1 0 0 1 1-1h6a1 1 0 0 0 0-2H6a3 3 0 0 0-3 3v12a3 3 0 0 0 3 3h12a3 3 0 0 0 3-3v-6a1 1 0 0 0-1-1z"></path> <path d="M16 5h1.58l-6.29 6.28a1 1 0 0 0 0 1.42 1 1 0 0 0 1.42 0L19 6.42V8a1 1 0 0 0 1 1 1 1 0 0 0 1-1V4a1 1 0 0 0-1-1h-4a1 1 0 0 0 0 2z"></path> </g></g> </svg> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="post col-sm-9"> <h1 class="post-title">Linear Regression</h1> <br> <br> <article class="post-content"> <div id="markdown-content"> <h2 id="what-is-linear-regression">What is Linear Regression</h2> <p>Training data to form a model, simply say, it is to find the bias and weights among the data. linear regression is one of the methods that find the relationship between features and a label to get the bias and weights.</p> <figure> <picture> <img src="/assets/img/car-data-points-with-model.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>During training, the model calculates the <strong>weight</strong> and <strong>bias</strong> that produce the best model.</p> <figure> <picture> <img src="/assets/img/linear-regression-equation.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="loss">Loss</h2> <p>Loss is a numerical metric that describes how wrong a model’s predictions are. Loss measures the distance between the model’s predictions and the actual labels. The goal of training a model is to minimize the loss, reducing it to its lowest possible value.</p> <figure> <picture> <img src="/assets/img/loss-lines.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The two most common methods to remove the sign are the following:</p> <ul> <li>Take the absolute value of the difference between the actual value and the prediction.</li> <li>Square the difference between the actual value and the prediction.</li> </ul> <p>There are four main types of loss:</p> <ul> <li> <strong>L1 loss</strong>: The sum of the absolute values of the difference between the predicted values and the actual values.</li> <li> <strong>Mean absolute error (MAE)</strong>: The average of L1 losses across a set of examples.</li> <li> <strong>L2 loss</strong>: The sum of the squared difference between the predicted values and the actual values.</li> <li> <strong>Mean squared error (MSE)</strong>: The average of L2 losses across a set of examples.</li> </ul> <h3 id="choosing-a-loss">Choosing a loss</h3> <p>In training, model will try to get the best bias and weights according to the LOSS. So choosing a loss is matter.</p> <p>When choosing the best loss function, also consider how you want the model to treat outliers. The outliers are closer to the model trained with MSE than to the model trained with MAE.</p> <figure> <picture> <img src="/assets/img/model-mse.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>A model trained with MSE moves the model closer to the outliers.</p> <figure> <picture> <img src="/assets/img/model-mae.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>A model trained with MAE is farther from the outliers.</p> <h2 id="gradient-descent">Gradient descent</h2> <p>Gradient descent is an iterative process that finds the best weights and bias that minimize the loss.</p> <p>Gradient descent is a <strong>mathematical technique</strong> that iteratively finds the weights and bias that produce the model with the lowest loss. Gradient descent finds the best weight and bias by repeating the following process for a number of user-defined iterations.</p> <p>The model begins training with randomized weights and biases near zero, and then repeats the following steps:</p> <ul> <li>Calculate the loss with the current weight and bias.</li> <li>Determine the direction to move the weights and bias that reduce loss.</li> <li>Move the weight and bias values a small amount in the direction that reduces loss.</li> <li>Return to step one and repeat the process until the model can’t reduce the loss any further.</li> </ul> <p>This is typical loss curve, Loss is on the y-axis and iterations are on the x-axis:</p> <figure> <picture> <img src="/assets/img/loss-convergence.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Loss surface showing the weight and bias values that produce the lowest loss.</p> <figure> <picture> <img src="/assets/img/loss-surface-points.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="hyperparameters">Hyperparameters</h2> <p>Hyperparameters are variables that control different aspects of training. Three common hyperparameters are:</p> <ul> <li>Learning rate</li> <li>Batch size</li> <li>Epochs</li> </ul> <p>In contrast, parameters are the variables, like the weights and bias, that are part of the model itself. In other words, hyperparameters are values that you control; parameters are values that the model calculates during training.</p> <h3 id="learning-rate">Learning rate</h3> <p>The learning rate determines the magnitude of the changes to make to the weights and bias during each step of the gradient descent process.</p> <p>The model multiplies the gradient by the learning rate to determine the model’s parameters (weight and bias values) for the next iteration. For example, if the <strong>gradient’s magnitude</strong> is 2.5 and the learning rate is 0.01, then the model will change the parameter by 0.025.</p> <p>Learning rate is a floating point number you set that influences how quickly the model converges.</p> <p>The ideal learning rate helps the model to converge within a reasonable number of iterations.</p> <p>What do it means when Learning Rate is 1? A learning rate of 1 means that the model updates its weights by the full amount of the calculated gradient. This almost inevitably leads to highly unstable training and the model failing to converge to a good solution.</p> <h3 id="batch-size">Batch size</h3> <p>Batch size is a hyperparameter that refers to the number of examples the model processes before updating its weights and bias.</p> <p>You might think that the model should do Full Batch, means calculating the loss for every example in the dataset before updating the weights and bias. However, when a dataset contains hundreds of thousands or even millions of examples, using the full batch isn’t practical.</p> <p>Two common techniques to get the right gradient on average without needing to look at every example in the dataset before updating the weights and bias:</p> <ul> <li>Stochastic gradient descent (SGD): Stochastic gradient descent uses only a single example (a batch size of one) per iteration. The term “stochastic” indicates that the one example comprising each batch is chosen at random. (Note that using stochastic gradient descent can produce noise throughout the entire loss curve, not just near convergence.)</li> <li>Mini-batch stochastic gradient descent (mini-batch SGD): Mini-batch stochastic gradient descent is a compromise between full-batch and SGD. The model chooses the examples included in each batch at random, averages their gradients, and then updates the weights and bias once per iteration.</li> </ul> <h3 id="epochs">Epochs</h3> <p>During training, an epoch means that the model has processed every example in the training set once. For example, given a training set with 1,000 examples and a mini-batch size of 100 examples, it will take the model 10 iterations to complete one epoch.</p> <p>Training typically requires many epochs. In general, more epochs produces a better model, but also takes more time to train.</p> <p>Here is an example to tell the difference:</p> <ul> <li>Full batch: After the model looks at all the examples in the dataset. For instance, if a dataset contains 1,000 examples and the model trains for 20 epochs, the model updates the weights and bias 20 times, once per epoch.</li> <li>Stochastic gradient descent: After the model looks at a single example from the dataset. For instance, if a dataset contains 1,000 examples and trains for 20 epochs, the model updates the weights and bias 20,000 times.</li> <li>Mini-batch stochastic gradient descent: After the model looks at the examples in each batch. For instance, if a dataset contains 1,000 examples, and the batch size is 100, and the model trains for 20 epochs, the model updates the weights and bias 200 times.</li> </ul> <h2 id="generate-a-correlation-matrix">Generate a correlation matrix</h2> <p>An important part of machine learning is determining which features correlate with the label. you can use a correlation matrix to identify features whose values correlate well with the label.</p> <p>Correlation values have the following meanings:</p> <ul> <li>1.0: perfect positive correlation; that is, when one attribute rises, the other attribute rises.</li> <li>-1.0: perfect negative correlation; that is, when one attribute rises, the other attribute falls.</li> <li>0.0: no correlation; the two columns are not linearly related. In general, the higher the absolute value of a correlation value, the greater its predictive power.</li> </ul> <p>dataframe can provide such function: <code class="language-plaintext highlighter-rouge">training_df.corr(numeric_only = True)</code></p> <h2 id="visualize-relationships-in-dataset">Visualize relationships in dataset</h2> <p>dataframe provide such function: <code class="language-plaintext highlighter-rouge">sns.pairplot(training_df, x_vars=["FARE", "TRIP_MILES", "TRIP_SECONDS"], y_vars=["FARE", "TRIP_MILES", "TRIP_SECONDS"])</code></p> <h2 id="references">References</h2> </div> </article> </div> <div class="col-sm-3 post-side-container"> <div class="card hoverable post-side-sticky"> <div class="card-body post-side-detail"> <h5 class="card-title">Post Detail</h5> <div class="detail-item"> <div class="item-title">Published : </div> <div class="item-content">May 6, 2025</div> </div> <div class="detail-item"> <div class="item-title">Category : </div> <div class="item-content"> <a href="/blog/category/ai"> <i class="fas fa-tag fa-sm"></i> AI</a> </div> </div> <div class="detail-item"> <div class="item-title">Tags : </div> </div> <div class="tags"> <a href="/blog/tag/ai"> <div class="tag"> AI </div> </a> <a href="/blog/tag/ml"> <div class="tag"> ML </div> </a> </div> </div> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Ben Wen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/jquery.min.js?d41d8cd98f00b204e9800998ecf8427e"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="/assets/js/tex-mml-chtml.js?d41d8cd98f00b204e9800998ecf8427e"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
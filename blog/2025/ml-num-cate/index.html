<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Numerical and Categorial Data | BEN WEN</title> <meta name="author" content="Ben Wen"> <meta name="description" content="A website to show the world of Ben Wen "> <meta name="keywords" content="jekyll, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/mdb.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link defer rel="stylesheet" href="/assets/css/bootstrap-table.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/all.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/academicons.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/github.css?d41d8cd98f00b204e9800998ecf8427e" media="" id="highlight_theme_light"> <link rel="icon" href="/assets/img/favicon.png"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://benwzj.github.io/blog/2025/ml-num-cate/"> <link rel="stylesheet" href="/assets/css/native.css?d41d8cd98f00b204e9800998ecf8427e" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <style>html{scroll-behavior:smooth}</style> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">BEN WEN</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/about/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume</a> </li> <li class="nav-item"> <a class="nav-link" href="https://github.com/benwzj" rel="external nofollow noopener" target="_blank"> GitHub <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" style="width:1rem;height:1rem;fill:currentColor"> <g data-name="Layer 2"><g data-name="external-link"> <rect width="24" height="24" opacity="0"></rect> <path d="M20 11a1 1 0 0 0-1 1v6a1 1 0 0 1-1 1H6a1 1 0 0 1-1-1V6a1 1 0 0 1 1-1h6a1 1 0 0 0 0-2H6a3 3 0 0 0-3 3v12a3 3 0 0 0 3 3h12a3 3 0 0 0 3-3v-6a1 1 0 0 0-1-1z"></path> <path d="M16 5h1.58l-6.29 6.28a1 1 0 0 0 0 1.42 1 1 0 0 0 1.42 0L19 6.42V8a1 1 0 0 0 1 1 1 1 0 0 0 1-1V4a1 1 0 0 0-1-1h-4a1 1 0 0 0 0 2z"></path> </g></g> </svg> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="post col-sm-9"> <h1 class="post-title">Numerical and Categorial Data</h1> <br> <br> <article class="post-content"> <div id="markdown-content"> <p>ML practitioners spend far more time evaluating, cleaning, and transforming data than building models. Here introduce how to handle dataset before training a model.</p> <p>We devide data into tow types:</p> <ul> <li>numerical data</li> <li>categorical data</li> </ul> <h2 id="numerical-data">Numerical Data</h2> <p>This unit focuses on numerical data, meaning integers or floating-point values that behave like numbers. That is, they are additive, countable, ordered, and so on.</p> <p>In real world, you will transfer many non-numerical features into Numerical features for ML purpose.</p> <h3 id="understand-feature-vector">Understand Feature Vector</h3> <p>The model actually don’t use raw values. It ingests feature vectors which are array of <strong>floating-point</strong> values.</p> <p>A Feature Vector is an array of feature values comprising an example. The feature vector is input during training and during inference.</p> <p>Usually, people will use Feature Engineering technique to transfer raw data into Feature vector.</p> <h4 id="feature-engineering">Feature Engineering</h4> <p>Transfering raw data into Feature vectors is called feature engineering. Every value in a feature vector must be a <strong>floating-point</strong> value.</p> <p>Many features are naturally strings or other non-numerical values. You can use feature engineering to represent non-numerical values as <strong>numerical</strong> values.</p> <p>The most common feature engineering techniques are:</p> <ul> <li>Normalization: Converting numerical values into a standard range.</li> <li>Binning (also referred to as bucketing): Converting numerical values into buckets of ranges.</li> </ul> <h3 id="feature-engineering-technique">Feature engineering technique</h3> <h4 id="normalization">Normalization</h4> <p>The goal of normalization is to transform features to be on a similar scale. Normalization methods:</p> <ul> <li>linear scaling</li> <li>Z-score scaling</li> <li>log scaling</li> <li>Clipping</li> </ul> <h4 id="binning">Binning</h4> <p>Binning (also called bucketing) is a feature engineering technique that groups different numerical subranges into bins or buckets. In many cases, binning turns numerical data into categorical data.</p> <p>When a feature appears more clumpy than linear, binning is a much better way to represent the data. When using Binning, the model can learn separate weights for each bin.</p> <p>Binning is a good alternative to scaling or clipping when either of the following conditions is met:</p> <ul> <li>The overall linear relationship between the feature and the label is weak or nonexistent.</li> <li>When the feature values are clustered.</li> </ul> <p>Binning example: Suppose you are creating a model that predicts the number of shoppers by the outside temperature for that day. The number of shoppers was highest when the temperature was most comfortable. So, the plot doesn’t really show any sort of linear relationship between the label and the feature value.</p> <figure> <picture> <img src="/assets/img/binning_temperature_vs_shoppers_divided_into_3_bins.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The graph suggests three clusters in the following subranges:</p> <ul> <li>Bin 1 is the temperature range 4-11.</li> <li>Bin 2 is the temperature range 12-26.</li> <li>Bin 3 is the temperature range 27-36.</li> </ul> <p>The model learns separate weights for each bin.</p> <h5 id="quantile-bucketing">Quantile bucketing</h5> <p><strong>Quantile bucketing</strong> creates bucketing boundaries such that the number of examples in each bucket is exactly or nearly equal.</p> <h4 id="scrubbing">Scrubbing</h4> <p>Many examples in datasets are unreliable. You can write a program or script to detect any of the following problems:</p> <ul> <li>Omitted values</li> <li>Duplicate examples</li> <li>Out-of-range feature values</li> </ul> <p>This is Scrubbing.</p> <h4 id="qualities-of-good-numerical-features">Qualities of good numerical features</h4> <ul> <li>Clearly named: Not recommended: <code class="language-plaintext highlighter-rouge">house_age: 851472000</code>; Recommended: <code class="language-plaintext highlighter-rouge">house_age_years: 27</code> </li> <li>Checked or tested before training: Bad <code class="language-plaintext highlighter-rouge">user_age_in_years: 224</code>; OK <code class="language-plaintext highlighter-rouge">user_age_in_years: 24</code> </li> <li>Sensible: A “magic value” is a purposeful discontinuity in an otherwise continuous feature.</li> </ul> <h4 id="polynomial-transforms">Polynomial transforms</h4> <p>A Polynomial Transform is a technique used to map your original input features into a higher-dimensional space by generating polynomial combinations of the original features.</p> <h3 id="steps-of-handling-numerical-data">Steps of handling numerical Data</h3> <h4 id="visualize-your-data">Visualize your data</h4> <p>Visualizations help you continually check your assumptions. Use Pandas for visualization:</p> <ul> <li>Working with Missing Data: <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html" rel="external nofollow noopener" target="_blank">pandas Documentation</a> </li> <li>Visualizations <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html" rel="external nofollow noopener" target="_blank">pandas Documentation</a> </li> </ul> <h4 id="statistically-evaluate-your-data">Statistically evaluate your data</h4> <p>Use Pandas <code class="language-plaintext highlighter-rouge">describe()</code></p> <h4 id="find-outliers">Find outliers</h4> <p>There are some common rule to find outliers. For example: When the delta between the 0th and 25th percentiles differs significantly from the delta between the 75th and 100th percentiles, the dataset probably contains outliers.</p> <ul> <li>The outlier can be due to a mistake.</li> <li>The outlier is a legitimate data point, you can keep these outliers in your training set. But extreme outliers can still hurt your model. You can delete the outliers or apply more invasive feature engineering techniques, such as clipping.</li> </ul> <h3 id="numerical-data-best-practices">Numerical data Best practices</h3> <p>Best practices for working with numerical data:</p> <ul> <li>Remember that your ML model interacts with the data in the feature vector, not the data in the dataset.</li> <li>Normalize most numerical features.</li> <li>If your first normalization strategy doesn’t succeed, consider a different way to normalize your data.</li> <li>Binning, also referred to as bucketing, is sometimes better than normalizing.</li> <li>Considering what your data should look like, write verification tests to validate those expectations. For example: <ul> <li>The absolute value of latitude should never exceed 90. You can write a test to check if a latitude value greater than 90 appears in your data.</li> <li>If your data is restricted to the state of Florida, you can write tests to check that the latitudes fall between 24 through 31, inclusive.</li> </ul> </li> <li>Visualize your data with scatter plots and histograms. Look for anomalies.</li> <li>Gather statistics not only on the entire dataset but also on smaller subsets of the dataset. That’s because aggregate statistics sometimes obscure problems in smaller sections of a dataset.</li> <li>Document all your data transformations.</li> </ul> <h2 id="categorical-data">Categorical Data</h2> <p>Categorical Data is the Data which behave like categories. The numerical data can be Categorical Data. Because models can only train on floating-point values, Categorical Data need to be <strong>Encoded</strong>.</p> <ul> <li>How to encode categorical data? <ul> <li>For low-dimensional categorical, one important concept is <strong>one-hot encoding</strong>.</li> <li>But for most of categorical data will be high-dimensional. Embedding is the big part of the encoding methods.</li> </ul> </li> <li>Numerical data is often recorded by scientific instruments or automated measurements. Categorical data, on the other hand, is often categorized by human beings(gold labels) or by machine learning models(silver labels). Who decides on categories and labels, and how they make those decisions, affects the reliability and usefulness of that data.</li> </ul> <h3 id="low-dimensional-categorical-features">Low-dimensional categorical features</h3> <p>Vocabulary encoding is for a low number of possible categories.</p> <p>With a vocabulary encoding, the model treats each possible categorical value as a separate feature. During training, the model learns different weights for each category.</p> <h4 id="index-numbers">Index numbers</h4> <p>First you must convert each category string to a unique index number.</p> <h4 id="one-hot-encoding">One-hot encoding</h4> <p>The next step in building a vocabulary is to convert each index number to its one-hot encoding.</p> <p>In a one-hot encoding:</p> <ul> <li>Each category is represented by a vector (array) of <code class="language-plaintext highlighter-rouge">N</code> elements, where <code class="language-plaintext highlighter-rouge">N</code> is the number of categories. For example, if car_color has eight possible categories, then the one-hot vector representing will have eight elements.</li> <li>Exactly one of the elements in a one-hot vector has the value 1.0; all the remaining elements have the value 0.0.</li> </ul> <blockquote class="block-warning"> <p>It is the one-hot vector, not the string or the index number, that gets passed to the feature vector. The model learns a separate weight for each element of the feature vector.</p> </blockquote> <h4 id="sparse-representation">sparse representation</h4> <p>Sparse representation means storing the position of the 1.0 in a sparse vector. For example, the one-hot vector for “Blue” is: <code class="language-plaintext highlighter-rouge">[0, 0, 1, 0, 0, 0, 0, 0]</code> Since the <code class="language-plaintext highlighter-rouge">1</code> is in position <code class="language-plaintext highlighter-rouge">2</code> (when starting the count at 0), the sparse representation for the preceding one-hot vector is: <code class="language-plaintext highlighter-rouge">2</code></p> <p>Sparse representation consumes far less memory than the eight-element one-hot vector.</p> <blockquote class="block-warning"> <p>Importantly, the model must train on the one-hot vector, not the sparse representation.</p> </blockquote> <h3 id="high-dimensional-categorical-features">High-dimensional categorical features</h3> <p>Some categorical features have a high number of dimensions, like <code class="language-plaintext highlighter-rouge">words_in_english</code>.</p> <p>When the number of categories is high, one-hot encoding is usually a bad choice.</p> <ul> <li>Embeddings are usually a much better choice. Embeddings substantially reduce the number of dimensions, which benefits models in two important ways: <ul> <li>The model typically trains faster.</li> <li>The built model typically infers predictions more quickly. That is, the model has lower latency.</li> </ul> </li> <li>Hashing (also called the hashing trick) is a less common way to reduce the number of dimensions.</li> </ul> <h3 id="what-is-feature-crosses">What is feature crosses</h3> <p>Feature crosses are created by crossing two or more categorical or bucketed features of the dataset.</p> <p>For example, consider a leaf dataset with the categorical features:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">edges</code>, containing values <code class="language-plaintext highlighter-rouge">smooth</code>, <code class="language-plaintext highlighter-rouge">toothed</code>, and <code class="language-plaintext highlighter-rouge">lobed</code> </li> <li> <code class="language-plaintext highlighter-rouge">arrangement</code>, containing values <code class="language-plaintext highlighter-rouge">opposite</code> and <code class="language-plaintext highlighter-rouge">alternate</code> </li> </ul> <p>The feature cross of these two features would be: <code class="language-plaintext highlighter-rouge">{Smooth_Opposite, Smooth_Alternate, Toothed_Opposite, Toothed_Alternate, Lobed_Opposite, Lobed_Alternate}</code></p> <h4 id="feature-crosses-vs-polynomial-transforms">feature crosses vs. polynomial transforms</h4> <p>Feature crosses are somewhat analogous to Polynomial transforms. Both combine multiple features into a new synthetic feature that the model can train on to learn nonlinearities.</p> <ul> <li>Polynomial transforms typically combine numerical data, while feature crosses combine categorical data.</li> <li>Like polynomial transforms, feature crosses allow linear models to handle nonlinearities. Feature crosses also encode interactions between features.</li> </ul> <h4 id="when-to-use-feature-crosses">When to use feature crosses</h4> <ul> <li>Domain knowledge can suggest a useful combination of features to cross. Without that domain knowledge, it can be difficult to determine effective feature crosses or polynomial transforms by hand.</li> <li>It’s often possible, if computationally expensive, to use <strong>neural networks</strong> to automatically find and apply useful feature combinations during training.</li> </ul> </div> </article> </div> <div class="col-sm-3 post-side-container"> <div class="card hoverable post-side-sticky"> <div class="card-body post-side-detail"> <h5 class="card-title">Post Detail</h5> <div class="detail-item"> <div class="item-title">Published : </div> <div class="item-content">Jun 16, 2025</div> </div> <div class="detail-item"> <div class="item-title">Category : </div> <div class="item-content"> <a href="/blog/category/ai"> <i class="fas fa-tag fa-sm"></i> AI</a> </div> </div> <div class="detail-item"> <div class="item-title">Tags : </div> </div> <div class="tags"> <a href="/blog/tag/python"> <div class="tag"> Python </div> </a> <a href="/blog/tag/ml"> <div class="tag"> ML </div> </a> </div> <div class="detail-item"> <div class="item-title">Content : </div> </div> <div class="toc-container"> <div class="toc-item"> <a href="#numerical-data"> # Numerical Data </a> </div> <div class="toc-item"> <a href="#understand-feature-vector">     # Understand Feature Vector </a> </div> <div class="toc-item"> <a href="#feature-engineering-technique">     # Feature engineering technique </a> </div> <div class="toc-item"> <a href="#steps-of-handling-numerical-data">     # Steps of handling numerical Data </a> </div> <div class="toc-item"> <a href="#numerical-data-best-practices">     # Numerical data Best practices </a> </div> <div class="toc-item"> <a href="#categorical-data"> # Categorical Data </a> </div> <div class="toc-item"> <a href="#low-dimensional-categorical-features">     # Low-dimensional categorical features </a> </div> <div class="toc-item"> <a href="#high-dimensional-categorical-features">     # High-dimensional categorical features </a> </div> <div class="toc-item"> <a href="#what-is-feature-crosses">     # What is feature crosses </a> </div> </div> </div> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Ben Wen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/jquery.min.js?d41d8cd98f00b204e9800998ecf8427e"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="/assets/js/tex-mml-chtml.js?d41d8cd98f00b204e9800998ecf8427e"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>
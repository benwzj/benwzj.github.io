<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Embedding | BEN WEN</title> <meta name="author" content="Ben Wen"> <meta name="description" content="A website to show the world of Ben Wen "> <meta name="keywords" content="jekyll, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/mdb.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link defer rel="stylesheet" href="/assets/css/bootstrap-table.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/all.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/academicons.min.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/github.css?d41d8cd98f00b204e9800998ecf8427e" media="" id="highlight_theme_light"> <link rel="icon" href="/assets/img/favicon.png"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://benwzj.github.io/blog/2025/embedding/"> <link rel="stylesheet" href="/assets/css/native.css?d41d8cd98f00b204e9800998ecf8427e" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <style>html{scroll-behavior:smooth}</style> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">BEN WEN</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/about/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Resume</a> </li> <li class="nav-item"> <a class="nav-link" href="https://github.com/benwzj" rel="external nofollow noopener" target="_blank"> GitHub <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" style="width:1rem;height:1rem;fill:currentColor"> <g data-name="Layer 2"><g data-name="external-link"> <rect width="24" height="24" opacity="0"></rect> <path d="M20 11a1 1 0 0 0-1 1v6a1 1 0 0 1-1 1H6a1 1 0 0 1-1-1V6a1 1 0 0 1 1-1h6a1 1 0 0 0 0-2H6a3 3 0 0 0-3 3v12a3 3 0 0 0 3 3h12a3 3 0 0 0 3-3v-6a1 1 0 0 0-1-1z"></path> <path d="M16 5h1.58l-6.29 6.28a1 1 0 0 0 0 1.42 1 1 0 0 0 1.42 0L19 6.42V8a1 1 0 0 0 1 1 1 1 0 0 0 1-1V4a1 1 0 0 0-1-1h-4a1 1 0 0 0 0 2z"></path> </g></g> </svg> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="post col-sm-9"> <h1 class="post-title">Embedding</h1> <br> <br> <article class="post-content"> <div id="markdown-content"> <p>Embeddings is the basic key point to understand Machine Learning. And it is Machine Learning’s Most Useful Multitool.</p> <p>Embeddings are a powerful technique in AI, enabling machines to understand and work with complex data in a more meaningful way. They play a crucial role in various applications, from natural language processing to computer vision and recommendation systems.</p> <p>In machine learning, an embedding is a way of representing data as points in n-dimensional space so that similar data points cluster together.</p> <h2 id="what-are-embeddings">What are Embeddings</h2> <p>Abstractively, embeddings allow us to find similar data points. The machine can understand things because of embedding. Realistically, embeddings store semantically meaning in arrays of number.</p> <p>What kinds of things can be embedded? All The Things! Text, Images, Videos, Music.</p> <p>Embeddings are typically learned from large datasets using machine learning techniques.</p> <h3 id="understand-embeddings">Understand Embeddings</h3> <p>When talk about embeddings, i though it is just lots of array of numbers which telling about the things. But how to define this dimensions? what algorithm to operate this dimensions? Embedding should be a whole system, it including the start and the end. it include algorithms, data training, etc. It can embed your input into arrays, it can understand those arrays, it can capture semantic relationships, and it can inference.</p> <p>So Creating embeddings starts with a large dataset. For example word embeddings, this is a text corpus. We define a model, often a <strong>shallow neural network</strong> like in Word2Vec. Word2Vec’s skip-gram architecture, for instance, predicts surrounding context words given a target word. The model’s hidden layer weights become the word embeddings.</p> <h2 id="word-embedding">Word Embedding</h2> <p>Word embeddings are extremely useful in natural language processing. They can be used to find synonyms (“semantic similarity”), to do clustering, or as a preprocessing step for a more complicated nlp model.</p> <p>Example: Imagine the words “cat,” “dog,” and “car.” Their embeddings might look like this (simplified):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat: [0.8, 0.2, 0.1]
dog: [0.7, 0.3, 0.2]
car: [0.1, 0.1, 0.9]
</code></pre></div></div> <p>Notice that “cat” and “dog” have similar embeddings, reflecting their semantic similarity, while “car” has a very different embedding.</p> <h2 id="pre-trained-models">Pre-trained Models</h2> <p>This pre-trained models also called Techniques which are used to create embeddings.</p> <p>A pre-trained embedding model is a model that has already been trained on a large dataset and can be used to generate embeddings for new data without further training (or with minimal fine-tuning). Think of it as a ready-to-use embedding generator.</p> <h3 id="some-famous-ones-are">Some famous ones are:</h3> <ul> <li>Word2Vec: a technique invented by Google in 2013. It Learns word embeddings by predicting surrounding words given a target word (or vice versa).</li> <li>GloVe (Global Vectors for Word Representation): Learns word embeddings by capturing global word co-occurrence statistics.</li> <li>FastText: An extension of Word2Vec that considers subword information, allowing it to generate embeddings for out-of-vocabulary words.</li> <li>Sentence Transformers: Generate embeddings for entire sentences or paragraphs.</li> <li>Graph Embeddings: Represent nodes in a graph as vectors.</li> </ul> <h3 id="how-to-use-a-pre-trained-embedding-model">How to use a pre-trained embedding model:</h3> <ul> <li>Choose a Model: Select a pre-trained model that suits your task and data. Popular choices include Word2Vec, GloVe, FastText, and sentence transformers like BERT. Consider factors like the size of the vocabulary, the dimensionality of the embeddings, and the type of data the model was trained on.</li> <li>Load the Model: Use a library like <code class="language-plaintext highlighter-rouge">Gensim</code> or <code class="language-plaintext highlighter-rouge">Hugging Face Transformers</code> to load the pre-trained model.</li> <li>Generate Embeddings: Input your data (e.g., words, sentences) into the loaded model to generate embeddings.</li> <li>Use the Embeddings: Use the generated embeddings as input features for your machine learning model or for other tasks like semantic search.</li> </ul> <p>Let’s say you want to build a sentiment analysis model. You could use pre-trained word embeddings from <strong>GloVe</strong>. You would load the GloVe model, then for each word in your input text, you would retrieve its corresponding pre-trained embedding vector. These vectors would then be used as input features for your sentiment analysis model.</p> <h3 id="example">Example</h3> <p>Using <code class="language-plaintext highlighter-rouge">google/universal-sentence-encoder</code> pre-trained model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow_hub</span> <span class="k">as</span> <span class="n">hub</span>

<span class="n">embed</span> <span class="o">=</span> <span class="n">hub</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">https://tfhub.dev/google/universal-sentence-encoder/4</span><span class="sh">"</span><span class="p">)</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="nf">embed</span><span class="p">([</span>
    <span class="sh">"</span><span class="s">The quick brown fox jumps over the lazy dog.</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">I am a sentence for which I would like to get its embedding</span><span class="sh">"</span><span class="p">])</span>

<span class="nf">print</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

<span class="c1"># Response looks like: [[0.001, 0.201, ...]]
# i.e., an array of vectors
</span></code></pre></div></div> <h2 id="what-is-word2vec-exactly">What is Word2vec exactly</h2> <p>Word2Vec is categorize as embedding model while BERT as a Language Model. Word2Vec provide methods, algorithms, neural network for embedding purpose. BERT can work to embedding, also can do much more various NLP tasks like generating contextualized word and sentence embeddings.</p> <h3 id="technique">Technique</h3> <p>You can say Word2vec is a technique. This technique is used for obtaining vector representations of words in natural language processing (NLP). These vectors capture information about the meaning of the word based on the surrounding words.</p> <h3 id="model">Model</h3> <p>You can say Word2vec is a group of models.<br> Word2vec is composed of a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words.</p> <h3 id="how-word2vec-approach">How Word2vec approach</h3> <p>Word2vec takes as its input a large corpus of text and produces a mapping of the set of words to a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a vector in the space. (Word2vec 将大量文本作为输入，并将词集映射到向量空间（通常有几百维），语料库中每个唯一的词都会在空间中分配一个向量。)</p> <p>Here are a little bit more detail: Word2vec can use either of two model architectures to produce these distributed representations of words: continuous bag of words (CBOW) or continuously sliding skip-gram. In both architectures, word2vec considers both individual words and a sliding context window as it iterates over the corpus.</p> <ul> <li>The CBOW can be viewed as a ‘fill in the blank’ task.</li> <li>In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words.</li> <li>CBOW is faster while skip-gram does a better job for infrequent words.</li> <li>A corpus is a sequence of words. Both CBOW and skip-gram are methods to learn one vector per word appearing in the corpus.</li> </ul> <h3 id="how-to-use-word2vec">How to use Word2Vec</h3> <p>Using Word2Vec involves two main steps: training the model (optional, as pre-trained models are available) and then using the trained model to generate word embeddings.</p> <h4 id="training-optional">Training (Optional)</h4> <p>This is optional, as pre-trained models are available. But you can still train it with your own data.</p> <h4 id="using-the-model">Using the Model</h4> <p>Load the Model:</p> <pre><code class="language-Python">from gensim.models import Word2Vec
model = Word2Vec.load("word2vec.model") # Or load a pre-trained model
</code></pre> <p>Get Word Embeddings:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vector</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">[</span><span class="sh">'</span><span class="s">cat</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># Get embedding for 'cat'
</span></code></pre></div></div> <p>Find Similar Words:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">similar_words</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="nf">most_similar</span><span class="p">(</span><span class="sh">'</span><span class="s">cat</span><span class="sh">'</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># Find the 5 most similar words to 'cat'
</span></code></pre></div></div> <p>Perform Word Arithmetic (Analogy):</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="nf">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">king</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">woman</span><span class="sh">'</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">man</span><span class="sh">'</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># king - man + woman = ? (queen)
</span></code></pre></div></div> <p>Use in Downstream Tasks: Use the embeddings as features in machine learning models for tasks like:</p> <ul> <li>Text Classification: Sentiment analysis, spam detection.</li> <li>Machine Translation: Encoding and decoding text in different languages.</li> <li>Information Retrieval: Semantic search.</li> <li>Recommendation Systems: Recommending similar items.</li> </ul> <h2 id="create-your-own-embeddings">Create Your Own Embeddings</h2> <p>Usually, when we are talking about Creating Our Own Embeddings, usually refer to fine-tuning some pre-trained model.</p> <p>Actually, you can creat one from scratch. Or saying train one. Here’s what you’ll need:</p> <ul> <li>Familiarity with Python and a deep learning framework like TensorFlow or PyTorch is <strong>essential</strong>.</li> <li>Data: A sufficiently large and relevant dataset is crucial.</li> <li>Algorithm: Choose an appropriate embedding algorithm. Word2Vec, GloVe, and FastText are good starting points for word embeddings.</li> <li>Computational Resources Access to a decent CPU, and ideally a GPU, will significantly speed up the process.</li> <li>Patience and Experimentation: requires patience and a willingness to experiment with different hyperparameters and architectures to achieve optimal results.</li> </ul> <p>Here are the highly simplified example:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Sample vocabulary and data
</span><span class="n">vocabulary</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">the</span><span class="sh">"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">cat</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">"</span><span class="s">sat</span><span class="sh">"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="sh">"</span><span class="s">on</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="sh">"</span><span class="s">mat</span><span class="sh">"</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="sh">"</span><span class="s">the</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">cat</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sat</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">on</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">the</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">mat</span><span class="sh">"</span><span class="p">]]</span>

<span class="c1"># Hyperparameters
</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">window_size</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Initialize embeddings randomly
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>

<span class="c1"># Training loop (simplified)
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>  <span class="c1"># Example number of epochs
</span>    <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)):</span>
            <span class="n">target_word</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">context_words</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">[</span><span class="nf">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">window_size</span><span class="p">):</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">sentence</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="nf">min</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">),</span> <span class="n">i</span> <span class="o">+</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>

            <span class="c1"># Simplified training logic (replace with actual gradient updates)
</span>            <span class="k">for</span> <span class="n">context_word</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">:</span>
                <span class="n">target_index</span> <span class="o">=</span> <span class="n">vocabulary</span><span class="p">[</span><span class="n">target_word</span><span class="p">]</span>
                <span class="n">context_index</span> <span class="o">=</span> <span class="n">vocabulary</span><span class="p">[</span><span class="n">context_word</span><span class="p">]</span>
                <span class="c1"># ... (Calculate gradients and update embeddings) ...
</span>
<span class="nf">print</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span> <span class="c1"># Your trained embeddings
</span></code></pre></div></div> <p>But A real implementation, it would involve things like</p> <ul> <li>A neural network: For predicting context words.</li> <li>Backpropagation: For calculating gradients.</li> <li>An optimization algorithm: Like SGD for updating embeddings.</li> </ul> <h2 id="faq">FAQ</h2> <ul> <li>When talking embedding, is it refer to an array of number?</li> <li>Who define the dimensions?</li> <li>Word2Vec is a pre-trained model or just a Techniques which difine demensions?</li> <li>How to create our own embedding model?</li> <li>What is the difference between text and image embedding?</li> <li>What is the relationship of Word2vec and Transformer?</li> <li>Can say BERT is a embedding model?</li> </ul> </div> </article> </div> <div class="col-sm-3 post-side-container"> <div class="card hoverable post-side-sticky"> <div class="card-body post-side-detail"> <h5 class="card-title">Post Detail</h5> <div class="detail-item"> <div class="item-title">Published : </div> <div class="item-content">May 6, 2025</div> </div> <div class="detail-item"> <div class="item-title">Category : </div> <div class="item-content"> <a href="/blog/category/ai"> <i class="fas fa-tag fa-sm"></i> AI</a> </div> </div> <div class="detail-item"> <div class="item-title">Tags : </div> </div> <div class="tags"> <a href="/blog/tag/ai"> <div class="tag"> AI </div> </a> <a href="/blog/tag/embedding"> <div class="tag"> Embedding </div> </a> <a href="/blog/tag/vector"> <div class="tag"> Vector </div> </a> <a href="/blog/tag/ml"> <div class="tag"> ML </div> </a> </div> <div class="detail-item"> <div class="item-title">Content : </div> </div> <div class="toc-container"> <div class="toc-item"> <a href="#what-are-embeddings"> # What are Embeddings </a> </div> <div class="toc-item"> <a href="#word-embedding"> # Word Embedding </a> </div> <div class="toc-item"> <a href="#pre-trained-models"> # Pre-trained Models </a> </div> <div class="toc-item"> <a href="#what-is-word2vec-exactly"> # What is Word2vec exactly </a> </div> <div class="toc-item"> <a href="#create-your-own-embeddings"> # Create Your Own Embeddings </a> </div> <div class="toc-item"> <a href="#faq"> # FAQ </a> </div> </div> </div> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Ben Wen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/jquery.min.js?d41d8cd98f00b204e9800998ecf8427e"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="/assets/js/tex-mml-chtml.js?d41d8cd98f00b204e9800998ecf8427e"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>